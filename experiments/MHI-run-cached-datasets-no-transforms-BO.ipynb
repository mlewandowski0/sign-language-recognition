{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will run Bayesian Optimization (BO) for MHI with decay specified in config\n",
    "\n",
    "Dataset is 100 samples.<br>\n",
    "DECAY = {BO will find}<br>\n",
    "FRAME_SIZE = {BO will find}<br>\n",
    "BINARY_THRESHOLD = {BO will find}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.0)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.14.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.16.6)\n",
      "Requirement already satisfied: tdqm in /usr/local/lib/python3.11/dist-packages (0.0.1)\n",
      "Requirement already satisfied: wurlitzer in /usr/local/lib/python3.11/dist-packages (3.0.3)\n",
      "Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (1.4.3)\n",
      "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (3.6.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.24.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.58.0)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.14.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.14.0)\n",
      "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.14.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (1.45.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.11/dist-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from tdqm) (4.66.2)\n",
      "Requirement already satisfied: scikit-image>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.23.1)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from albumentations) (4.9.0.80)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.13.1)\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.8.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.29)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (1.3.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
      "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.21.0->albumentations) (3.3)\n",
      "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.21.0->albumentations) (2.34.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.21.0->albumentations) (2024.2.12)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.21.0->albumentations) (0.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.23.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.3.7)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python numpy tensorflow scikit-learn matplotlib wandb tdqm wurlitzer albumentations optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 01:23:10.957987: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-16 01:23:10.958036: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-16 01:23:10.958066: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-16 01:23:10.964930: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#all the required dependencies of the project\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import wandb\n",
    "\n",
    "from typing import Tuple\n",
    "from collections import Counter\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "import json\n",
    "import tensorflow.keras as keras \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    ROOT_DIRECTORY = os.path.join(\".\")\n",
    "    JSON_FILE = \"WLASL_v0.3.json\"\n",
    "    NSLT_FILE = \"nslt_100.json\"\n",
    "    VIDEO_FOLDER = \"videos\"\n",
    "\n",
    "    mean=[0, 0, 0]\n",
    "    std=[1, 1, 1]\n",
    "    DEBUG = True\n",
    "    \n",
    "    P_OF_TRANSFORM = 0.8\n",
    "    P_OF_TRANSFORM_COLOR = 0.2\n",
    "    \n",
    "    SHIFT_LIMIT=0.1\n",
    "    SCALE_LIMIT=0.05\n",
    "    ROTATE_LIMIT=10\n",
    "    \n",
    "    # set to small, when prototyping, or 0 when deploying to cloud or PC with loads of RAM\n",
    "    DATA_LIMIT = 100\n",
    "    FRAME_SIZE = 20\n",
    "    \n",
    "    BATCH_SIZE = 4\n",
    "    PORTION_OF_DATA_FOR_TRAINING = 0.8\n",
    "\n",
    "    ROUND_DIGIT = 3\n",
    "    #WANDB_RUN = \"mediapipe-asl-dataset\"\n",
    "    DECAY = 0.09\n",
    "    \n",
    "    USE_WANDB = True\n",
    "\n",
    "    WANDB_RUN = \"MHI-CNN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, LeakyReLU\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras import backend as K\n",
    "from wandb.keras import WandbMetricsLogger\n",
    "from keras.callbacks import Callback\n",
    "from wandb.keras import WandbMetricsLogger\n",
    "import tensorflow as tf\n",
    "import wandb\n",
    "\n",
    "\n",
    "class CosineAnnealingLearningRateScheduler(Callback):\n",
    "    def __init__(self, max_lr, min_lr, T_max):\n",
    "        super(CosineAnnealingLearningRateScheduler, self).__init__()\n",
    "        self.max_lr = max_lr  # Maximum learning rate (i.e., start learning rate)\n",
    "        self.min_lr = min_lr  # Minimum learning rate\n",
    "        self.T_max = T_max    # Specifies the number of epochs per cycle\n",
    "        self.t = 0            # Current epoch\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.t += 1\n",
    "        cos = np.cos(np.pi * (self.t % self.T_max) / self.T_max)\n",
    "        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + cos)\n",
    "\n",
    "        keras.backend.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "def keras_train(model, filepath : str, run_name : str,\n",
    "                train_ds, val_ds, dataset_name,\n",
    "                train_ds_len : int, val_ds_len : int, unique_labels_num :int,\n",
    "                max_lr = 1e-4, min_lr = 5e-5,\n",
    "                T_max=50, epochs=100,\n",
    "                decay = 0.09,USE_WANDB=True): \n",
    "    \n",
    "    cosine_annealer = CosineAnnealingLearningRateScheduler(max_lr=max_lr,\n",
    "                                                           min_lr=min_lr,\n",
    "                                                           T_max=T_max)\n",
    "\n",
    "    callbacks = [cosine_annealer]\n",
    "\n",
    "    if USE_WANDB:\n",
    "        wandb.init(project=CONFIG.WANDB_RUN,\n",
    "                        name=run_name,\n",
    "                        notes=\"Model summary : \\n\" + str(model),\n",
    "                        config={\"max_lr\" : max_lr, \n",
    "                                \"min_lr\" : min_lr, \n",
    "                                \"scheduler\" : \"cosineAnnealer\", \n",
    "                                \"epochs\" : epochs, \n",
    "                                \"T_max\" : T_max, \n",
    "                                \"train_size\" : train_ds_len,\n",
    "                                \"val_size\" : val_ds_len,\n",
    "                                \"unique_classes\" : unique_labels_num, \n",
    "                                \"video_length\" : CONFIG.FRAME_SIZE,\n",
    "                                \"decay\" : decay,\n",
    "                                \"SHIFT_LIMIT\" : CONFIG.SHIFT_LIMIT,\n",
    "                                \"SCALE_LIMIT\" : CONFIG.SCALE_LIMIT,\n",
    "                                \"ROTATE_LIMIT\" : CONFIG.ROTATE_LIMIT,\n",
    "                                \"CACHEING\" : dataset_name\n",
    "                               })\n",
    "        callbacks.append(WandbMetricsLogger())\n",
    "    \n",
    "    #Adam Optimizer - fixed learning rate.\n",
    "    adam_optimizer = tf.keras.optimizers.Adam(learning_rate=max_lr, clipnorm=1.)\n",
    "    #lr_metric = get_lr_metric(adam_optimizer)\n",
    "\n",
    "    model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    \n",
    "    \n",
    "    #history = model.fit(train_dataset_parquet, epochs=epochs, validation_data = val_dataset_parquet, batch_size = 8, callbacks=[WandbMetricsLogger(), checkpoint, cosine_annealer])\n",
    "    history = model.fit(train_ds, epochs=epochs, validation_data = val_ds, batch_size = CONFIG.BATCH_SIZE,\n",
    "                        verbose=0, callbacks=callbacks)\n",
    "    #wandb.finish()\n",
    "    if USE_WANDB:      \n",
    "        wandb.finish()\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cacheing V1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from math import ceil\n",
    "\n",
    "class SignRecognitionDataset(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, max_start : int, max_end) -> None:\n",
    "        # setup the paths\n",
    "        video_path = os.path.join(CONFIG.ROOT_DIRECTORY, CONFIG.VIDEO_FOLDER)\n",
    "        dataset_description = os.path.join(CONFIG.ROOT_DIRECTORY)\n",
    "\n",
    "        # load the filepaths for videos\n",
    "        self.video_paths = [os.path.join(video_path, file) for file in os.listdir(video_path)]\n",
    "\n",
    "        # load the dataset config json\n",
    "        self.config_json = None\n",
    "        with open(os.path.join(CONFIG.ROOT_DIRECTORY, CONFIG.JSON_FILE)) as f:\n",
    "            self.config_json = json.load(f)\n",
    "\n",
    "        # load the dataset json\n",
    "        self.dataset_json = None\n",
    "        with open(os.path.join(CONFIG.ROOT_DIRECTORY, CONFIG.NSLT_FILE)) as f:\n",
    "            self.dataset_json = json.load(f)\n",
    "        \n",
    "        self.videos_paths = []\n",
    "        self.paths_not_found = []\n",
    "        self.labels = []\n",
    "        self.start_frames = []\n",
    "        self.end_frames = []\n",
    "       \n",
    "        self.dataset_json = None\n",
    "        with open(os.path.join(CONFIG.ROOT_DIRECTORY, CONFIG.NSLT_FILE)) as f:\n",
    "            self.dataset_json = json.load(f) \n",
    "\n",
    "        for el in self.dataset_json.items():\n",
    "            video_id, properties = el[0], el[1]\n",
    "            path = os.path.join(video_path, video_id + \".mp4\")\n",
    "            \n",
    "            if not os.path.exists(path):\n",
    "                self.paths_not_found.append(path)\n",
    "                continue\n",
    "\n",
    "            subset = properties[\"subset\"]\n",
    "            label, start, end = properties[\"action\"]\n",
    "            \n",
    "            if start > max_start:\n",
    "                continue\n",
    "                \n",
    "            if end > max_end:\n",
    "                continue\n",
    "            \n",
    "            self.videos_paths.append(path)\n",
    "            self.labels.append(label)\n",
    "            self.start_frames.append(start)\n",
    "            self.end_frames.append(end)\n",
    "    \n",
    "        self.videos_paths = np.array(self.video_paths)\n",
    "        self.paths_not_found = np.array(self.paths_not_found)\n",
    "        self.labels = np.array(self.labels)\n",
    "        self.start_frames = np.array(self.start_frames)\n",
    "        self.end_frames = np.array(self.end_frames)\n",
    "\n",
    "        self.unique_labels = np.unique(self.labels)\n",
    "        \n",
    "    \n",
    "    def preprocess_trajectory(self, traj : List[np.ndarray]):\n",
    "        return traj\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.videos_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.videos_paths[idx], self.labels[idx]\n",
    "        trajectory = SignRecognitionDataset.get_video(path)\n",
    "        \n",
    "        return self.preprocess_trajectory(trajectory), label\n",
    "\n",
    "    def permutate(self):\n",
    "        l = len(self.videos_paths)\n",
    "        mask = np.arange(l)\n",
    "        np.random.shuffle(mask)\n",
    "        \n",
    "        self.videos_paths = np.array(self.videos_paths)[mask]\n",
    "        self.labels = np.array(self.labels)[mask]\n",
    "        self.start_frames = np.array(self.start_frames)[mask]\n",
    "        self.end_frames = np.array(self.end_frames)[mask]\n",
    "        \n",
    "    def sort_by_size(self):\n",
    "        c = Counter(self.labels)\n",
    "        _mask = sorted([ (10000 * c[l] + l, i) for i, l in enumerate(self.labels)])[::-1]\n",
    "        mask_by_size = np.array([el[1] for el in _mask])\n",
    "\n",
    "        self.video_paths        = np.array(self.video_paths)[mask_by_size]\n",
    "        self.labels             = np.array(self.labels)[mask_by_size]\n",
    "        self.start_frames       = np.array(self.start_frames)[mask_by_size]\n",
    "        self.end_frames         = np.array(self.end_frames)[mask_by_size]\n",
    "\n",
    "    def crop_video(self, trajectory : np.array) -> np.array:\n",
    "        cropped = trajectory\n",
    "        \n",
    "        if self.FRAME_SIZE != 0:\n",
    "            frame_size = len(trajectory)\n",
    "            start = 0 \n",
    "            \n",
    "            if frame_size > self.FRAME_SIZE:\n",
    "                start = np.random.randint(0, frame_size - self.FRAME_SIZE)\n",
    "            cropped = trajectory[start: (start + self.FRAME_SIZE)]\n",
    "                    \n",
    "            if len(cropped) < self.FRAME_SIZE:\n",
    "                necessary = self.FRAME_SIZE - len(cropped)\n",
    "                t, h, w, c = trajectory.shape\n",
    "                cropped = np.concatenate([cropped, np.zeros((necessary, h, w, c))], axis= 0)\n",
    "                \n",
    "            return cropped\n",
    "                        \n",
    "        return trajectory\n",
    "                \n",
    "    @staticmethod\n",
    "    def get_video(video_path : str) -> List[np.ndarray]:\n",
    "\n",
    "        if not os.path.exists(video_path):\n",
    "            return None\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            return None\n",
    "\n",
    "        frames = []\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame_array = np.array(frame)\n",
    "                frames.append(cv2.cvtColor(frame_array, cv2.COLOR_BGR2RGB))\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        return np.array(frames)\n",
    "\n",
    "    @staticmethod\n",
    "    def rescale_video(frames : np.ndarray, desired_shape) -> np.ndarray:\n",
    "        refined = []\n",
    "        for img in frames: \n",
    "            y, x, c = img.shape\n",
    "            cropped = img[:, (x // 2 - y//2) : (x // 2 + y//2), :]\n",
    "            scaled = cv2.resize(cropped, desired_shape)\n",
    "            refined.append(scaled)\n",
    "    \n",
    "        return np.array(refined)\n",
    "        \n",
    "class SignRecognitionDatasetMHICached(SignRecognitionDataset):\n",
    "\n",
    "    def __init__(self, max_start: int, max_end, \n",
    "                 per_image_transform=None,\n",
    "                 after_MHI_transform=None,\n",
    "                 scaled_resolution : Tuple[int]= (224, 224),\n",
    "                 frame_size:int=CONFIG.FRAME_SIZE,\n",
    "                 data_limit : int = CONFIG.DATA_LIMIT,\n",
    "                 decay : float = 0.7,\n",
    "                 threshold_method : str = \"regular\",\n",
    "                 threshold_val : float = 25/255.,\n",
    "                 by_size=True,\n",
    "                 train_val_split:float=CONFIG.PORTION_OF_DATA_FOR_TRAINING,\n",
    "                 split:str=\"train\") -> None:\n",
    "        super().__init__(max_start, max_end)\n",
    "        self.scaled_resolution = scaled_resolution\n",
    "        self.DATA_LIMIT = data_limit\n",
    "        self.by_size = by_size\n",
    "        self.per_image_transform = per_image_transform\n",
    "        self.after_MHI_transform = after_MHI_transform\n",
    "        self.FRAME_SIZE = frame_size\n",
    "        self.decay = decay\n",
    "        self.threshold_method = threshold_method\n",
    "        self.threshold_val = threshold_val\n",
    "        \n",
    "        self.keywords = [\"image\" ] + list(str(i) for i in range(frame_size-1))\n",
    "        \n",
    "        self.sort_by_size()\n",
    "        \n",
    "        if self.DATA_LIMIT > 0:\n",
    "            self.video_paths        = self.video_paths[:self.DATA_LIMIT]\n",
    "            self.labels             = self.labels[:self.DATA_LIMIT]\n",
    "            self.start_frames       = self.start_frames[:self.DATA_LIMIT]\n",
    "            self.end_frames         = self.end_frames[:self.DATA_LIMIT]\n",
    "        \n",
    "        \n",
    "        self.label_2_id = { key : i for i, key in enumerate(np.unique(self.labels))}\n",
    "        if data_limit < 0:\n",
    "            self.unique_labels = np.unique(self.labels)\n",
    "            train_ds_x, val_ds_x = train_test_split(self.videos_paths, train_size=train_val_split, random_state=1337)\n",
    "            train_ds_y, val_ds_y = train_test_split(self.labels, train_size=train_val_split, random_state=1337)\n",
    "            \n",
    "        else:\n",
    "            self.unique_labels = np.unique(self.labels[:data_limit])\n",
    "            self.label_2_id = { key : i for i, key in enumerate(np.unique(self.labels))}\n",
    "            \n",
    "            train_ds_x, val_ds_x = train_test_split(self.videos_paths[:data_limit], train_size=train_val_split, random_state=1337)\n",
    "            train_ds_y, val_ds_y = train_test_split(self.labels[:data_limit], train_size=train_val_split, random_state=1337)\n",
    "                                    \n",
    "        if split.lower() == \"train\":\n",
    "            self.videos_paths = train_ds_x\n",
    "            self.labels = train_ds_y\n",
    "            \n",
    "        elif split.lower() == \"val\":\n",
    "            self.videos_paths = val_ds_x\n",
    "            self.labels = val_ds_y\n",
    "        \n",
    "        else: \n",
    "            raise Exception(\"\")\n",
    "\n",
    "        self.cache_data()\n",
    "\n",
    "    \n",
    "    def cache_data(self):    \n",
    "        self.cached_X = []\n",
    "        self.cached_Y = []\n",
    "        \n",
    "        for i,path in enumerate(self.videos_paths):\n",
    "            \n",
    "            trajectory = SignRecognitionDataset.get_video(path)  \n",
    "            trajectory, label = self.preprocess_trajectory(trajectory), self.labels[i]            \n",
    "            \n",
    "            onehotencoded = np.zeros(len(self.unique_labels))\n",
    "            onehotencoded[self.label_2_id[label]] = 1.0\n",
    "                    \n",
    "            self.cached_X.append(trajectory)\n",
    "            self.cached_Y.append(onehotencoded)\n",
    "        \n",
    "        \n",
    "    def preprocess_trajectory(self, traj : List[np.ndarray]):\n",
    "        return SignRecognitionDataset.rescale_video(traj, self.scaled_resolution)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.videos_paths)\n",
    "\n",
    "    def postprocess_trajectory(self, traj : np.ndarray) -> np.ndarray:\n",
    "        # timeframe, Width, height, channels\n",
    "        ts, w, h, c = traj.shape\n",
    "        \n",
    "        mhi = np.zeros((w, h))\n",
    "                \n",
    "        for i in range(1, ts):\n",
    "            frame_diff = np.abs(traj[i] - traj[i-1])\n",
    "            gray_diff = np.mean(frame_diff, axis=2)\n",
    "            \n",
    "            _, binary_diff = cv2.threshold(gray_diff, self.threshold_val, 1.0, cv2.THRESH_BINARY)\n",
    "    \n",
    "    \n",
    "            mhi = mhi * (1.0 - self.decay) + binary_diff\n",
    "\n",
    "        mhi = mhi.reshape(mhi.shape[0], mhi.shape[1], 1)\n",
    "        return np.concatenate([mhi,mhi,mhi], axis=2)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        trajectory, label = self.crop_video(self.cached_X[idx]), self.cached_Y[idx]\n",
    "        \n",
    "        if self.per_image_transform is not None:            \n",
    "            frames = {self.keywords[i] : frame for i, frame in enumerate(trajectory)}\n",
    "            processing = self.per_image_transform(**frames)       \n",
    "\n",
    "            trajectory = np.array([processing[kw] for kw in self.keywords])    \n",
    "\n",
    "        return self.postprocess_trajectory(trajectory), label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BO task creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BO_experiment(trial):\n",
    "    \n",
    "    # BO suggest parameters\n",
    "    suggested_decay = trial.suggest_float('decay', 0, 1)\n",
    "    suggested_threshold_val = trial.suggest_float(\"threshold\", 0, 1)\n",
    "    suggested_train_val_split = trial.suggest_float(\"trainval_split\", 0.5, 1)\n",
    "    suggested_frame_size = trial.suggest_int(\"framelength\", 5, 50)\n",
    "    \n",
    "\n",
    "    transform = A.Compose(\n",
    "        [\n",
    "            A.Normalize(mean=CONFIG.mean, std=CONFIG.std),\n",
    "            #A.HorizontalFlip(p=CONFIG.P_OF_TRANSFORM),\n",
    "            #A.ShiftScaleRotate(p=CONFIG.P_OF_TRANSFORM, shift_limit=CONFIG.SHIFT_LIMIT, \n",
    "            #                scale_limit=CONFIG.SCALE_LIMIT, rotate_limit=CONFIG.ROTATE_LIMIT),\n",
    "            #A.RandomBrightnessContrast(p=CONFIG.P_OF_TRANSFORM_COLOR),\n",
    "            #A.RGBShift(p=CONFIG.P_OF_TRANSFORM_COLOR),\n",
    "        ],\n",
    "        additional_targets={str(i) : \"image\" for i in range(CONFIG.FRAME_SIZE)}\n",
    "    )\n",
    "\n",
    "    train_ds = SignRecognitionDatasetMHICached(1, 150, per_image_transform=transform, \n",
    "                                               decay=suggested_decay,\n",
    "                                               threshold_val=suggested_threshold_val,\n",
    "                                               train_val_split=suggested_train_val_split,\n",
    "                                               frame_size=suggested_frame_size, \n",
    "                                               split=\"train\")\n",
    "    \n",
    "    val_ds = SignRecognitionDatasetMHICached(1, 150, per_image_transform=transform, \n",
    "                                               decay=suggested_decay,\n",
    "                                               threshold_val=suggested_threshold_val,\n",
    "                                               train_val_split=suggested_train_val_split,\n",
    "                                               frame_size=suggested_frame_size, \n",
    "                                               split=\"val\")\n",
    "\n",
    "    train_ds_len = len(train_ds)\n",
    "    val_ds_len = len(val_ds)\n",
    "    unique_labels_num = len(train_ds.unique_labels)\n",
    "\n",
    "    x, y = next(iter(train_ds))\n",
    "    x_shape = x.shape\n",
    "    y_shape = y.shape\n",
    "    \n",
    "    \n",
    "    def dataset_train_generator():\n",
    "        # Instantiate your existing dataset loader\n",
    "        for i in range(len(train_ds)):\n",
    "            X_batch, Y_batch = train_ds[i]\n",
    "            yield X_batch, Y_batch\n",
    "        \n",
    "    def dataset_val_generator():\n",
    "        # Instantiate your existing dataset loader\n",
    "\n",
    "        for i in range(len(val_ds)):\n",
    "            X_batch, Y_batch = val_ds[i]\n",
    "            yield X_batch, Y_batch        \n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: dataset_train_generator(),\n",
    "        output_types=(tf.float32, tf.float32),  # Adjust types based on your actual data\n",
    "        output_shapes=(x_shape, y_shape)\n",
    "    ).prefetch(tf.data.AUTOTUNE).batch(CONFIG.BATCH_SIZE)\n",
    "\n",
    "    val_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: dataset_val_generator(),\n",
    "        output_types=(tf.float32, tf.float32),  # Adjust types based on your actual data\n",
    "        output_shapes=(x_shape, y_shape)\n",
    "    ).prefetch(tf.data.AUTOTUNE).batch(CONFIG.BATCH_SIZE)   \n",
    "    \n",
    "    \n",
    "    MHI_SHAPE = (224,224,3)\n",
    "    # get the base model, exclude final dense layers - we will modify/output this\n",
    "    base_model = tf.keras.applications.MobileNetV3Small(input_shape = MHI_SHAPE,\n",
    "                                                include_top = False,\n",
    "                                                weights = 'imagenet',\n",
    "                                                pooling='max')\n",
    "    # Freeze the convolutional base\n",
    "    base_model.trainable = True\n",
    "\n",
    "    #len(train_ds.unique_labels) outputs the length of the labels.\n",
    "    #MobileNetV3 without additional dense layer - add softmax classification layer\n",
    "    model = Sequential()\n",
    "    model.add(base_model)\n",
    "    model.add(Dense(len(train_ds.unique_labels), activation='softmax'))\n",
    "\n",
    "    hist = keras_train(model, run_name=\"C1_MobileNetV3Small\", \n",
    "                train_ds=train_dataset, \n",
    "                val_ds=val_dataset, \n",
    "                train_ds_len=train_ds_len,\n",
    "                val_ds_len=val_ds_len,\n",
    "                unique_labels_num=unique_labels_num,\n",
    "                dataset_name=\"transforms_for_video\", \n",
    "                max_lr = 1e-4, \n",
    "                min_lr = 1e-5,\n",
    "                T_max=101, \n",
    "                epochs=200,\n",
    "                filepath=os.path.join(\"models\", \"MHI_MobileNetV3Small.tf\"), \n",
    "                USE_WANDB=CONFIG.USE_WANDB)\n",
    "\n",
    "    return max(hist.history['val_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run BO for MobileNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-16 01:26:39,932] A new study created in memory with name: no-name-dadcabd2-1fcc-445d-9d2f-9ab9033aa116\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(BO_experiment, n_trials=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cacheing V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2038/2038 [00:00<00:00, 166738.02it/s]\n",
      "Cacheing: 80it [02:07,  1.60s/it]\n",
      "100%|██████████| 2038/2038 [00:00<00:00, 128467.82it/s]\n",
      "Cacheing: 20it [00:28,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train_ds = 80, size of val_ds = 20\n",
      "(224, 224, 3) (7,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class SignRecognitionDatasetMHICachedV2(SignRecognitionDataset):\n",
    "\n",
    "    def __init__(self, max_start: int, max_end, \n",
    "                 per_image_transform=None,\n",
    "                 after_MHI_transform=None,\n",
    "                 scaled_resolution : Tuple[int]= (224, 224),\n",
    "                 frame_size:int=CONFIG.FRAME_SIZE,\n",
    "                 data_limit : int = CONFIG.DATA_LIMIT,\n",
    "                 decay : float = 0.7,\n",
    "                 threshold_method : str = \"regular\",\n",
    "                 threshold_val : float = 25/255.,\n",
    "                 by_size=True,\n",
    "                 train_val_split:float=CONFIG.PORTION_OF_DATA_FOR_TRAINING,\n",
    "                 split:str=\"train\") -> None:\n",
    "        super().__init__(max_start, max_end)\n",
    "        self.scaled_resolution = scaled_resolution\n",
    "        self.DATA_LIMIT = data_limit\n",
    "        self.by_size = by_size\n",
    "        self.per_image_transform = per_image_transform\n",
    "        self.after_MHI_transform = after_MHI_transform\n",
    "        self.FRAME_SIZE = frame_size\n",
    "        self.decay = decay\n",
    "        self.threshold_method = threshold_method\n",
    "        self.threshold_val = threshold_val\n",
    "        \n",
    "        self.keywords = [\"image\" ] + list(str(i) for i in range(frame_size-1))\n",
    "        \n",
    "        self.sort_by_size()\n",
    "        \n",
    "        if self.DATA_LIMIT > 0:\n",
    "            self.video_paths        = self.video_paths[:self.DATA_LIMIT]\n",
    "            self.labels             = self.labels[:self.DATA_LIMIT]\n",
    "            self.start_frames       = self.start_frames[:self.DATA_LIMIT]\n",
    "            self.end_frames         = self.end_frames[:self.DATA_LIMIT]\n",
    "        \n",
    "        \n",
    "        self.label_2_id = { key : i for i, key in enumerate(np.unique(self.labels))}\n",
    "        if data_limit < 0:\n",
    "            train_ds_x, val_ds_x = train_test_split(self.videos_paths, train_size=train_val_split, random_state=42)\n",
    "            train_ds_y, val_ds_y = train_test_split(self.labels, train_size=train_val_split, random_state=42)\n",
    "            \n",
    "        else:\n",
    "            self.unique_labels = np.unique(self.labels[:data_limit])\n",
    "            self.label_2_id = { key : i for i, key in enumerate(np.unique(self.labels))}\n",
    "            \n",
    "            train_ds_x, val_ds_x = train_test_split(self.videos_paths[:data_limit], train_size=train_val_split, random_state=42)\n",
    "            train_ds_y, val_ds_y = train_test_split(self.labels[:data_limit], train_size=train_val_split, random_state=42)\n",
    "                                    \n",
    "        if split.lower() == \"train\":\n",
    "            self.videos_paths = train_ds_x\n",
    "            self.labels = train_ds_y\n",
    "            \n",
    "        elif split.lower() == \"val\":\n",
    "            self.videos_paths = val_ds_x\n",
    "            self.labels = val_ds_y\n",
    "        \n",
    "        else: \n",
    "            raise Exception(\"\")\n",
    "\n",
    "        self.cache_data()\n",
    "\n",
    "    \n",
    "    def cache_data(self):    \n",
    "        self.cached_X = []\n",
    "        self.cached_Y = []\n",
    "        \n",
    "        for i,path in enumerate(self.videos_paths):\n",
    "            \n",
    "            trajectory = SignRecognitionDataset.get_video(path)  \n",
    "            trajectory, label = self.preprocess_trajectory(trajectory), self.labels[i]     \n",
    "            \n",
    "            cached_X = []\n",
    "            \n",
    "            trajectory_length = trajectory.shape[0]\n",
    "            if trajectory_length < self.FRAME_SIZE:\n",
    "                cached_X.append(self.postprocess_trajectory(trajectory))\n",
    "            else:\n",
    "                for i in range(0, trajectory_length - self.FRAME_SIZE):\n",
    "                    cached_X.append(self.postprocess_trajectory(trajectory[i: (i + self.FRAME_SIZE)]))\n",
    "\n",
    "            \n",
    "            onehotencoded = np.zeros(len(self.unique_labels))\n",
    "            onehotencoded[self.label_2_id[label]] = 1.0\n",
    "                    \n",
    "            self.cached_X.append(cached_X)\n",
    "            self.cached_Y.append(onehotencoded)\n",
    "        \n",
    "    def preprocess_trajectory(self, traj : List[np.ndarray]):\n",
    "        return SignRecognitionDataset.rescale_video(traj, self.scaled_resolution)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.videos_paths)\n",
    "\n",
    "    def postprocess_trajectory(self, traj : np.ndarray) -> np.ndarray:\n",
    "        # timeframe, Width, height, channels\n",
    "        ts, w, h, c = traj.shape\n",
    "        \n",
    "        mhi = np.zeros((w, h))\n",
    "                \n",
    "        for i in range(1, ts):\n",
    "            frame_diff = np.abs(traj[i] - traj[i-1])\n",
    "            gray_diff = np.mean(frame_diff, axis=2)\n",
    "            \n",
    "            _, binary_diff = cv2.threshold(gray_diff, self.threshold_val, 1.0, cv2.THRESH_BINARY)\n",
    "    \n",
    "    \n",
    "            mhi = mhi * (1.0 - self.decay) + binary_diff\n",
    "\n",
    "        mhi = mhi.reshape(mhi.shape[0], mhi.shape[1], 1)\n",
    "        return np.concatenate([mhi,mhi,mhi], axis=2)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        trajectory, label = self.cached_X[idx], self.cached_Y[idx]\n",
    "        \n",
    "        # get random MHI from trajectory\n",
    "        traj_len = len(trajectory)\n",
    "        \n",
    "        trajectory = trajectory[np.random.randint(traj_len)]        \n",
    "        trajectory = self.per_image_transform(image=trajectory)[\"image\"]\n",
    "\n",
    "        return trajectory, label\n",
    "    \n",
    "    \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "transform = A.Compose(\n",
    "    [\n",
    "        A.Normalize(mean=CONFIG.mean, std=CONFIG.std),\n",
    "        A.HorizontalFlip(p=CONFIG.P_OF_TRANSFORM),\n",
    "        A.ShiftScaleRotate(p=CONFIG.P_OF_TRANSFORM, shift_limit=CONFIG.SHIFT_LIMIT, \n",
    "                           scale_limit=CONFIG.SCALE_LIMIT, rotate_limit=CONFIG.ROTATE_LIMIT),\n",
    "        #A.RandomBrightnessContrast(p=CONFIG.P_OF_TRANSFORM_COLOR),\n",
    "        #A.RGBShift(p=CONFIG.P_OF_TRANSFORM_COLOR),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_ds = SignRecognitionDatasetMHICachedV2(1, 150, per_image_transform=transform, decay=CONFIG.DECAY, split=\"train\")\n",
    "val_ds = SignRecognitionDatasetMHICachedV2(1, 150, per_image_transform=transform, decay=CONFIG.DECAY, split=\"val\")\n",
    "\n",
    "\n",
    "train_ds_len = len(train_ds)\n",
    "val_ds_len = len(val_ds)\n",
    "unique_labels_num = len(train_ds.unique_labels)\n",
    "\n",
    "print(f\"size of train_ds = {len(train_ds)}, size of val_ds = {len(val_ds)}\")\n",
    "x, y = next(iter(train_ds))\n",
    "x_shape = x.shape\n",
    "y_shape = y.shape\n",
    "print(x_shape, y_shape)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1589971,
     "sourceId": 2632847,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30684,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
