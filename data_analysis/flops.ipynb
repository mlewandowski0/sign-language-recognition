{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install calflops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  61.1 M  \n",
      "fwd MACs:                                                               714.188 MMACs\n",
      "fwd FLOPs:                                                              1.4297 GFLOPS\n",
      "fwd+bwd MACs:                                                           2.1426 GMACs\n",
      "fwd+bwd FLOPs:                                                          4.2892 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "AlexNet(\n",
      "  61.1 M = 100% Params, 714.19 MMACs = 100% MACs, 1.43 GFLOPS = 49.9523% FLOPs\n",
      "  (features): Sequential(\n",
      "    2.47 M = 4.042% Params, 655.57 MMACs = 91.7918% MACs, 1.31 GFLOPS = 45.8521% FLOPs\n",
      "    (0): Conv2d(23.3 K = 0.0381% Params, 70.28 MMACs = 9.8401% MACs, 140.75 MFLOPS = 4.9154% FLOPs, 3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 193.6 KFLOPS = 0% FLOPs, inplace=True)\n",
      "    (2): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 193.6 KFLOPS = 0% FLOPs, kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(307.39 K = 0.5031% Params, 223.95 MMACs = 31.3571% MACs, 448.04 MFLOPS = 15.6636% FLOPs, 64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 139.97 KFLOPS = 0% FLOPs, inplace=True)\n",
      "    (5): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 139.97 KFLOPS = 0% FLOPs, kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(663.94 K = 1.0866% Params, 112.14 MMACs = 15.7018% MACs, 224.35 MFLOPS = 7.8434% FLOPs, 192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 64.9 KFLOPS = 0% FLOPs, inplace=True)\n",
      "    (8): Conv2d(884.99 K = 1.4484% Params, 149.52 MMACs = 20.9357% MACs, 299.08 MFLOPS = 10.4579% FLOPs, 384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 43.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "    (10): Conv2d(590.08 K = 0.9657% Params, 99.68 MMACs = 13.9571% MACs, 199.4 MFLOPS = 6.9719% FLOPs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 43.26 KFLOPS = 0% FLOPs, inplace=True)\n",
      "    (12): MaxPool2d(0 = 0% Params, 0 MACs = 0% MACs, 43.26 KFLOPS = 0% FLOPs, kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 9.22 KFLOPS = 0% FLOPs, output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    58.63 M = 95.958% Params, 58.62 MMACs = 8.2082% MACs, 117.25 MFLOPS = 4.1002% FLOPs\n",
      "    (0): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.5, inplace=False)\n",
      "    (1): Linear(37.75 M = 61.7877% Params, 37.75 MMACs = 5.2855% MACs, 75.5 MFLOPS = 2.6402% FLOPs, in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.1 KFLOPS = 0% FLOPs, inplace=True)\n",
      "    (3): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.5, inplace=False)\n",
      "    (4): Linear(16.78 M = 27.4649% Params, 16.78 MMACs = 2.3491% MACs, 33.55 MFLOPS = 1.1734% FLOPs, in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 4.1 KFLOPS = 0% FLOPs, inplace=True)\n",
      "    (6): Linear(4.1 M = 6.7053% Params, 4.1 MMACs = 0.5735% MACs, 8.19 MFLOPS = 0.2865% FLOPs, in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Alexnet FLOPs:1.4297 GFLOPS   MACs:714.188 MMACs   Params:61.1008 M \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from calflops import calculate_flops\n",
    "from torchvision import models\n",
    "\n",
    "model = models.alexnet()\n",
    "batch_size = 1\n",
    "input_shape = (batch_size, 3, 224, 224)\n",
    "flops, macs, params = calculate_flops(model=model, \n",
    "                                      input_shape=input_shape,\n",
    "                                      output_as_string=True,\n",
    "                                      output_precision=4)\n",
    "print(\"Alexnet FLOPs:%s   MACs:%s   Params:%s \\n\" %(flops, macs, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  84.69 K \n",
      "fwd MACs:                                                               5.376 KMACs\n",
      "fwd FLOPs:                                                              9.4929 MFLOPS\n",
      "fwd+bwd MACs:                                                           16.128 KMACs\n",
      "fwd+bwd FLOPs:                                                          28.4788 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "MyModel(\n",
      "  84.69 K = 100% Params, 5.38 KMACs = 100% MACs, 9.49 MFLOPS = 0.0566% FLOPs\n",
      "  (gru): GRU(79.06 K = 93.3442% Params, 0 MACs = 0% MACs, 9.48 MFLOPS = 0% FLOPs, 1629, 16, batch_first=True)\n",
      "  (leaky_relu): LeakyReLU(0 = 0% Params, 0 MACs = 0% MACs, 16 FLOPS = 0% FLOPs, negative_slope=0.2)\n",
      "  (dense1): Linear(4.35 K = 5.1386% Params, 4.1 KMACs = 76.1905% MACs, 8.19 KFLOPS = 0.0431% FLOPs, in_features=16, out_features=256, bias=True)\n",
      "  (dense2): Linear(1.28 K = 1.5172% Params, 1.28 KMACs = 23.8095% MACs, 2.56 KFLOPS = 0.0135% FLOPs, in_features=256, out_features=5, bias=True)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "GRU FLOPs:9.4929 MFLOPS   MACs:5.376 KMACs   Params:84.693 K \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, video_length, num_features, num_classes):\n",
    "        super(MyModel, self).__init__()\n",
    "        # GRU layer\n",
    "        self.gru = nn.GRU(input_size=num_features, hidden_size=16, batch_first=True)\n",
    "        # Regularization via weight decay in the optimizer\n",
    "        # Apply LeakyReLU separately due to PyTorch's design\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        # Dense layers\n",
    "        self.dense1 = nn.Linear(16, 256)\n",
    "        self.dense2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through GRU\n",
    "        out, _ = self.gru(x)\n",
    "        # Since return_sequences=False, only take the output of the last time step\n",
    "        out = out[:, -1, :]\n",
    "        out = self.leaky_relu(out)\n",
    "        out = self.dense1(out)\n",
    "        out = F.gelu(out)\n",
    "        out = self.dense2(out)\n",
    "        out = F.softmax(out, dim=-1)\n",
    "        return out\n",
    "\n",
    "# Constants\n",
    "VIDEO_LENGTH = 60  # Assuming this is defined somewhere in your config\n",
    "NUM_FEATURES = 1629  # Number of input features\n",
    "NUM_CLASSES = 5  # Number of output classes\n",
    "\n",
    "# Instantiate the model\n",
    "model = MyModel(video_length=VIDEO_LENGTH, num_features=NUM_FEATURES, num_classes=NUM_CLASSES)\n",
    "\n",
    "\n",
    "batch = np.random.randn(1, VIDEO_LENGTH, NUM_FEATURES)\n",
    "model(torch.Tensor(batch))\n",
    "\n",
    "flops, macs, params = calculate_flops(model=model, \n",
    "                                      input_shape=batch.shape,\n",
    "                                      output_as_string=True,\n",
    "                                      output_precision=4)\n",
    "print(\"GRU FLOPs:%s   MACs:%s   Params:%s \\n\" %(flops, macs, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "was expecting embedding dimension of 60, but got 1629",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 70\u001b[0m\n\u001b[0;32m     67\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m NUM_CLASSES  \u001b[38;5;66;03m# Number of output classes\u001b[39;00m\n\u001b[0;32m     68\u001b[0m model \u001b[38;5;241m=\u001b[39m TransformerModel(batch\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1639\u001b[39m, num_classes)\n\u001b[1;32m---> 70\u001b[0m model(torch\u001b[38;5;241m.\u001b[39mTensor(batch))\n",
      "File \u001b[1;32me:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[17], line 56\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     55\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_layer(x)\n\u001b[1;32m---> 56\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_block(x)\n\u001b[0;32m     57\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     58\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling(x)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32me:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[17], line 21\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Attention part\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     attn_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matt(x, x, x)\n\u001b[0;32m     22\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(attn_output)\n\u001b[0;32m     23\u001b[0m     out1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm1(x \u001b[38;5;241m+\u001b[39m attn_output)\n",
      "File \u001b[1;32me:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1241\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   1227\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1228\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[0;32m   1229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1238\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[0;32m   1239\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[0;32m   1240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1241\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1242\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[0;32m   1243\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   1244\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_k, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_v, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_zero_attn,\n\u001b[0;32m   1245\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m   1246\u001b[0m         training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[0;32m   1247\u001b[0m         key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[0;32m   1248\u001b[0m         need_weights\u001b[38;5;241m=\u001b[39mneed_weights,\n\u001b[0;32m   1249\u001b[0m         attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[0;32m   1250\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[0;32m   1251\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32me:\\Anaconda\\Lib\\site-packages\\torch\\nn\\functional.py:5316\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   5310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key_padding_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5311\u001b[0m         \u001b[38;5;66;03m# We have the attn_mask, and use that to merge kpm into it.\u001b[39;00m\n\u001b[0;32m   5312\u001b[0m         \u001b[38;5;66;03m# Turn off use of is_causal hint, as the merged mask is no\u001b[39;00m\n\u001b[0;32m   5313\u001b[0m         \u001b[38;5;66;03m# longer causal.\u001b[39;00m\n\u001b[0;32m   5314\u001b[0m         is_causal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 5316\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m embed_dim \u001b[38;5;241m==\u001b[39m embed_dim_to_check, \\\n\u001b[0;32m   5317\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwas expecting embedding dimension of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membed_dim_to_check\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membed_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(embed_dim, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m   5319\u001b[0m     \u001b[38;5;66;03m# embed_dim can be a tensor when JIT tracing\u001b[39;00m\n\u001b[0;32m   5320\u001b[0m     head_dim \u001b[38;5;241m=\u001b[39m embed_dim\u001b[38;5;241m.\u001b[39mdiv(num_heads, rounding_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrunc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: was expecting embedding dimension of 60, but got 1629"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(rate)\n",
    "        self.dropout2 = nn.Dropout(rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Attention part\n",
    "        attn_output, _ = self.att(x, x, x)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        # Feed forward part\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        return out2\n",
    "\n",
    "class TokenAndPositionEmbedding(nn.Module):\n",
    "    def __init__(self, maxlen, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = nn.Embedding(maxlen, embed_dim)\n",
    "        self.pos_emb = nn.Embedding(maxlen, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #positions = torch.arange(0, x.size(1)).unsqueeze(0).repeat(x.size(0), 1).to(x.device)\n",
    "        #print(positions.shape)\n",
    "        #pos = self.pos_emb(positions)\n",
    "        #print(x.shape, pos.shape)\n",
    "        #x = x + pos\n",
    "        return x\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_shape, num_heads, ff_dim, num_classes):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(input_shape[0], input_shape[1])\n",
    "        self.transformer_block = TransformerBlock(input_shape[1], num_heads, ff_dim)\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.dense1 = nn.Linear(input_shape[1], 128)\n",
    "        self.outputs = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_layer(x)\n",
    "        x = self.transformer_block(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.pooling(x).squeeze(-1)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.dense1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.softmax(self.outputs(x), dim=-1)\n",
    "        return x\n",
    "\n",
    "# Assuming the shape of your input and number of unique labels are defined\n",
    "batch = np.random.randn(1, VIDEO_LENGTH, NUM_FEATURES)\n",
    "num_classes = NUM_CLASSES  # Number of output classes\n",
    "model = TransformerModel(batch.shape, 2, 64, num_classes)\n",
    "\n",
    "model(torch.Tensor(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 5\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  339.59 K\n",
      "fwd MACs:                                                               54.2717 MMACs\n",
      "fwd FLOPs:                                                              109.235 MFLOPS\n",
      "fwd+bwd MACs:                                                           162.815 MMACs\n",
      "fwd+bwd FLOPs:                                                          327.704 MFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "TransformerModel(\n",
      "  339.59 K = 100% Params, 54.27 MMACs = 100% MACs, 109.23 MFLOPS = 49.6836% FLOPs\n",
      "  (pos_encoder): PositionalEncoding(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "  (encoder_layer): TransformerEncoderLayer(\n",
      "    33.47 K = 9.8566% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs\n",
      "    (self_attn): MultiheadAttention(\n",
      "      16.64 K = 4.9% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(4.16 K = 1.225% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=64, out_features=64, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(8.32 K = 2.45% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=64, out_features=128, bias=True)\n",
      "    (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1, inplace=False)\n",
      "    (linear2): Linear(8.26 K = 2.4312% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=128, out_features=64, bias=True)\n",
      "    (norm1): LayerNorm(128 = 0.0377% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, (64,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm(128 = 0.0377% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, (64,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1, inplace=False)\n",
      "    (dropout2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    200.83 K = 59.1397% Params, 35.39 MMACs = 65.2079% MACs, 71.47 MFLOPS = 32.3977% FLOPs\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayer(\n",
      "        33.47 K = 9.8566% Params, 5.9 MMACs = 10.868% MACs, 11.91 MFLOPS = 5.3996% FLOPs\n",
      "        (self_attn): MultiheadAttention(\n",
      "          16.64 K = 4.9% Params, 2.95 MMACs = 5.434% MACs, 5.9 MFLOPS = 2.6998% FLOPs\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(4.16 K = 1.225% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(8.32 K = 2.45% Params, 1.47 MMACs = 2.717% MACs, 2.95 MFLOPS = 1.3499% FLOPs, in_features=64, out_features=128, bias=True)\n",
      "        (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1, inplace=False)\n",
      "        (linear2): Linear(8.26 K = 2.4312% Params, 1.47 MMACs = 2.717% MACs, 2.95 MFLOPS = 1.3499% FLOPs, in_features=128, out_features=64, bias=True)\n",
      "        (norm1): LayerNorm(128 = 0.0377% Params, 0 MACs = 0% MACs, 57.6 KFLOPS = 0% FLOPs, (64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm(128 = 0.0377% Params, 0 MACs = 0% MACs, 57.6 KFLOPS = 0% FLOPs, (64,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (input_proj): Linear(104.96 K = 30.908% Params, 18.88 MMACs = 34.7903% MACs, 37.76 MFLOPS = 17.2851% FLOPs, in_features=1639, out_features=64, bias=True)\n",
      "  (output_proj): Linear(325 = 0.0957% Params, 960 MACs = 0.0018% MACs, 1.92 KFLOPS = 0.0009% FLOPs, in_features=64, out_features=5, bias=True)\n",
      "  (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Alexnet FLOPs:109.235 MFLOPS   MACs:54.2717 MMACs   Params:339.589 K \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Create a long enough 'position encoding' matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        # Register buffer: Not a parameter, but should be part of the state\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, num_classes, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n",
    "                                                        dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)  # Project input feature dimension to model dimension\n",
    "        print(d_model, num_classes)\n",
    "        self.output_proj = nn.Linear(d_model, num_classes)  # Final classification layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Project the src from input_dim to d_model\n",
    "        src = self.input_proj(src)\n",
    "        # Add positional encoding\n",
    "        src = self.pos_encoder(src)\n",
    "        # Transformer Encoder\n",
    "        output = self.transformer_encoder(src)\n",
    "        # Average Pooling across the sequence dimension (frames)\n",
    "        output = output.mean(dim=1)\n",
    "        output = self.dropout(output)\n",
    "        # Classification head\n",
    "        output = self.output_proj(output)\n",
    "        return output\n",
    "\n",
    "# Parameters\n",
    "input_dim = 1639  # Dimension of each input vector (already embedded)\n",
    "d_model = 64  # Dimensionality of the model (should be a multiple of nhead)\n",
    "nhead = 2  # Number of attention heads\n",
    "num_encoder_layers = 6  # Number of transformer layers\n",
    "dim_feedforward = 128  # Dimension of feedforward network\n",
    "num_classes = 5  # Number of classes for classification\n",
    "dropout = 0.1  #\n",
    "\n",
    "# Initialize model\n",
    "model = TransformerModel(input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, num_classes, dropout)\n",
    "\n",
    "# Example input: batch_size x frames x features\n",
    "batch_size = 1\n",
    "frames = 60\n",
    "features = 1639  # Features per frame\n",
    "\n",
    "# Create a random sample\n",
    "src = torch.randn(batch_size, frames, features)\n",
    "\n",
    "\n",
    "flops, macs, params = calculate_flops(model=model, \n",
    "                                      input_shape=tuple(src.shape),\n",
    "                                      output_as_string=True,\n",
    "                                      output_precision=4)\n",
    "print(\"Alexnet FLOPs:%s   MACs:%s   Params:%s \\n\" %(flops, macs, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  4.01 M  \n",
      "fwd MACs:                                                               1.5382 GMACs\n",
      "fwd FLOPs:                                                              3.1644 GFLOPS\n",
      "fwd+bwd MACs:                                                           4.6145 GMACs\n",
      "fwd+bwd FLOPs:                                                          9.4931 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "EfficientNet(\n",
      "  4.01 M = 100% Params, 1.54 GMACs = 100% MACs, 3.16 GFLOPS = 48.6088% FLOPs\n",
      "  (features): Sequential(\n",
      "    4.01 M = 99.8404% Params, 1.54 GMACs = 99.9983% MACs, 3.16 GFLOPS = 48.608% FLOPs\n",
      "    (0): Conv2dNormActivation(\n",
      "      928 = 0.0231% Params, 43.35 MMACs = 2.8184% MACs, 91.52 MFLOPS = 1.37% FLOPs\n",
      "      (0): Conv2d(864 = 0.0215% Params, 43.35 MMACs = 2.8184% MACs, 86.7 MFLOPS = 1.37% FLOPs, 3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64 = 0.0016% Params, 0 MACs = 0% MACs, 3.21 MFLOPS = 0% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 1.61 MFLOPS = 0% FLOPs, inplace=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      1.45 K = 0.0361% Params, 40.14 MMACs = 2.6098% MACs, 88.31 MFLOPS = 1.2686% FLOPs\n",
      "      (0): MBConv(\n",
      "        1.45 K = 0.0361% Params, 40.14 MMACs = 2.6098% MACs, 88.31 MFLOPS = 1.2686% FLOPs\n",
      "        (block): Sequential(\n",
      "          1.45 K = 0.0361% Params, 40.14 MMACs = 2.6098% MACs, 88.31 MFLOPS = 1.2686% FLOPs\n",
      "          (0): Conv2dNormActivation(\n",
      "            352 = 0.0088% Params, 14.45 MMACs = 0.9395% MACs, 33.72 MFLOPS = 0.4567% FLOPs\n",
      "            (0): Conv2d(288 = 0.0072% Params, 14.45 MMACs = 0.9395% MACs, 28.9 MFLOPS = 0.4567% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "            (1): BatchNorm2d(64 = 0.0016% Params, 0 MACs = 0% MACs, 3.21 MFLOPS = 0% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 1.61 MFLOPS = 0% FLOPs, inplace=True)\n",
      "          )\n",
      "          (1): SqueezeExcitation(\n",
      "            552 = 0.0138% Params, 2.05 KMACs = 0.0001% MACs, 1.61 MFLOPS = 0.0001% FLOPs\n",
      "            (avgpool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 1.61 MFLOPS = 0% FLOPs, output_size=1)\n",
      "            (fc1): Conv2d(264 = 0.0066% Params, 1.02 KMACs = 0.0001% MACs, 2.08 KFLOPS = 0% FLOPs, 32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(288 = 0.0072% Params, 1.02 KMACs = 0.0001% MACs, 2.18 KFLOPS = 0% FLOPs, 8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 32 FLOPS = 0% FLOPs, inplace=True)\n",
      "            (scale_activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "          )\n",
      "          (2): Conv2dNormActivation(\n",
      "            544 = 0.0136% Params, 25.69 MMACs = 1.6702% MACs, 52.99 MFLOPS = 0.8119% FLOPs\n",
      "            (0): Conv2d(512 = 0.0128% Params, 25.69 MMACs = 1.6702% MACs, 51.38 MFLOPS = 0.8119% FLOPs, 32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(32 = 0.0008% Params, 0 MACs = 0% MACs, 1.61 MFLOPS = 0% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      16.71 K = 0.4164% Params, 219.78 MMACs = 14.2885% MACs, 472.68 MFLOPS = 6.9455% FLOPs\n",
      "      (0): MBConv(\n",
      "        6 K = 0.1496% Params, 116.81 MMACs = 7.5943% MACs, 253.5 MFLOPS = 3.6915% FLOPs\n",
      "        (block): Sequential(\n",
      "          6 K = 0.1496% Params, 116.81 MMACs = 7.5943% MACs, 253.5 MFLOPS = 3.6915% FLOPs\n",
      "          (0): Conv2dNormActivation(\n",
      "            1.73 K = 0.043% Params, 77.07 MMACs = 5.0105% MACs, 168.59 MFLOPS = 2.4356% FLOPs\n",
      "            (0): Conv2d(1.54 K = 0.0383% Params, 77.07 MMACs = 5.0105% MACs, 154.14 MFLOPS = 2.4356% FLOPs, 16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192 = 0.0048% Params, 0 MACs = 0% MACs, 9.63 MFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 4.82 MFLOPS = 0% FLOPs, inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            1.06 K = 0.0263% Params, 10.84 MMACs = 0.7046% MACs, 25.29 MFLOPS = 0.3425% FLOPs\n",
      "            (0): Conv2d(864 = 0.0215% Params, 10.84 MMACs = 0.7046% MACs, 21.68 MFLOPS = 0.3425% FLOPs, 96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "            (1): BatchNorm2d(192 = 0.0048% Params, 0 MACs = 0% MACs, 2.41 MFLOPS = 0% FLOPs, 96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 1.2 MFLOPS = 0% FLOPs, inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            868 = 0.0216% Params, 3.07 KMACs = 0.0002% MACs, 1.21 MFLOPS = 0.0001% FLOPs\n",
      "            (avgpool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 1.2 MFLOPS = 0% FLOPs, output_size=1)\n",
      "            (fc1): Conv2d(388 = 0.0097% Params, 1.54 KMACs = 0.0001% MACs, 3.09 KFLOPS = 0% FLOPs, 96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(480 = 0.012% Params, 1.54 KMACs = 0.0001% MACs, 3.46 KFLOPS = 0% FLOPs, 4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 16 FLOPS = 0% FLOPs, inplace=True)\n",
      "            (scale_activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            2.35 K = 0.0586% Params, 28.9 MMACs = 1.879% MACs, 58.4 MFLOPS = 0.9133% FLOPs\n",
      "            (0): Conv2d(2.3 K = 0.0574% Params, 28.9 MMACs = 1.879% MACs, 57.8 MFLOPS = 0.9133% FLOPs, 96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(48 = 0.0012% Params, 0 MACs = 0% MACs, 602.11 KFLOPS = 0% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        10.71 K = 0.2668% Params, 102.97 MMACs = 6.6942% MACs, 219.18 MFLOPS = 3.254% FLOPs\n",
      "        (block): Sequential(\n",
      "          10.71 K = 0.2668% Params, 102.97 MMACs = 6.6942% MACs, 219.18 MFLOPS = 3.254% FLOPs\n",
      "          (0): Conv2dNormActivation(\n",
      "            3.74 K = 0.0933% Params, 43.35 MMACs = 2.8184% MACs, 92.12 MFLOPS = 1.37% FLOPs\n",
      "            (0): Conv2d(3.46 K = 0.0861% Params, 43.35 MMACs = 2.8184% MACs, 86.7 MFLOPS = 1.37% FLOPs, 24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(288 = 0.0072% Params, 0 MACs = 0% MACs, 3.61 MFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 1.81 MFLOPS = 0% FLOPs, inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            1.58 K = 0.0395% Params, 16.26 MMACs = 1.0569% MACs, 37.93 MFLOPS = 0.5138% FLOPs\n",
      "            (0): Conv2d(1.3 K = 0.0323% Params, 16.26 MMACs = 1.0569% MACs, 32.51 MFLOPS = 0.5138% FLOPs, 144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "            (1): BatchNorm2d(288 = 0.0072% Params, 0 MACs = 0% MACs, 3.61 MFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 1.81 MFLOPS = 0% FLOPs, inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            1.88 K = 0.0468% Params, 6.91 KMACs = 0.0004% MACs, 1.82 MFLOPS = 0.0002% FLOPs\n",
      "            (avgpool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 1.81 MFLOPS = 0% FLOPs, output_size=1)\n",
      "            (fc1): Conv2d(870 = 0.0217% Params, 3.46 KMACs = 0.0002% MACs, 6.94 KFLOPS = 0.0001% FLOPs, 144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(1.01 K = 0.0251% Params, 3.46 KMACs = 0.0002% MACs, 7.49 KFLOPS = 0.0001% FLOPs, 6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 24 FLOPS = 0% FLOPs, inplace=True)\n",
      "            (scale_activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            3.5 K = 0.0873% Params, 43.35 MMACs = 2.8184% MACs, 87.31 MFLOPS = 1.37% FLOPs\n",
      "            (0): Conv2d(3.46 K = 0.0861% Params, 43.35 MMACs = 2.8184% MACs, 86.7 MFLOPS = 1.37% FLOPs, 144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(48 = 0.0012% Params, 0 MACs = 0% MACs, 602.11 KFLOPS = 0% FLOPs, 24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      46.64 K = 1.1619% Params, 151.76 MMACs = 9.8662% MACs, 316.51 MFLOPS = 4.7958% FLOPs\n",
      "      (0): MBConv(\n",
      "        15.35 K = 0.3824% Params, 72.71 MMACs = 4.7272% MACs, 152.9 MFLOPS = 2.2978% FLOPs\n",
      "        (block): Sequential(\n",
      "          15.35 K = 0.3824% Params, 72.71 MMACs = 4.7272% MACs, 152.9 MFLOPS = 2.2978% FLOPs\n",
      "          (0): Conv2dNormActivation(\n",
      "            3.74 K = 0.0933% Params, 43.35 MMACs = 2.8184% MACs, 92.12 MFLOPS = 1.37% FLOPs\n",
      "            (0): Conv2d(3.46 K = 0.0861% Params, 43.35 MMACs = 2.8184% MACs, 86.7 MFLOPS = 1.37% FLOPs, 24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(288 = 0.0072% Params, 0 MACs = 0% MACs, 3.61 MFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 1.81 MFLOPS = 0% FLOPs, inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            3.89 K = 0.0969% Params, 11.29 MMACs = 0.734% MACs, 23.93 MFLOPS = 0.3568% FLOPs\n",
      "            (0): Conv2d(3.6 K = 0.0897% Params, 11.29 MMACs = 0.734% MACs, 22.58 MFLOPS = 0.3568% FLOPs, 144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
      "            (1): BatchNorm2d(288 = 0.0072% Params, 0 MACs = 0% MACs, 903.17 KFLOPS = 0% FLOPs, 144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 451.58 KFLOPS = 0% FLOPs, inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            1.88 K = 0.0468% Params, 6.91 KMACs = 0.0004% MACs, 466.03 KFLOPS = 0.0002% FLOPs\n",
      "            (avgpool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 451.58 KFLOPS = 0% FLOPs, output_size=1)\n",
      "            (fc1): Conv2d(870 = 0.0217% Params, 3.46 KMACs = 0.0002% MACs, 6.94 KFLOPS = 0.0001% FLOPs, 144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(1.01 K = 0.0251% Params, 3.46 KMACs = 0.0002% MACs, 7.49 KFLOPS = 0.0001% FLOPs, 6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 24 FLOPS = 0% FLOPs, inplace=True)\n",
      "            (scale_activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            5.84 K = 0.1455% Params, 18.06 MMACs = 1.1743% MACs, 36.38 MFLOPS = 0.5708% FLOPs\n",
      "            (0): Conv2d(5.76 K = 0.1435% Params, 18.06 MMACs = 1.1743% MACs, 36.13 MFLOPS = 0.5708% FLOPs, 144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(80 = 0.002% Params, 0 MACs = 0% MACs, 250.88 KFLOPS = 0% FLOPs, 40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        31.29 K = 0.7795% Params, 79.05 MMACs = 5.139% MACs, 163.61 MFLOPS = 2.498% FLOPs\n",
      "        (block): Sequential(\n",
      "          31.29 K = 0.7795% Params, 79.05 MMACs = 5.139% MACs, 163.61 MFLOPS = 2.498% FLOPs\n",
      "          (0): Conv2dNormActivation(\n",
      "            10.08 K = 0.2511% Params, 30.11 MMACs = 1.9572% MACs, 62.47 MFLOPS = 0.9514% FLOPs\n",
      "            (0): Conv2d(9.6 K = 0.2392% Params, 30.11 MMACs = 1.9572% MACs, 60.21 MFLOPS = 0.9514% FLOPs, 40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(480 = 0.012% Params, 0 MACs = 0% MACs, 1.51 MFLOPS = 0% FLOPs, 240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 752.64 KFLOPS = 0% FLOPs, inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            6.48 K = 0.1614% Params, 18.82 MMACs = 1.2233% MACs, 39.89 MFLOPS = 0.5946% FLOPs\n",
      "            (0): Conv2d(6 K = 0.1495% Params, 18.82 MMACs = 1.2233% MACs, 37.63 MFLOPS = 0.5946% FLOPs, 240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "            (1): BatchNorm2d(480 = 0.012% Params, 0 MACs = 0% MACs, 1.51 MFLOPS = 0% FLOPs, 240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 752.64 KFLOPS = 0% FLOPs, inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            5.05 K = 0.1258% Params, 19.2 KMACs = 0.0012% MACs, 792.08 KFLOPS = 0.0006% FLOPs\n",
      "            (avgpool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 752.64 KFLOPS = 0% FLOPs, output_size=1)\n",
      "            (fc1): Conv2d(2.41 K = 0.06% Params, 9.6 KMACs = 0.0006% MACs, 19.24 KFLOPS = 0.0003% FLOPs, 240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(2.64 K = 0.0658% Params, 9.6 KMACs = 0.0006% MACs, 20.16 KFLOPS = 0.0003% FLOPs, 10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 40 FLOPS = 0% FLOPs, inplace=True)\n",
      "            (scale_activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            9.68 K = 0.2412% Params, 30.11 MMACs = 1.9572% MACs, 60.46 MFLOPS = 0.9514% FLOPs\n",
      "            (0): Conv2d(9.6 K = 0.2392% Params, 30.11 MMACs = 1.9572% MACs, 60.21 MFLOPS = 0.9514% FLOPs, 240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(80 = 0.002% Params, 0 MACs = 0% MACs, 250.88 KFLOPS = 0% FLOPs, 40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      242.93 K = 6.0521% Params, 174.22 MMACs = 11.3265% MACs, 357.1 MFLOPS = 5.5057% FLOPs\n",
      "      (0): MBConv(\n",
      "        37.13 K = 0.925% Params, 46.87 MMACs = 3.0472% MACs, 96.88 MFLOPS = 1.4812% FLOPs\n",
      "        (block): Sequential(\n",
      "          37.13 K = 0.925% Params, 46.87 MMACs = 3.0472% MACs, 96.88 MFLOPS = 1.4812% FLOPs\n",
      "          (0): Conv2dNormActivation(\n",
      "            10.08 K = 0.2511% Params, 30.11 MMACs = 1.9572% MACs, 62.47 MFLOPS = 0.9514% FLOPs\n",
      "            (0): Conv2d(9.6 K = 0.2392% Params, 30.11 MMACs = 1.9572% MACs, 60.21 MFLOPS = 0.9514% FLOPs, 40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(480 = 0.012% Params, 0 MACs = 0% MACs, 1.51 MFLOPS = 0% FLOPs, 240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 752.64 KFLOPS = 0% FLOPs, inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            2.64 K = 0.0658% Params, 1.69 MMACs = 0.1101% MACs, 3.95 MFLOPS = 0.0535% FLOPs\n",
      "            (0): Conv2d(2.16 K = 0.0538% Params, 1.69 MMACs = 0.1101% MACs, 3.39 MFLOPS = 0.0535% FLOPs, 240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "            (1): BatchNorm2d(480 = 0.012% Params, 0 MACs = 0% MACs, 376.32 KFLOPS = 0% FLOPs, 240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 188.16 KFLOPS = 0% FLOPs, inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            5.05 K = 0.1258% Params, 19.2 KMACs = 0.0012% MACs, 227.6 KFLOPS = 0.0006% FLOPs\n",
      "            (avgpool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 188.16 KFLOPS = 0% FLOPs, output_size=1)\n",
      "            (fc1): Conv2d(2.41 K = 0.06% Params, 9.6 KMACs = 0.0006% MACs, 19.24 KFLOPS = 0.0003% FLOPs, 240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(2.64 K = 0.0658% Params, 9.6 KMACs = 0.0006% MACs, 20.16 KFLOPS = 0.0003% FLOPs, 10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 40 FLOPS = 0% FLOPs, inplace=True)\n",
      "            (scale_activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            19.36 K = 0.4823% Params, 15.05 MMACs = 0.9786% MACs, 30.23 MFLOPS = 0.4757% FLOPs\n",
      "            (0): Conv2d(19.2 K = 0.4783% Params, 15.05 MMACs = 0.9786% MACs, 30.11 MFLOPS = 0.4757% FLOPs, 240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(160 = 0.004% Params, 0 MACs = 0% MACs, 125.44 KFLOPS = 0% FLOPs, 80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        102.9 K = 2.5636% Params, 63.67 MMACs = 4.1397% MACs, 130.11 MFLOPS = 2.0122% FLOPs\n",
      "        (block): Sequential(\n",
      "          102.9 K = 2.5636% Params, 63.67 MMACs = 4.1397% MACs, 130.11 MFLOPS = 2.0122% FLOPs\n",
      "          (0): Conv2dNormActivation(\n",
      "            39.36 K = 0.9806% Params, 30.11 MMACs = 1.9572% MACs, 61.34 MFLOPS = 0.9514% FLOPs\n",
      "            (0): Conv2d(38.4 K = 0.9567% Params, 30.11 MMACs = 1.9572% MACs, 60.21 MFLOPS = 0.9514% FLOPs, 80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(960 = 0.0239% Params, 0 MACs = 0% MACs, 752.64 KFLOPS = 0% FLOPs, 480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 376.32 KFLOPS = 0% FLOPs, inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            5.28 K = 0.1315% Params, 3.39 MMACs = 0.2202% MACs, 7.9 MFLOPS = 0.107% FLOPs\n",
      "            (0): Conv2d(4.32 K = 0.1076% Params, 3.39 MMACs = 0.2202% MACs, 6.77 MFLOPS = 0.107% FLOPs, 480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "            (1): BatchNorm2d(960 = 0.0239% Params, 0 MACs = 0% MACs, 752.64 KFLOPS = 0% FLOPs, 480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 376.32 KFLOPS = 0% FLOPs, inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            19.7 K = 0.4908% Params, 76.8 KMACs = 0.005% MACs, 532 KFLOPS = 0.0024% FLOPs\n",
      "            (avgpool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 376.32 KFLOPS = 0% FLOPs, output_size=1)\n",
      "            (fc1): Conv2d(9.62 K = 0.2397% Params, 38.4 KMACs = 0.0025% MACs, 76.88 KFLOPS = 0.0012% FLOPs, 480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(10.08 K = 0.2511% Params, 38.4 KMACs = 0.0025% MACs, 78.72 KFLOPS = 0.0012% FLOPs, 20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 80 FLOPS = 0% FLOPs, inplace=True)\n",
      "            (scale_activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            38.56 K = 0.9606% Params, 30.11 MMACs = 1.9572% MACs, 60.34 MFLOPS = 0.9514% FLOPs\n",
      "            (0): Conv2d(38.4 K = 0.9567% Params, 30.11 MMACs = 1.9572% MACs, 60.21 MFLOPS = 0.9514% FLOPs, 480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(160 = 0.004% Params, 0 MACs = 0% MACs, 125.44 KFLOPS = 0% FLOPs, 80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
      "      )\n",
      "      (2): MBConv(\n",
      "        102.9 K = 2.5636% Params, 63.67 MMACs = 4.1397% MACs, 130.11 MFLOPS = 2.0122% FLOPs\n",
      "        (block): Sequential(\n",
      "          102.9 K = 2.5636% Params, 63.67 MMACs = 4.1397% MACs, 130.11 MFLOPS = 2.0122% FLOPs\n",
      "          (0): Conv2dNormActivation(\n",
      "            39.36 K = 0.9806% Params, 30.11 MMACs = 1.9572% MACs, 61.34 MFLOPS = 0.9514% FLOPs\n",
      "            (0): Conv2d(38.4 K = 0.9567% Params, 30.11 MMACs = 1.9572% MACs, 60.21 MFLOPS = 0.9514% FLOPs, 80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(960 = 0.0239% Params, 0 MACs = 0% MACs, 752.64 KFLOPS = 0% FLOPs, 480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 376.32 KFLOPS = 0% FLOPs, inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            5.28 K = 0.1315% Params, 3.39 MMACs = 0.2202% MACs, 7.9 MFLOPS = 0.107% FLOPs\n",
      "            (0): Conv2d(4.32 K = 0.1076% Params, 3.39 MMACs = 0.2202% MACs, 6.77 MFLOPS = 0.107% FLOPs, 480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "            (1): BatchNorm2d(960 = 0.0239% Params, 0 MACs = 0% MACs, 752.64 KFLOPS = 0% FLOPs, 480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 376.32 KFLOPS = 0% FLOPs, inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            19.7 K = 0.4908% Params, 76.8 KMACs = 0.005% MACs, 532 KFLOPS = 0.0024% FLOPs\n",
      "            (avgpool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 376.32 KFLOPS = 0% FLOPs, output_size=1)\n",
      "            (fc1): Conv2d(9.62 K = 0.2397% Params, 38.4 KMACs = 0.0025% MACs, 76.88 KFLOPS = 0.0012% FLOPs, 480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(10.08 K = 0.2511% Params, 38.4 KMACs = 0.0025% MACs, 78.72 KFLOPS = 0.0012% FLOPs, 20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 80 FLOPS = 0% FLOPs, inplace=True)\n",
      "            (scale_activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            38.56 K = 0.9606% Params, 30.11 MMACs = 1.9572% MACs, 60.34 MFLOPS = 0.9514% FLOPs\n",
      "            (0): Conv2d(38.4 K = 0.9567% Params, 30.11 MMACs = 1.9572% MACs, 60.21 MFLOPS = 0.9514% FLOPs, 480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(160 = 0.004% Params, 0 MACs = 0% MACs, 125.44 KFLOPS = 0% FLOPs, 80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      543.15 K = 13.5315% Params, 344.41 MMACs = 22.3909% MACs, 699.36 MFLOPS = 10.884% FLOPs\n",
      "      (0): MBConv(\n",
      "        126 K = 3.1391% Params, 81.74 MMACs = 5.314% MACs, 166.29 MFLOPS = 2.5831% FLOPs\n",
      "        (block): Sequential(\n",
      "          126 K = 3.1391% Params, 81.74 MMACs = 5.314% MACs, 166.29 MFLOPS = 2.5831% FLOPs\n",
      "          (0): Conv2dNormActivation(\n",
      "            39.36 K = 0.9806% Params, 30.11 MMACs = 1.9572% MACs, 61.34 MFLOPS = 0.9514% FLOPs\n",
      "            (0): Conv2d(38.4 K = 0.9567% Params, 30.11 MMACs = 1.9572% MACs, 60.21 MFLOPS = 0.9514% FLOPs, 80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(960 = 0.0239% Params, 0 MACs = 0% MACs, 752.64 KFLOPS = 0% FLOPs, 480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 376.32 KFLOPS = 0% FLOPs, inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            12.96 K = 0.3229% Params, 9.41 MMACs = 0.6116% MACs, 19.94 MFLOPS = 0.2973% FLOPs\n",
      "            (0): Conv2d(12 K = 0.299% Params, 9.41 MMACs = 0.6116% MACs, 18.82 MFLOPS = 0.2973% FLOPs, 480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
      "            (1): BatchNorm2d(960 = 0.0239% Params, 0 MACs = 0% MACs, 752.64 KFLOPS = 0% FLOPs, 480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 376.32 KFLOPS = 0% FLOPs, inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            19.7 K = 0.4908% Params, 76.8 KMACs = 0.005% MACs, 532 KFLOPS = 0.0024% FLOPs\n",
      "            (avgpool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 376.32 KFLOPS = 0% FLOPs, output_size=1)\n",
      "            (fc1): Conv2d(9.62 K = 0.2397% Params, 38.4 KMACs = 0.0025% MACs, 76.88 KFLOPS = 0.0012% FLOPs, 480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(10.08 K = 0.2511% Params, 38.4 KMACs = 0.0025% MACs, 78.72 KFLOPS = 0.0012% FLOPs, 20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 80 FLOPS = 0% FLOPs, inplace=True)\n",
      "            (scale_activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            53.98 K = 1.3449% Params, 42.15 MMACs = 2.7401% MACs, 84.47 MFLOPS = 1.3319% FLOPs\n",
      "            (0): Conv2d(53.76 K = 1.3393% Params, 42.15 MMACs = 2.7401% MACs, 84.3 MFLOPS = 1.3319% FLOPs, 480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(224 = 0.0056% Params, 0 MACs = 0% MACs, 175.62 KFLOPS = 0% FLOPs, 112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        208.57 K = 5.1962% Params, 131.34 MMACs = 8.5385% MACs, 266.54 MFLOPS = 4.1504% FLOPs\n",
      "        (block): Sequential(\n",
      "          208.57 K = 5.1962% Params, 131.34 MMACs = 8.5385% MACs, 266.54 MFLOPS = 4.1504% FLOPs\n",
      "          (0): Conv2dNormActivation(\n",
      "            76.61 K = 1.9085% Params, 59.01 MMACs = 3.8362% MACs, 119.59 MFLOPS = 1.8647% FLOPs\n",
      "            (0): Conv2d(75.26 K = 1.8751% Params, 59.01 MMACs = 3.8362% MACs, 118.01 MFLOPS = 1.8647% FLOPs, 112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1.34 K = 0.0335% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0% FLOPs, 672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 526.85 KFLOPS = 0% FLOPs, inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            18.14 K = 0.452% Params, 13.17 MMACs = 0.8563% MACs, 27.92 MFLOPS = 0.4162% FLOPs\n",
      "            (0): Conv2d(16.8 K = 0.4185% Params, 13.17 MMACs = 0.8563% MACs, 26.34 MFLOPS = 0.4162% FLOPs, 672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "            (1): BatchNorm2d(1.34 K = 0.0335% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0% FLOPs, 672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 526.85 KFLOPS = 0% FLOPs, inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            38.33 K = 0.955% Params, 150.53 KMACs = 0.0098% MACs, 830.82 KFLOPS = 0.0048% FLOPs\n",
      "            (avgpool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 526.85 KFLOPS = 0% FLOPs, output_size=1)\n",
      "            (fc1): Conv2d(18.84 K = 0.4695% Params, 75.26 KMACs = 0.0049% MACs, 150.64 KFLOPS = 0.0024% FLOPs, 672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(19.49 K = 0.4855% Params, 75.26 KMACs = 0.0049% MACs, 153.22 KFLOPS = 0.0024% FLOPs, 28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 112 FLOPS = 0% FLOPs, inplace=True)\n",
      "            (scale_activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            75.49 K = 1.8806% Params, 59.01 MMACs = 3.8362% MACs, 118.19 MFLOPS = 1.8647% FLOPs\n",
      "            (0): Conv2d(75.26 K = 1.8751% Params, 59.01 MMACs = 3.8362% MACs, 118.01 MFLOPS = 1.8647% FLOPs, 672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(224 = 0.0056% Params, 0 MACs = 0% MACs, 175.62 KFLOPS = 0% FLOPs, 112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
      "      )\n",
      "      (2): MBConv(\n",
      "        208.57 K = 5.1962% Params, 131.34 MMACs = 8.5385% MACs, 266.54 MFLOPS = 4.1504% FLOPs\n",
      "        (block): Sequential(\n",
      "          208.57 K = 5.1962% Params, 131.34 MMACs = 8.5385% MACs, 266.54 MFLOPS = 4.1504% FLOPs\n",
      "          (0): Conv2dNormActivation(\n",
      "            76.61 K = 1.9085% Params, 59.01 MMACs = 3.8362% MACs, 119.59 MFLOPS = 1.8647% FLOPs\n",
      "            (0): Conv2d(75.26 K = 1.8751% Params, 59.01 MMACs = 3.8362% MACs, 118.01 MFLOPS = 1.8647% FLOPs, 112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1.34 K = 0.0335% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0% FLOPs, 672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 526.85 KFLOPS = 0% FLOPs, inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            18.14 K = 0.452% Params, 13.17 MMACs = 0.8563% MACs, 27.92 MFLOPS = 0.4162% FLOPs\n",
      "            (0): Conv2d(16.8 K = 0.4185% Params, 13.17 MMACs = 0.8563% MACs, 26.34 MFLOPS = 0.4162% FLOPs, 672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "            (1): BatchNorm2d(1.34 K = 0.0335% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0% FLOPs, 672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 526.85 KFLOPS = 0% FLOPs, inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            38.33 K = 0.955% Params, 150.53 KMACs = 0.0098% MACs, 830.82 KFLOPS = 0.0048% FLOPs\n",
      "            (avgpool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 526.85 KFLOPS = 0% FLOPs, output_size=1)\n",
      "            (fc1): Conv2d(18.84 K = 0.4695% Params, 75.26 KMACs = 0.0049% MACs, 150.64 KFLOPS = 0.0024% FLOPs, 672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(19.49 K = 0.4855% Params, 75.26 KMACs = 0.0049% MACs, 153.22 KFLOPS = 0.0024% FLOPs, 28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 112 FLOPS = 0% FLOPs, inplace=True)\n",
      "            (scale_activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            75.49 K = 1.8806% Params, 59.01 MMACs = 3.8362% MACs, 118.19 MFLOPS = 1.8647% FLOPs\n",
      "            (0): Conv2d(75.26 K = 1.8751% Params, 59.01 MMACs = 3.8362% MACs, 118.01 MFLOPS = 1.8647% FLOPs, 672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(224 = 0.0056% Params, 0 MACs = 0% MACs, 175.62 KFLOPS = 0% FLOPs, 112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      2.03 M = 50.4826% Params, 366.11 MMACs = 23.8019% MACs, 739.39 MFLOPS = 11.5698% FLOPs\n",
      "      (0): MBConv(\n",
      "        262.49 K = 6.5395% Params, 87.74 MMACs = 5.7041% MACs, 177.66 MFLOPS = 2.7727% FLOPs\n",
      "        (block): Sequential(\n",
      "          262.49 K = 6.5395% Params, 87.74 MMACs = 5.7041% MACs, 177.66 MFLOPS = 2.7727% FLOPs\n",
      "          (0): Conv2dNormActivation(\n",
      "            76.61 K = 1.9085% Params, 59.01 MMACs = 3.8362% MACs, 119.59 MFLOPS = 1.8647% FLOPs\n",
      "            (0): Conv2d(75.26 K = 1.8751% Params, 59.01 MMACs = 3.8362% MACs, 118.01 MFLOPS = 1.8647% FLOPs, 112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1.34 K = 0.0335% Params, 0 MACs = 0% MACs, 1.05 MFLOPS = 0% FLOPs, 672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 526.85 KFLOPS = 0% FLOPs, inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            18.14 K = 0.452% Params, 3.29 MMACs = 0.2141% MACs, 6.98 MFLOPS = 0.1041% FLOPs\n",
      "            (0): Conv2d(16.8 K = 0.4185% Params, 3.29 MMACs = 0.2141% MACs, 6.59 MFLOPS = 0.1041% FLOPs, 672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "            (1): BatchNorm2d(1.34 K = 0.0335% Params, 0 MACs = 0% MACs, 263.42 KFLOPS = 0% FLOPs, 672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 131.71 KFLOPS = 0% FLOPs, inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            38.33 K = 0.955% Params, 150.53 KMACs = 0.0098% MACs, 435.68 KFLOPS = 0.0048% FLOPs\n",
      "            (avgpool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 131.71 KFLOPS = 0% FLOPs, output_size=1)\n",
      "            (fc1): Conv2d(18.84 K = 0.4695% Params, 75.26 KMACs = 0.0049% MACs, 150.64 KFLOPS = 0.0024% FLOPs, 672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(19.49 K = 0.4855% Params, 75.26 KMACs = 0.0049% MACs, 153.22 KFLOPS = 0.0024% FLOPs, 28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 112 FLOPS = 0% FLOPs, inplace=True)\n",
      "            (scale_activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            129.41 K = 3.224% Params, 25.29 MMACs = 1.6441% MACs, 50.65 MFLOPS = 0.7992% FLOPs\n",
      "            (0): Conv2d(129.02 K = 3.2144% Params, 25.29 MMACs = 1.6441% MACs, 50.58 MFLOPS = 0.7992% FLOPs, 672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(384 = 0.0096% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        587.95 K = 14.6477% Params, 92.79 MMACs = 6.0326% MACs, 187.24 MFLOPS = 2.9324% FLOPs\n",
      "        (block): Sequential(\n",
      "          587.95 K = 14.6477% Params, 92.79 MMACs = 6.0326% MACs, 187.24 MFLOPS = 2.9324% FLOPs\n",
      "          (0): Conv2dNormActivation(\n",
      "            223.49 K = 5.5678% Params, 43.35 MMACs = 2.8184% MACs, 87.38 MFLOPS = 1.37% FLOPs\n",
      "            (0): Conv2d(221.18 K = 5.5104% Params, 43.35 MMACs = 2.8184% MACs, 86.7 MFLOPS = 1.37% FLOPs, 192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(2.3 K = 0.0574% Params, 0 MACs = 0% MACs, 451.58 KFLOPS = 0% FLOPs, 1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 225.79 KFLOPS = 0% FLOPs, inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            31.1 K = 0.7749% Params, 5.64 MMACs = 0.367% MACs, 11.97 MFLOPS = 0.1784% FLOPs\n",
      "            (0): Conv2d(28.8 K = 0.7175% Params, 5.64 MMACs = 0.367% MACs, 11.29 MFLOPS = 0.1784% FLOPs, 1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(2.3 K = 0.0574% Params, 0 MACs = 0% MACs, 451.58 KFLOPS = 0% FLOPs, 1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 225.79 KFLOPS = 0% FLOPs, inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            111.79 K = 2.7851% Params, 442.37 KMACs = 0.0288% MACs, 1.12 MFLOPS = 0.014% FLOPs\n",
      "            (avgpool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 225.79 KFLOPS = 0% FLOPs, output_size=1)\n",
      "            (fc1): Conv2d(55.34 K = 1.3788% Params, 221.18 KMACs = 0.0144% MACs, 442.56 KFLOPS = 0.007% FLOPs, 1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(56.45 K = 1.4063% Params, 221.18 KMACs = 0.0144% MACs, 446.98 KFLOPS = 0.007% FLOPs, 48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 192 FLOPS = 0% FLOPs, inplace=True)\n",
      "            (scale_activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            221.57 K = 5.5199% Params, 43.35 MMACs = 2.8184% MACs, 86.78 MFLOPS = 1.37% FLOPs\n",
      "            (0): Conv2d(221.18 K = 5.5104% Params, 43.35 MMACs = 2.8184% MACs, 86.7 MFLOPS = 1.37% FLOPs, 1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(384 = 0.0096% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
      "      )\n",
      "      (2): MBConv(\n",
      "        587.95 K = 14.6477% Params, 92.79 MMACs = 6.0326% MACs, 187.24 MFLOPS = 2.9324% FLOPs\n",
      "        (block): Sequential(\n",
      "          587.95 K = 14.6477% Params, 92.79 MMACs = 6.0326% MACs, 187.24 MFLOPS = 2.9324% FLOPs\n",
      "          (0): Conv2dNormActivation(\n",
      "            223.49 K = 5.5678% Params, 43.35 MMACs = 2.8184% MACs, 87.38 MFLOPS = 1.37% FLOPs\n",
      "            (0): Conv2d(221.18 K = 5.5104% Params, 43.35 MMACs = 2.8184% MACs, 86.7 MFLOPS = 1.37% FLOPs, 192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(2.3 K = 0.0574% Params, 0 MACs = 0% MACs, 451.58 KFLOPS = 0% FLOPs, 1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 225.79 KFLOPS = 0% FLOPs, inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            31.1 K = 0.7749% Params, 5.64 MMACs = 0.367% MACs, 11.97 MFLOPS = 0.1784% FLOPs\n",
      "            (0): Conv2d(28.8 K = 0.7175% Params, 5.64 MMACs = 0.367% MACs, 11.29 MFLOPS = 0.1784% FLOPs, 1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(2.3 K = 0.0574% Params, 0 MACs = 0% MACs, 451.58 KFLOPS = 0% FLOPs, 1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 225.79 KFLOPS = 0% FLOPs, inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            111.79 K = 2.7851% Params, 442.37 KMACs = 0.0288% MACs, 1.12 MFLOPS = 0.014% FLOPs\n",
      "            (avgpool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 225.79 KFLOPS = 0% FLOPs, output_size=1)\n",
      "            (fc1): Conv2d(55.34 K = 1.3788% Params, 221.18 KMACs = 0.0144% MACs, 442.56 KFLOPS = 0.007% FLOPs, 1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(56.45 K = 1.4063% Params, 221.18 KMACs = 0.0144% MACs, 446.98 KFLOPS = 0.007% FLOPs, 48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 192 FLOPS = 0% FLOPs, inplace=True)\n",
      "            (scale_activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            221.57 K = 5.5199% Params, 43.35 MMACs = 2.8184% MACs, 86.78 MFLOPS = 1.37% FLOPs\n",
      "            (0): Conv2d(221.18 K = 5.5104% Params, 43.35 MMACs = 2.8184% MACs, 86.7 MFLOPS = 1.37% FLOPs, 1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(384 = 0.0096% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
      "      )\n",
      "      (3): MBConv(\n",
      "        587.95 K = 14.6477% Params, 92.79 MMACs = 6.0326% MACs, 187.24 MFLOPS = 2.9324% FLOPs\n",
      "        (block): Sequential(\n",
      "          587.95 K = 14.6477% Params, 92.79 MMACs = 6.0326% MACs, 187.24 MFLOPS = 2.9324% FLOPs\n",
      "          (0): Conv2dNormActivation(\n",
      "            223.49 K = 5.5678% Params, 43.35 MMACs = 2.8184% MACs, 87.38 MFLOPS = 1.37% FLOPs\n",
      "            (0): Conv2d(221.18 K = 5.5104% Params, 43.35 MMACs = 2.8184% MACs, 86.7 MFLOPS = 1.37% FLOPs, 192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(2.3 K = 0.0574% Params, 0 MACs = 0% MACs, 451.58 KFLOPS = 0% FLOPs, 1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 225.79 KFLOPS = 0% FLOPs, inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            31.1 K = 0.7749% Params, 5.64 MMACs = 0.367% MACs, 11.97 MFLOPS = 0.1784% FLOPs\n",
      "            (0): Conv2d(28.8 K = 0.7175% Params, 5.64 MMACs = 0.367% MACs, 11.29 MFLOPS = 0.1784% FLOPs, 1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(2.3 K = 0.0574% Params, 0 MACs = 0% MACs, 451.58 KFLOPS = 0% FLOPs, 1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 225.79 KFLOPS = 0% FLOPs, inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            111.79 K = 2.7851% Params, 442.37 KMACs = 0.0288% MACs, 1.12 MFLOPS = 0.014% FLOPs\n",
      "            (avgpool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 225.79 KFLOPS = 0% FLOPs, output_size=1)\n",
      "            (fc1): Conv2d(55.34 K = 1.3788% Params, 221.18 KMACs = 0.0144% MACs, 442.56 KFLOPS = 0.007% FLOPs, 1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(56.45 K = 1.4063% Params, 221.18 KMACs = 0.0144% MACs, 446.98 KFLOPS = 0.007% FLOPs, 48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 192 FLOPS = 0% FLOPs, inplace=True)\n",
      "            (scale_activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            221.57 K = 5.5199% Params, 43.35 MMACs = 2.8184% MACs, 86.78 MFLOPS = 1.37% FLOPs\n",
      "            (0): Conv2d(221.18 K = 5.5104% Params, 43.35 MMACs = 2.8184% MACs, 86.7 MFLOPS = 1.37% FLOPs, 1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(384 = 0.0096% Params, 0 MACs = 0% MACs, 75.26 KFLOPS = 0% FLOPs, 192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      717.23 K = 17.8685% Params, 118.08 MMACs = 7.6767% MACs, 237.87 MFLOPS = 3.7315% FLOPs\n",
      "      (0): MBConv(\n",
      "        717.23 K = 17.8685% Params, 118.08 MMACs = 7.6767% MACs, 237.87 MFLOPS = 3.7315% FLOPs\n",
      "        (block): Sequential(\n",
      "          717.23 K = 17.8685% Params, 118.08 MMACs = 7.6767% MACs, 237.87 MFLOPS = 3.7315% FLOPs\n",
      "          (0): Conv2dNormActivation(\n",
      "            223.49 K = 5.5678% Params, 43.35 MMACs = 2.8184% MACs, 87.38 MFLOPS = 1.37% FLOPs\n",
      "            (0): Conv2d(221.18 K = 5.5104% Params, 43.35 MMACs = 2.8184% MACs, 86.7 MFLOPS = 1.37% FLOPs, 192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(2.3 K = 0.0574% Params, 0 MACs = 0% MACs, 451.58 KFLOPS = 0% FLOPs, 1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 225.79 KFLOPS = 0% FLOPs, inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            12.67 K = 0.3157% Params, 2.03 MMACs = 0.1321% MACs, 4.74 MFLOPS = 0.0642% FLOPs\n",
      "            (0): Conv2d(10.37 K = 0.2583% Params, 2.03 MMACs = 0.1321% MACs, 4.06 MFLOPS = 0.0642% FLOPs, 1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(2.3 K = 0.0574% Params, 0 MACs = 0% MACs, 451.58 KFLOPS = 0% FLOPs, 1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 225.79 KFLOPS = 0% FLOPs, inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            111.79 K = 2.7851% Params, 442.37 KMACs = 0.0288% MACs, 1.12 MFLOPS = 0.014% FLOPs\n",
      "            (avgpool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 225.79 KFLOPS = 0% FLOPs, output_size=1)\n",
      "            (fc1): Conv2d(55.34 K = 1.3788% Params, 221.18 KMACs = 0.0144% MACs, 442.56 KFLOPS = 0.007% FLOPs, 1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(56.45 K = 1.4063% Params, 221.18 KMACs = 0.0144% MACs, 446.98 KFLOPS = 0.007% FLOPs, 48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 192 FLOPS = 0% FLOPs, inplace=True)\n",
      "            (scale_activation): Sigmoid(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            369.28 K = 9.1999% Params, 72.25 MMACs = 4.6974% MACs, 144.63 MFLOPS = 2.2833% FLOPs\n",
      "            (0): Conv2d(368.64 K = 9.184% Params, 72.25 MMACs = 4.6974% MACs, 144.51 MFLOPS = 2.2833% FLOPs, 1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(640 = 0.0159% Params, 0 MACs = 0% MACs, 125.44 KFLOPS = 0% FLOPs, 320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (8): Conv2dNormActivation(\n",
      "      412.16 K = 10.2682% Params, 80.28 MMACs = 5.2193% MACs, 161.32 MFLOPS = 2.537% FLOPs\n",
      "      (0): Conv2d(409.6 K = 10.2044% Params, 80.28 MMACs = 5.2193% MACs, 160.56 MFLOPS = 2.537% FLOPs, 320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(2.56 K = 0.0638% Params, 0 MACs = 0% MACs, 501.76 KFLOPS = 0% FLOPs, 1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 250.88 KFLOPS = 0% FLOPs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(0 = 0% Params, 0 MACs = 0% MACs, 250.88 KFLOPS = 0% FLOPs, output_size=1)\n",
      "  (classifier): Sequential(\n",
      "    6.41 K = 0.1596% Params, 25.6 KMACs = 0.0017% MACs, 51.2 KFLOPS = 0.0008% FLOPs\n",
      "    (0): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=True)\n",
      "    (1): Linear(6.41 K = 0.1596% Params, 25.6 KMACs = 0.0017% MACs, 51.2 KFLOPS = 0.0008% FLOPs, in_features=1280, out_features=5, bias=True)\n",
      "  )\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "EfficientNetB0 FLOPs:3.1644 GFLOPS   MACs:1.5382 GMACs   Params:4.014 M \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load the pre-trained EfficientNet B0 model\n",
    "model = models.efficientnet_b0(pretrained=True)\n",
    "\n",
    "# Number of features in the last layer (bottleneck layer output size)\n",
    "num_ftrs = model.classifier[1].in_features\n",
    "\n",
    "# Replace the classifier head with a new one adjusted to 5 classes\n",
    "model.classifier[1] = nn.Linear(num_ftrs, 5)\n",
    "\n",
    "# Example of how to forward a sample through the model (assuming you have a transformed input)\n",
    "dummy_input = torch.randn(1, 3, 224, 224)  # Adjust size as necessary for your application\n",
    "\n",
    "flops, macs, params = calculate_flops(model=model, \n",
    "                                      input_shape=tuple(dummy_input.shape),\n",
    "                                      output_as_string=True,\n",
    "                                      output_precision=4)\n",
    "print(\"EfficientNetB0 FLOPs:%s   MACs:%s   Params:%s \\n\" %(flops, macs, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
