{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 2632847,
     "sourceType": "datasetVersion",
     "datasetId": 1589971
    }
   ],
   "dockerImageVersionId": 30673,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Config"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import json\n",
    "import os \n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.image import imread\n",
    "from math import log, ceil, sqrt\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchinfo\n",
    "import torch\n",
    "import wandb\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from typing import Dict\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_sched\n",
    "from torchinfo import summary\n",
    "\n",
    "class CONFIG:\n",
    "\n",
    "    ROOT_DIRECTORY = os.path.join(\"..\",\"data\", \"WLASL\")\n",
    "    JSON_FILE = \"WLASL_v0.3.json\"\n",
    "    NSLT_FILE = \"nslt_100.json\"\n",
    "    VIDEO_FOLDER = \"videos\"\n",
    "    #mean=[0.485, 0.456, 0.406]\n",
    "    #std=[0.229, 0.224, 0.225]\n",
    "    mean = [0, 0, 0]\n",
    "    std = [1,1,1]\n",
    "    DEBUG = True\n",
    "    \n",
    "    P_OF_TRANSFORM = 0.9\n",
    "    P_OF_TRANSFORM_COLOR = 0.2\n",
    "    \n",
    "    SHIFT_LIMIT=0.1\n",
    "    SCALE_LIMIT=0.1\n",
    "    ROTATE_LIMIT=10\n",
    "    \n",
    "    # set to small, when prototyping, or 0 when deploying to cloud or PC with loads of RAM\n",
    "    DATA_LIMIT = 100\n",
    "    FRAME_SIZE = 30\n",
    "    \n",
    "    \n",
    "    DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    NUM_WORKERS = 0\n",
    "    ROUND_NUMBER = 3\n",
    "    TASK_NAME = \"WLASL_RGB\"\n",
    "\n",
    "    BATCH_SIZE\n",
    "    PORTION_OF_DATA_FOR_TRAINING = 0.8\n",
    "    \n",
    "torch.backends.cudnn.benchmark = True\n",
    "print(f\"Device : {CONFIG.DEVICE}\")    "
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2024-04-03T19:45:37.657965Z",
     "iopub.execute_input": "2024-04-03T19:45:37.658372Z",
     "iopub.status.idle": "2024-04-03T19:45:37.666248Z",
     "shell.execute_reply.started": "2024-04-03T19:45:37.658340Z",
     "shell.execute_reply": "2024-04-03T19:45:37.665056Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-07T14:43:29.072701400Z",
     "start_time": "2024-04-07T14:43:29.050228800Z"
    }
   },
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cuda\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Read the necessary files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dataset consists of 2038 videos\n",
      "there are 11980 videos in total\n"
     ]
    }
   ],
   "source": [
    "# setup the paths\n",
    "video_path = os.path.join(CONFIG.ROOT_DIRECTORY, CONFIG.VIDEO_FOLDER)\n",
    "dataset_description = os.path.join(CONFIG.ROOT_DIRECTORY)\n",
    "\n",
    "# load the filepaths for videos\n",
    "video_paths = [os.path.join(video_path, file) for file in os.listdir(video_path)]\n",
    "\n",
    "# load the dataset config json\n",
    "config_json = None\n",
    "with open(os.path.join(CONFIG.ROOT_DIRECTORY, CONFIG.JSON_FILE)) as f:\n",
    "    config_json = json.load(f)\n",
    "    \n",
    "# load the dataset json\n",
    "dataset_json = None\n",
    "with open(os.path.join(CONFIG.ROOT_DIRECTORY, CONFIG.NSLT_FILE)) as f:\n",
    "    dataset_json = json.load(f)\n",
    "\n",
    "print(f\"the dataset consists of {len(dataset_json.keys())} videos\")\n",
    "print(f\"there are {len(video_paths)} videos in total\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T13:33:14.924943800Z",
     "start_time": "2024-04-07T13:33:14.733791700Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Util functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def Format(x, rnd_digits=3):\n",
    "    prefix = \"\"\n",
    "    if 1024 <= x < 1024**2:\n",
    "        prefix = \"K\"\n",
    "        x /= 1024\n",
    "    elif 1024**2 <= x < 1024**3:\n",
    "        prefix = \"M\"\n",
    "        x /= 1024**2\n",
    "    elif x >= 1024**3:\n",
    "        prefix = \"G\"\n",
    "        x /= 1024**3\n",
    "    \n",
    "    return f\"{round(x,rnd_digits)}{prefix}B\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T13:33:42.241247600Z",
     "start_time": "2024-04-07T13:33:42.235237700Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Regular Dataset : store the data on disk and load it from there"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os \n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "        \n",
    "class SignRecognitionDataset(Dataset):\n",
    "\n",
    "    def __init__(self, max_start : int, max_end) -> None:\n",
    "        # setup the paths\n",
    "        video_path = os.path.join(CONFIG.ROOT_DIRECTORY, CONFIG.VIDEO_FOLDER)\n",
    "        dataset_description = os.path.join(CONFIG.ROOT_DIRECTORY)\n",
    "\n",
    "        # load the filepaths for videos\n",
    "        self.video_paths = [os.path.join(video_path, file) for file in os.listdir(video_path)]\n",
    "\n",
    "        # load the dataset config json\n",
    "        self.config_json = None\n",
    "        with open(os.path.join(CONFIG.ROOT_DIRECTORY, CONFIG.JSON_FILE)) as f:\n",
    "            self.config_json = json.load(f)\n",
    "\n",
    "        # load the dataset json\n",
    "        self.dataset_json = None\n",
    "        with open(os.path.join(CONFIG.ROOT_DIRECTORY, CONFIG.NSLT_FILE)) as f:\n",
    "            self.dataset_json = json.load(f)\n",
    "        \n",
    "        self.videos_paths = []\n",
    "        self.paths_not_found = []\n",
    "        self.labels = []\n",
    "        self.start_frames = []\n",
    "        self.end_frames = []\n",
    "        \n",
    "\n",
    "        for el in tqdm(dataset_json.items()):\n",
    "            video_id, properties = el[0], el[1]\n",
    "            path = os.path.join(video_path, video_id + \".mp4\")\n",
    "            \n",
    "            if not os.path.exists(path):\n",
    "                self.paths_not_found.append(path)\n",
    "                continue\n",
    "\n",
    "            subset = properties[\"subset\"]\n",
    "            label, start, end = properties[\"action\"]\n",
    "            \n",
    "            if start > max_start:\n",
    "                continue\n",
    "                \n",
    "            if end > max_end:\n",
    "                continue\n",
    "            \n",
    "            self.videos_paths.append(path)\n",
    "            self.labels.append(label)\n",
    "            self.start_frames.append(start)\n",
    "            self.end_frames.append(end)\n",
    "    \n",
    "        self.videos_paths = np.array(self.video_paths)\n",
    "        self.paths_not_found = np.array(self.paths_not_found)\n",
    "        self.labels = np.array(self.labels)\n",
    "        self.start_frames = np.array(self.start_frames)\n",
    "        self.end_frames = np.array(self.end_frames)\n",
    "\n",
    "        self.unique_labels = np.unique(self.labels)\n",
    "        \n",
    "    \n",
    "    def preprocess_trajectory(self, traj : List[np.ndarray]):\n",
    "        return traj\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.videos_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.videos_paths[idx], self.labels[idx]\n",
    "        trajectory = SignRecognitionDataset.get_video(path)\n",
    "        \n",
    "        return self.preprocess_trajectory(trajectory), label\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def get_video(video_path : str) -> List[np.ndarray]:\n",
    "\n",
    "        if not os.path.exists(video_path):\n",
    "            return None\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            return None\n",
    "\n",
    "        frames = []\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame_array = np.array(frame)\n",
    "                frames.append(cv2.cvtColor(frame_array, cv2.COLOR_BGR2RGB))\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        return np.array(frames)\n",
    "\n",
    "    @staticmethod\n",
    "    def rescale_video(frames : np.ndarray, desired_shape) -> np.ndarray:\n",
    "        refined = []\n",
    "        for img in frames: \n",
    "            y, x, c = img.shape\n",
    "            cropped = img[:, (x // 2 - y//2) : (x // 2 + y//2), :]\n",
    "            scaled = cv2.resize(cropped, desired_shape)\n",
    "            refined.append(scaled)\n",
    "    \n",
    "        return np.array(refined)\n",
    "        \n",
    "    \n",
    "ds = SignRecognitionDataset(max_start=1, max_end=150)\n",
    "print(f\"after filtering : size of dataset={len(ds)}\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-03T19:57:22.011358Z",
     "iopub.execute_input": "2024-04-03T19:57:22.011877Z",
     "iopub.status.idle": "2024-04-03T19:57:23.972773Z",
     "shell.execute_reply.started": "2024-04-03T19:57:22.011847Z",
     "shell.execute_reply": "2024-04-03T19:57:23.971801Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-07T14:03:46.206793200Z",
     "start_time": "2024-04-07T14:03:45.914517800Z"
    }
   },
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2038/2038 [00:00<00:00, 35362.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after filtering : size of dataset=11980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cached Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2038/2038 [00:00<00:00, 45848.73it/s]\n",
      "Loading/scaling trajectories: 100%|██████████| 32/32 [00:07<00:00,  4.46it/s]\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "from collections import Counter\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "class SignRecognitionDatasetCached(SignRecognitionDataset):\n",
    "\n",
    "    def __init__(self, max_start: int, max_end, per_image_transform=None,\n",
    "                 scaled_resolution : Tuple[int]= (240, 240),\n",
    "                 frame_size:int=CONFIG.FRAME_SIZE,\n",
    "                 data_limit : int = CONFIG.DATA_LIMIT,\n",
    "                 by_size=True) -> None:\n",
    "        super().__init__(max_start, max_end)\n",
    "        self.scaled_resolution = scaled_resolution\n",
    "        self.DATA_LIMIT = data_limit\n",
    "        self.by_size = by_size\n",
    "        self.cache_data()\n",
    "        self.per_image_transform = per_image_transform\n",
    "        self.FRAME_SIZE = frame_size\n",
    "        \n",
    "        self.keywords = [\"image\" ] + list(str(i) for i in range(frame_size-1))\n",
    "        \n",
    "    def preprocess_trajectory(self, traj : List[np.ndarray]):\n",
    "        return SignRecognitionDataset.rescale_video(traj, self.scaled_resolution)\n",
    "\n",
    "    def permutate(self):\n",
    "        l = len(self.videos_paths)\n",
    "        mask = np.arange(l)\n",
    "        np.random.shuffle(mask)\n",
    "        \n",
    "        self.videos_paths = np.array(self.videos_paths)[mask]\n",
    "        self.labels = np.array(self.labels)[mask]\n",
    "        self.start_frames = np.array(self.start_frames)[mask]\n",
    "        self.end_frames = np.array(self.end_frames)[mask]\n",
    "        \n",
    "    def sort_by_size(self):\n",
    "        c = Counter(self.labels)\n",
    "        _mask = sorted([ (10000 * c[l] + l, i) for i, l in enumerate(self.labels)])[::-1]\n",
    "        mask_by_size = np.array([el[1] for el in _mask])\n",
    "\n",
    "        self.video_paths        = np.array(self.video_paths)[mask_by_size]\n",
    "        self.labels             = np.array(self.labels)[mask_by_size]\n",
    "        self.start_frames       = np.array(self.start_frames)[mask_by_size]\n",
    "        self.end_frames         = np.array(self.end_frames)[mask_by_size]\n",
    "\n",
    " \n",
    "    def cache_data(self):\n",
    "        if not self.by_size:\n",
    "            self.permutate()\n",
    "        else:\n",
    "            self.sort_by_size()\n",
    "        \n",
    "        self.cached_data_x = []\n",
    "        self.cached_data_y = []\n",
    "        \n",
    "        pbar = tqdm(range(self.DATA_LIMIT))\n",
    "        pbar.set_description(\"Loading/scaling trajectories\")\n",
    "        for i in pbar:\n",
    "            trajectory = SignRecognitionDataset.get_video(self.videos_paths[i])            \n",
    "            trajectory = self.preprocess_trajectory(trajectory)\n",
    "\n",
    "            self.cached_data_x.append(trajectory)\n",
    "            self.cached_data_y.append(self.labels[i])\n",
    "        \n",
    "        self.unique_labels = np.unique(self.cached_data_y)\n",
    "        self.label_to_new_id = {self.unique_labels[i] : i for i in range(len(self.unique_labels))}\n",
    "        self.new_id_to_label = {v : k for k,v in self.label_to_new_id.items()}\n",
    "        \n",
    "        for i in range(len(self.cached_data_y)):\n",
    "            self.cached_data_y[i] = self.label_to_new_id[self.cached_data_y[i]]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.cached_data_x)\n",
    "\n",
    "    def crop_video(self, trajectory : np.array) -> np.array:\n",
    "        cropped = trajectory\n",
    "        \n",
    "        if self.FRAME_SIZE != 0:\n",
    "            frame_size = len(trajectory)\n",
    "            start = 0 \n",
    "            \n",
    "            if frame_size > self.FRAME_SIZE:\n",
    "                start = np.random.randint(0, frame_size - self.FRAME_SIZE)\n",
    "            cropped = trajectory[start: (start + self.FRAME_SIZE)]\n",
    "                    \n",
    "            if len(cropped) < self.FRAME_SIZE:\n",
    "                necessary = self.FRAME_SIZE - len(cropped)\n",
    "                t, h, w, c = trajectory.shape\n",
    "                cropped = np.concatenate([cropped, np.zeros((necessary, h, w, c))], axis= 0)\n",
    "                \n",
    "            return cropped\n",
    "                        \n",
    "        return trajectory\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        trajectory, label = self.crop_video(self.cached_data_x[idx]), self.cached_data_y[idx]\n",
    "\n",
    "        if self.per_image_transform is not None:            \n",
    "            frames = {self.keywords[i] : frame for i, frame in enumerate(trajectory)}\n",
    "            processing = self.per_image_transform(**frames)       \n",
    "\n",
    "            return np.stack([processing[kw] for kw in self.keywords]), label    \n",
    "        return torch.Tensor(trajectory), label\n",
    "    \n",
    "\n",
    "transform = A.Compose(\n",
    "    [\n",
    "        A.Normalize(mean=CONFIG.mean, std=CONFIG.std),\n",
    "        A.HorizontalFlip(p=CONFIG.P_OF_TRANSFORM),\n",
    "        A.ShiftScaleRotate(p=CONFIG.P_OF_TRANSFORM, shift_limit=0.1, scale_limit=0.1, rotate_limit=10),\n",
    "        #A.RandomBrightnessContrast(p=0.2),\n",
    "        #A.RGBShift(p=0.2),\n",
    "        ToTensorV2()\n",
    "    ],\n",
    "    additional_targets={str(i) : \"image\" for i in range(CONFIG.FRAME_SIZE)}\n",
    ")\n",
    "    \n",
    "\n",
    "ds = SignRecognitionDatasetCached(max_start=1, max_end=150,\n",
    "                                  data_limit=CONFIG.DATA_LIMIT, per_image_transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T14:04:04.210388700Z",
     "start_time": "2024-04-07T14:03:50.678980300Z"
    }
   },
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from collections import Counter\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "class SignRecognitionDatasetCachedMHI(SignRecognitionDataset):\n",
    "\n",
    "    def __init__(self, max_start: int, max_end, \n",
    "                 per_image_transform=None,\n",
    "                 after_MHI_transform=None,\n",
    "                 scaled_resolution : Tuple[int]= (240, 240),\n",
    "                 frame_size:int=CONFIG.FRAME_SIZE,\n",
    "                 data_limit : int = CONFIG.DATA_LIMIT,\n",
    "                 decay : float = 0.7,\n",
    "                 threshold_method : str = \"regular\",\n",
    "                 threshold_val : int = 0.3 * 255,\n",
    "                 by_size=True) -> None:\n",
    "        super().__init__(max_start, max_end)\n",
    "        self.scaled_resolution = scaled_resolution\n",
    "        self.DATA_LIMIT = data_limit\n",
    "        self.by_size = by_size\n",
    "        self.cache_data()\n",
    "        self.per_image_transform = per_image_transform\n",
    "        self.after_MHI_transform = after_MHI_transform\n",
    "        self.FRAME_SIZE = frame_size\n",
    "        \n",
    "        self.keywords = [\"image\" ] + list(str(i) for i in range(frame_size-1))\n",
    "        \n",
    "    def preprocess_trajectory(self, traj : List[np.ndarray]):\n",
    "        return SignRecognitionDataset.rescale_video(traj, self.scaled_resolution)\n",
    "\n",
    "    def permutate(self):\n",
    "        l = len(self.videos_paths)\n",
    "        mask = np.arange(l)\n",
    "        np.random.shuffle(mask)\n",
    "        \n",
    "        self.videos_paths = np.array(self.videos_paths)[mask]\n",
    "        self.labels = np.array(self.labels)[mask]\n",
    "        self.start_frames = np.array(self.start_frames)[mask]\n",
    "        self.end_frames = np.array(self.end_frames)[mask]\n",
    "        \n",
    "    def sort_by_size(self):\n",
    "        c = Counter(self.labels)\n",
    "        _mask = sorted([ (10000 * c[l] + l, i) for i, l in enumerate(self.labels)])[::-1]\n",
    "        mask_by_size = np.array([el[1] for el in _mask])\n",
    "\n",
    "        self.video_paths        = np.array(self.video_paths)[mask_by_size]\n",
    "        self.labels             = np.array(self.labels)[mask_by_size]\n",
    "        self.start_frames       = np.array(self.start_frames)[mask_by_size]\n",
    "        self.end_frames         = np.array(self.end_frames)[mask_by_size]\n",
    "\n",
    " \n",
    "    def cache_data(self):\n",
    "        if not self.by_size:\n",
    "            self.permutate()\n",
    "        else:\n",
    "            self.sort_by_size()\n",
    "        \n",
    "        self.cached_data_x = []\n",
    "        self.cached_data_y = []\n",
    "        \n",
    "        pbar = tqdm(range(self.DATA_LIMIT))\n",
    "        pbar.set_description(\"Loading/scaling trajectories\")\n",
    "        for i in pbar:\n",
    "            trajectory = SignRecognitionDataset.get_video(self.videos_paths[i])            \n",
    "            trajectory = self.preprocess_trajectory(trajectory)\n",
    "\n",
    "            self.cached_data_x.append(trajectory)\n",
    "            self.cached_data_y.append(self.labels[i])\n",
    "        \n",
    "        self.unique_labels = np.unique(self.cached_data_y)\n",
    "        self.label_to_new_id = {self.unique_labels[i] : i for i in range(len(self.unique_labels))}\n",
    "        self.new_id_to_label = {v : k for k,v in self.label_to_new_id.items()}\n",
    "        \n",
    "        for i in range(len(self.cached_data_y)):\n",
    "            self.cached_data_y[i] = self.label_to_new_id[self.cached_data_y[i]]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.cached_data_x)\n",
    "\n",
    "    def crop_video(self, trajectory : np.array) -> np.array:\n",
    "        cropped = trajectory\n",
    "        \n",
    "        if self.FRAME_SIZE != 0:\n",
    "            frame_size = len(trajectory)\n",
    "            start = 0 \n",
    "            \n",
    "            if frame_size > self.FRAME_SIZE:\n",
    "                start = np.random.randint(0, frame_size - self.FRAME_SIZE)\n",
    "            cropped = trajectory[start: (start + self.FRAME_SIZE)]\n",
    "                    \n",
    "            if len(cropped) < self.FRAME_SIZE:\n",
    "                necessary = self.FRAME_SIZE - len(cropped)\n",
    "                t, h, w, c = trajectory.shape\n",
    "                cropped = np.concatenate([cropped, np.zeros((necessary, h, w, c))], axis= 0)\n",
    "                \n",
    "            return cropped\n",
    "                        \n",
    "        return trajectory\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        trajectory, label = self.crop_video(self.cached_data_x[idx]), self.cached_data_y[idx]\n",
    "\n",
    "        if self.per_image_transform is not None:            \n",
    "            frames = {self.keywords[i] : frame for i, frame in enumerate(trajectory)}\n",
    "            processing = self.per_image_transform(**frames)       \n",
    "\n",
    "            frames = np.array([processing[kw] for kw in self.keywords])    \n",
    "        else:\n",
    "            frames = torch.Tensor(trajectory)         \n",
    "            \n",
    "        return torch.Tensor(frames), label\n",
    "    \n",
    "\n",
    "transform = A.Compose(\n",
    "    [\n",
    "        A.Normalize(mean=CONFIG.mean, std=CONFIG.std),\n",
    "        A.HorizontalFlip(p=CONFIG.P_OF_TRANSFORM),\n",
    "        A.ShiftScaleRotate(p=CONFIG.P_OF_TRANSFORM, shift_limit=CONFIG.SHIFT_LIMIT, \n",
    "                           scale_limit=CONFIG.SCALE_LIMIT, rotate_limit=CONFIG.ROTATE_LIMIT),\n",
    "        #A.RandomBrightnessContrast(p=CONFIG.P_OF_TRANSFORM_COLOR),\n",
    "        #A.RGBShift(p=CONFIG.P_OF_TRANSFORM_COLOR),\n",
    "        ToTensorV2()\n",
    "    ],\n",
    "    additional_targets={str(i) : \"image\" for i in range(CONFIG.FRAME_SIZE)}\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T14:54:03.145998200Z",
     "start_time": "2024-04-07T14:54:03.133096800Z"
    }
   },
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 30, 3, 240, 240])\n",
      "[14 20]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "ds = SignRecognitionDatasetCached(max_start=1, max_end=150, data_limit=CONFIG.DATA_LIMIT, per_image_transform=transform)\n",
    "\n",
    "# do train/val split\n",
    "dataset_size = len(ds)\n",
    "train_size = int(ds * CONFIG.PORTION_OF_DATA_FOR_TRAINING)\n",
    "val_size = dataset_size - train_size\n",
    "\n",
    "print(f\"splitting into : {train_size} {val_size}\")   \n",
    "\n",
    "# splitting dataset \n",
    "train_dataset, val_dataset = random_split(ds, [train_size, val_size])\n",
    "\n",
    "print(f\"sizes of datasets : len(train)={len(train_dataset)} len(val)={len(val_dataset)}\")\n",
    "\n",
    "# Create Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, pin_memory=CONFIG.PIN_MEMORY, num_workers=CONFIG.NUM_WORKERS,  shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG.BATCH_SIZE, pin_memory=CONFIG.PIN_MEMORY, num_workers=CONFIG.NUM_WORKERS)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T00:24:30.428259500Z",
     "start_time": "2024-04-05T00:24:30.274525900Z"
    }
   },
   "execution_count": 203
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def test(model : nn.Module, val_dataset : DataLoader, cfg : CONFIG,   run = None):\n",
    "    # change the model to evaluation\n",
    "    model.eval()\n",
    "    \n",
    "    # get the number of datapoints\n",
    "    number_of_datapoints = len(val_dataset.dataset)    \n",
    "\n",
    "    # allocate the memory for these datapoints (no need to keep appending the data, which will make it slower)\n",
    "    predictions_prob = np.zeros((number_of_datapoints, len(val_dataset.dataset.unique_labels)))\n",
    "    predictions = np.zeros(number_of_datapoints)\n",
    "    true_values = np.zeros(number_of_datapoints) \n",
    "    \n",
    "    # get the number of batches\n",
    "    dataset_len = len(val_dataset)\n",
    "\n",
    "    # create the progreess bar \n",
    "    pbar = tqdm(val_dataset)\n",
    "\n",
    "    # variable that will track where we are in terms of all data (after iteration add batch size to it)\n",
    "    c = 0\n",
    "    \n",
    "    for i, (x,y) in enumerate(pbar): \n",
    "        # get the predictions\n",
    "        pred = model(x.to(cfg.DEVICE).float())\n",
    "    \n",
    "        # get the batch size\n",
    "        bs = x.shape[0]\n",
    "\n",
    "        true_values[c : (c + bs)] = y.detach().numpy()\n",
    "        predictions_prob[c : (c + bs)] = torch.softmax(pred.cpu().detach(), dim=1).numpy()\n",
    "        predictions[c : (c + bs)] = torch.argmax(pred, 1).cpu().detach().numpy()\n",
    "        c += bs \n",
    "            \n",
    "        if i % (dataset_len//10) == 0 or i == dataset_len -1:\n",
    "            acc = accuracy_score(predictions[:c], true_values[:c])\n",
    "            try:\n",
    "                roc_auc = roc_auc_score(true_values[:c], predictions_prob[:c, :], multi_class='ovr')            \n",
    "                \n",
    "            # It can happen at the beginning\n",
    "            except Exception as e:\n",
    "                roc_auc = 0\n",
    "\n",
    "            pbar.set_description(f\"examples seen so far : {c}, accuracy = {round(acc, cfg.ROUND_NUMBER)}, AUC ROC = {round(roc_auc, CONFIG.ROUND_NUMBER)}\")\n",
    "    \n",
    "    return {\"predition_prob\" : predictions_prob, \"predictions\" : predictions, \"true\" : true_values}\n",
    "\n",
    "def report_metrics(results : Dict, epoch : int, WANDB_ON : bool = True, prefix=\"val\", run=None) -> Dict:\n",
    "    predictions = results[\"predictions\"]\n",
    "    true_values = results[\"true\"]\n",
    "    predictions_prob = results[\"predition_prob\"]\n",
    "    \n",
    "    acc = accuracy_score(predictions, true_values)\n",
    "    roc_auc_ovr = roc_auc_score(true_values, predictions_prob, multi_class='ovr')            \n",
    "    roc_auc_ovo = roc_auc_score(true_values, predictions_prob, multi_class='ovo')  \n",
    "    \n",
    "    if WANDB_ON:\n",
    "        wandb.log({f\"{prefix}_acc\": acc, f\"{prefix}_ROC_AUC_ovr\": roc_auc_ovr, f\"{prefix}_ROC_AUC_ovo\" : roc_auc_ovo})\n",
    "        wandb.log({f\"{prefix}_ROC_epoch={epoch}\" : wandb.plot.roc_curve(true_values, predictions_prob)})\n",
    "    \n",
    "    return {\"accuracy\" : acc, \"ROC_AUC_OVR\" :  roc_auc_ovr, \"ROC_AUC_OVO\" : roc_auc_ovo}\n",
    "\n",
    "def save_model(model : nn.Module, metrics_results : Dict, metric_keyword : str, best_metric : float, savepath : str):\n",
    "    \n",
    "    if metrics_results[metric_keyword] > best_metric:\n",
    "        print(f\"Saving metric with {metric_keyword}={metrics_results[metric_keyword]} (previous : {best_metric})\")\n",
    "        torch.save(model.state_dict(), savepath)\n",
    "        \n",
    "    return max(metrics_results[metric_keyword], best_metric)\n",
    "\n",
    "def train(train_dataloader : torch.utils.data.DataLoader, \n",
    "          model : nn.Module, \n",
    "          optimizer : optim.Optimizer, \n",
    "          scheduler : lr_sched.LRScheduler, \n",
    "          criterion, \n",
    "          epoch : int, \n",
    "          cfg : CONFIG,\n",
    "          categorical_cast : bool  = True,\n",
    "          WANDB_ON : bool=True):\n",
    "    model.train()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    i = 1\n",
    "    train_len = len(train_dataloader)\n",
    "    \n",
    "    pb = tqdm(train_dataloader)\n",
    "    for inputs, labels in pb:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inputs = inputs.to(cfg.DEVICE).float()\n",
    "        if categorical_cast:\n",
    "            labels = labels.to(cfg.DEVICE).long()\n",
    "        else:\n",
    "            labels = labels.to(cfg.DEVICE).float()\n",
    "        \n",
    "        #with torch.autocast(device_type=\"cuda\"):\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "                        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                       \n",
    "        running_loss += loss.item()\n",
    "            \n",
    "        if (i-1) % (train_len//10) == 0 or i == train_len:   \n",
    "            lr = 0\n",
    "            cnt = 0\n",
    "            for param_group in optimizer.param_groups:\n",
    "                learning_rate = param_group['lr']\n",
    "                lr += learning_rate\n",
    "                    \n",
    "                cnt += 1\n",
    "                \n",
    "            pb.set_description(f\"EPOCH : {epoch}, average loss : {running_loss / i}, lr={lr / cnt}\")\n",
    "        i += 1\n",
    "    \n",
    "    if WANDB_ON:\n",
    "        wandb.log({\"loss\" : running_loss/len(train_dataloader)})\n",
    "\n",
    "def run_experiment(train_dataloader : torch.utils.data.DataLoader,\n",
    "                   val_dataloader : torch.utils.data.DataLoader,\n",
    "                   Model : nn.Module, \n",
    "                   run_name : str, \n",
    "                   model_parameters : dict, \n",
    "                   epochs : int, \n",
    "                   learning_rate : float, \n",
    "                   optimizer : str, \n",
    "                   savepath : str,\n",
    "                   cfg : CONFIG,\n",
    "                   saved_path_file : str = None,\n",
    "                   min_lr:float=1e-5, \n",
    "                   cosine_annealer_epochs=20,\n",
    "                   scheduler_en : bool = True,\n",
    "                   metric_keyword : str = \"acc\",\n",
    "                   lr_steps : int = 1000,\n",
    "                   WANDB_ON : bool = True):\n",
    "\n",
    "    try:\n",
    "        os.mkdir(\"models\") \n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    model = Model(**model_parameters).to(\"cuda\")\n",
    "    if saved_path_file is not None and os.path.exists(saved_path_file):\n",
    "        model.load_state_dict(torch.load(saved_path_file))\n",
    "        print(\"loaded state dict!\")\n",
    "    \n",
    "    config = {\"model name\" : model.__class__,\n",
    "              \"run name\" : run_name,\n",
    "              \"epochs\" : epochs,\n",
    "              \"learning rate\" : learning_rate,\n",
    "              \"optimizer\" : optimizer, \n",
    "              \"uses scheduler\" : scheduler_en,\n",
    "              \"min_lr\" : min_lr,\n",
    "              \"lr_steps\" : lr_steps}\n",
    "    \n",
    "    config.update(model_parameters)    \n",
    "    \n",
    "    model_summary_str = str(summary(model, input_size=(cfg.BATCH_SIZE, 1, 150, 150)))\n",
    "        \n",
    "    if WANDB_ON:\n",
    "        run = wandb.init(project=cfg.TASK_NAME,\n",
    "                     name=f\"experiment_{run_name}\",\n",
    "                     notes=\"Model summary : \\n\" + model_summary_str,\n",
    "                     config=config)\n",
    "\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Observe that all parameters are being optimized\n",
    "    if optimizer.lower() == \"adam\":\n",
    "        optimizer_ = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer.lower() == \"adamw\":\n",
    "        optimizer_ = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "        raise Exception(\"specify correctly the optimizer !\")\n",
    "\n",
    "    if scheduler_en:\n",
    "        scheduler = lr_sched.CosineAnnealingLR(optimizer_, cosine_annealer_epochs, eta_min=min_lr)\n",
    "\n",
    "    best_metric = 0 \n",
    "    class_labels = val_dataloader.dataset.class_names\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train(train_dataloader, model, optimizer_, scheduler, criterion, epoch=epoch, WANDB_ON=WANDB_ON, cfg=cfg)\n",
    "\n",
    "        test_res = test(model, train_dataloader, cfg=cfg)\n",
    "        evaluation = report_metrics(test_res, epoch=epoch, class_labels=class_labels, prefix=\"train\", WANDB_ON=WANDB_ON)\n",
    "            \n",
    "        test_res = test(model, val_dataloader, cfg=cfg)\n",
    "        evaluation = report_metrics(test_res, epoch=epoch, class_labels=class_labels, prefix=\"val\", WANDB_ON=WANDB_ON)\n",
    "\n",
    "        best_metric = save_model(model, evaluation, metric_keyword, best_metric, savepath)\n",
    "            \n",
    "        if scheduler_en:\n",
    "            scheduler.step()\n",
    "    \n",
    "    if WANDB_ON:\n",
    "        wandb.finish()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T14:41:22.849322500Z",
     "start_time": "2024-04-07T14:41:22.837647300Z"
    }
   },
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Resnet18 + LSTM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_classes, hid_size):\n",
    "        super().__init__() \n",
    "        self.backbone = torchvision.models.resnet18()\n",
    "        self.backbone.fc = nn.Identity() \n",
    "        self.lstm = nn.LSTM(input_size=512, hidden_size=hid_size, num_layers=1, batch_first=True)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Classifier layer\n",
    "        self.fc = nn.Linear(hid_size, num_classes)\n",
    "\n",
    "        \n",
    "    def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
    "        # assumed that the data is in format (batch, time, channels, height, width)\n",
    "        batch_size, frames, C, H, W = x.size()\n",
    "\n",
    "        x = x.view(batch_size * frames, C, H, W)\n",
    "\n",
    "        # Extract features for each frame using ResNet\n",
    "        # TODO: calculate gradient or not for backbone ? \n",
    "        with torch.no_grad(): \n",
    "            features = self.backbone(x)            \n",
    "        \n",
    "        features = features.view(batch_size, frames, -1)        \n",
    "        x, _ = self.lstm(features)\n",
    "\n",
    "        # Take the output of the last time step\n",
    "        x = x[:, -1, :]\n",
    "        \n",
    "        return self.fc(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T14:41:27.755914600Z",
     "start_time": "2024-04-07T14:41:27.744712900Z"
    }
   },
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "    \n",
    "m = Model(hid_size=64, num_classes=len(ds.unique_labels))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=3e-4)\n",
    "\n",
    "run_experiment(m,\n",
    "               )"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
