{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T16:23:26.566671700Z",
     "start_time": "2024-04-07T16:23:26.548795900Z"
    },
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-03T19:45:37.658372Z",
     "iopub.status.busy": "2024-04-03T19:45:37.657965Z",
     "iopub.status.idle": "2024-04-03T19:45:37.666248Z",
     "shell.execute_reply": "2024-04-03T19:45:37.665056Z",
     "shell.execute_reply.started": "2024-04-03T19:45:37.658340Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import json\n",
    "import os \n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.image import imread\n",
    "from math import log, ceil, sqrt\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchinfo\n",
    "import torch\n",
    "import wandb\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from typing import Dict\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_sched\n",
    "from torchinfo import summary\n",
    "\n",
    "class CONFIG:\n",
    "\n",
    "    ROOT_DIRECTORY = os.path.join(\"..\",\"data\", \"WLASL\")\n",
    "    JSON_FILE = \"WLASL_v0.3.json\"\n",
    "    NSLT_FILE = \"nslt_100.json\"\n",
    "    VIDEO_FOLDER = \"videos\"\n",
    "    mean=[0.485, 0.456, 0.406]\n",
    "    std=[0.229, 0.224, 0.225]\n",
    "    #mean = [0, 0, 0]\n",
    "    #std = [1,1,1]\n",
    "    DEBUG = True\n",
    "    \n",
    "    P_OF_TRANSFORM = 0.9\n",
    "    P_OF_TRANSFORM_COLOR = 0.2\n",
    "    \n",
    "    SHIFT_LIMIT=0.1\n",
    "    SCALE_LIMIT=0.1\n",
    "    ROTATE_LIMIT=10\n",
    "    \n",
    "    # set to small, when prototyping, or 0 when deploying to cloud or PC with loads of RAM\n",
    "    DATA_LIMIT = 100\n",
    "    FRAME_SIZE = 30\n",
    "    \n",
    "    \n",
    "    DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    NUM_WORKERS = 0\n",
    "    ROUND_NUMBER = 3\n",
    "    TASK_NAME = \"WLASL_RGB\"\n",
    "\n",
    "    BATCH_SIZE = 4\n",
    "    PORTION_OF_DATA_FOR_TRAINING = 0.8\n",
    "    PIN_MEMORY = False\n",
    "    \n",
    "torch.backends.cudnn.benchmark = True\n",
    "print(f\"Device : {CONFIG.DEVICE}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T16:27:48.890818Z",
     "start_time": "2024-04-07T16:27:32.094482400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "210cd91a744349f3bbdfc08704b20125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.020 MB of 0.020 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 9.7%             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▁</td></tr><tr><td>train_ROC_AUC_ovo</td><td>█▁</td></tr><tr><td>train_ROC_AUC_ovr</td><td>█▁</td></tr><tr><td>train_acc</td><td>▁▁</td></tr><tr><td>val_ROC_AUC_ovo</td><td>█▁</td></tr><tr><td>val_ROC_AUC_ovr</td><td>█▁</td></tr><tr><td>val_acc</td><td>▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>1.92663</td></tr><tr><td>train_ROC_AUC_ovo</td><td>0.44922</td></tr><tr><td>train_ROC_AUC_ovr</td><td>0.44855</td></tr><tr><td>train_acc</td><td>0.1875</td></tr><tr><td>val_ROC_AUC_ovo</td><td>0.61574</td></tr><tr><td>val_ROC_AUC_ovr</td><td>0.59143</td></tr><tr><td>val_acc</td><td>0.05</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">experiment_resnet18+LSTM_hid=64</strong> at: <a href='https://wandb.ai/mlewand7/WLASL_RGB/runs/0vg73154/workspace' target=\"_blank\">https://wandb.ai/mlewand7/WLASL_RGB/runs/0vg73154/workspace</a><br/>Synced 6 W&B file(s), 4 media file(s), 6 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240407_172600-0vg73154\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\mlewand\\.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.finish()\n",
    "wandb.login(relogin=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Read the necessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T16:23:29.793368400Z",
     "start_time": "2024-04-07T16:23:29.543431400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dataset consists of 2038 videos\n",
      "there are 11980 videos in total\n"
     ]
    }
   ],
   "source": [
    "# setup the paths\n",
    "video_path = os.path.join(CONFIG.ROOT_DIRECTORY, CONFIG.VIDEO_FOLDER)\n",
    "dataset_description = os.path.join(CONFIG.ROOT_DIRECTORY)\n",
    "\n",
    "# load the filepaths for videos\n",
    "video_paths = [os.path.join(video_path, file) for file in os.listdir(video_path)]\n",
    "\n",
    "# load the dataset config json\n",
    "config_json = None\n",
    "with open(os.path.join(CONFIG.ROOT_DIRECTORY, CONFIG.JSON_FILE)) as f:\n",
    "    config_json = json.load(f)\n",
    "    \n",
    "# load the dataset json\n",
    "dataset_json = None\n",
    "with open(os.path.join(CONFIG.ROOT_DIRECTORY, CONFIG.NSLT_FILE)) as f:\n",
    "    dataset_json = json.load(f)\n",
    "\n",
    "print(f\"the dataset consists of {len(dataset_json.keys())} videos\")\n",
    "print(f\"there are {len(video_paths)} videos in total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T16:23:29.900613500Z",
     "start_time": "2024-04-07T16:23:29.888101900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Format(x, rnd_digits=3):\n",
    "    prefix = \"\"\n",
    "    if 1024 <= x < 1024**2:\n",
    "        prefix = \"K\"\n",
    "        x /= 1024\n",
    "    elif 1024**2 <= x < 1024**3:\n",
    "        prefix = \"M\"\n",
    "        x /= 1024**2\n",
    "    elif x >= 1024**3:\n",
    "        prefix = \"G\"\n",
    "        x /= 1024**3\n",
    "    \n",
    "    return f\"{round(x,rnd_digits)}{prefix}B\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Regular Dataset : store the data on disk and load it from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T16:23:30.784623200Z",
     "start_time": "2024-04-07T16:23:30.135840400Z"
    },
    "execution": {
     "iopub.execute_input": "2024-04-03T19:57:22.011877Z",
     "iopub.status.busy": "2024-04-03T19:57:22.011358Z",
     "iopub.status.idle": "2024-04-03T19:57:23.972773Z",
     "shell.execute_reply": "2024-04-03T19:57:23.971801Z",
     "shell.execute_reply.started": "2024-04-03T19:57:22.011847Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2038/2038 [00:00<00:00, 14949.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after filtering : size of dataset=11980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "        \n",
    "class SignRecognitionDataset(Dataset):\n",
    "\n",
    "    def __init__(self, max_start : int, max_end) -> None:\n",
    "        # setup the paths\n",
    "        video_path = os.path.join(CONFIG.ROOT_DIRECTORY, CONFIG.VIDEO_FOLDER)\n",
    "        dataset_description = os.path.join(CONFIG.ROOT_DIRECTORY)\n",
    "\n",
    "        # load the filepaths for videos\n",
    "        self.video_paths = [os.path.join(video_path, file) for file in os.listdir(video_path)]\n",
    "\n",
    "        # load the dataset config json\n",
    "        self.config_json = None\n",
    "        with open(os.path.join(CONFIG.ROOT_DIRECTORY, CONFIG.JSON_FILE)) as f:\n",
    "            self.config_json = json.load(f)\n",
    "\n",
    "        # load the dataset json\n",
    "        self.dataset_json = None\n",
    "        with open(os.path.join(CONFIG.ROOT_DIRECTORY, CONFIG.NSLT_FILE)) as f:\n",
    "            self.dataset_json = json.load(f)\n",
    "        \n",
    "        self.videos_paths = []\n",
    "        self.paths_not_found = []\n",
    "        self.labels = []\n",
    "        self.start_frames = []\n",
    "        self.end_frames = []\n",
    "        \n",
    "\n",
    "        for el in tqdm(dataset_json.items()):\n",
    "            video_id, properties = el[0], el[1]\n",
    "            path = os.path.join(video_path, video_id + \".mp4\")\n",
    "            \n",
    "            if not os.path.exists(path):\n",
    "                self.paths_not_found.append(path)\n",
    "                continue\n",
    "\n",
    "            subset = properties[\"subset\"]\n",
    "            label, start, end = properties[\"action\"]\n",
    "            \n",
    "            if start > max_start:\n",
    "                continue\n",
    "                \n",
    "            if end > max_end:\n",
    "                continue\n",
    "            \n",
    "            self.videos_paths.append(path)\n",
    "            self.labels.append(label)\n",
    "            self.start_frames.append(start)\n",
    "            self.end_frames.append(end)\n",
    "    \n",
    "        self.videos_paths = np.array(self.video_paths)\n",
    "        self.paths_not_found = np.array(self.paths_not_found)\n",
    "        self.labels = np.array(self.labels)\n",
    "        self.start_frames = np.array(self.start_frames)\n",
    "        self.end_frames = np.array(self.end_frames)\n",
    "\n",
    "        self.unique_labels = np.unique(self.labels)\n",
    "        \n",
    "    \n",
    "    def preprocess_trajectory(self, traj : List[np.ndarray]):\n",
    "        return traj\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.videos_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.videos_paths[idx], self.labels[idx]\n",
    "        trajectory = SignRecognitionDataset.get_video(path)\n",
    "        \n",
    "        return self.preprocess_trajectory(trajectory), label\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def get_video(video_path : str) -> List[np.ndarray]:\n",
    "\n",
    "        if not os.path.exists(video_path):\n",
    "            return None\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            return None\n",
    "\n",
    "        frames = []\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame_array = np.array(frame)\n",
    "                frames.append(cv2.cvtColor(frame_array, cv2.COLOR_BGR2RGB))\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        return np.array(frames)\n",
    "\n",
    "    @staticmethod\n",
    "    def rescale_video(frames : np.ndarray, desired_shape) -> np.ndarray:\n",
    "        refined = []\n",
    "        for img in frames: \n",
    "            y, x, c = img.shape\n",
    "            cropped = img[:, (x // 2 - y//2) : (x // 2 + y//2), :]\n",
    "            scaled = cv2.resize(cropped, desired_shape)\n",
    "            refined.append(scaled)\n",
    "    \n",
    "        return np.array(refined)\n",
    "        \n",
    "    \n",
    "ds = SignRecognitionDataset(max_start=1, max_end=150)\n",
    "print(f\"after filtering : size of dataset={len(ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Cached Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T16:23:54.795404200Z",
     "start_time": "2024-04-07T16:23:30.780621900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2038/2038 [00:00<00:00, 48506.95it/s]\n",
      "Loading/scaling trajectories: 100%|██████████| 100/100 [00:22<00:00,  4.36it/s]\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "from collections import Counter\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "class SignRecognitionDatasetCached(SignRecognitionDataset):\n",
    "\n",
    "    def __init__(self, max_start: int, max_end, per_image_transform=None,\n",
    "                 scaled_resolution : Tuple[int]= (240, 240),\n",
    "                 frame_size:int=CONFIG.FRAME_SIZE,\n",
    "                 data_limit : int = CONFIG.DATA_LIMIT,\n",
    "                 by_size=True) -> None:\n",
    "        super().__init__(max_start, max_end)\n",
    "        self.scaled_resolution = scaled_resolution\n",
    "        self.DATA_LIMIT = data_limit\n",
    "        self.by_size = by_size\n",
    "        self.cache_data()\n",
    "        self.per_image_transform = per_image_transform\n",
    "        self.FRAME_SIZE = frame_size\n",
    "        \n",
    "        self.keywords = [\"image\" ] + list(str(i) for i in range(frame_size-1))\n",
    "        \n",
    "    def preprocess_trajectory(self, traj : List[np.ndarray]):\n",
    "        return SignRecognitionDataset.rescale_video(traj, self.scaled_resolution)\n",
    "\n",
    "    def permutate(self):\n",
    "        l = len(self.videos_paths)\n",
    "        mask = np.arange(l)\n",
    "        np.random.shuffle(mask)\n",
    "        \n",
    "        self.videos_paths = np.array(self.videos_paths)[mask]\n",
    "        self.labels = np.array(self.labels)[mask]\n",
    "        self.start_frames = np.array(self.start_frames)[mask]\n",
    "        self.end_frames = np.array(self.end_frames)[mask]\n",
    "        \n",
    "    def sort_by_size(self):\n",
    "        c = Counter(self.labels)\n",
    "        _mask = sorted([ (10000 * c[l] + l, i) for i, l in enumerate(self.labels)])[::-1]\n",
    "        mask_by_size = np.array([el[1] for el in _mask])\n",
    "\n",
    "        self.video_paths        = np.array(self.video_paths)[mask_by_size]\n",
    "        self.labels             = np.array(self.labels)[mask_by_size]\n",
    "        self.start_frames       = np.array(self.start_frames)[mask_by_size]\n",
    "        self.end_frames         = np.array(self.end_frames)[mask_by_size]\n",
    "\n",
    " \n",
    "    def cache_data(self):\n",
    "        if not self.by_size:\n",
    "            self.permutate()\n",
    "        else:\n",
    "            self.sort_by_size()\n",
    "        \n",
    "        self.cached_data_x = []\n",
    "        self.cached_data_y = []\n",
    "        \n",
    "        pbar = tqdm(range(self.DATA_LIMIT))\n",
    "        pbar.set_description(\"Loading/scaling trajectories\")\n",
    "        for i in pbar:\n",
    "            trajectory = SignRecognitionDataset.get_video(self.videos_paths[i])            \n",
    "            trajectory = self.preprocess_trajectory(trajectory)\n",
    "\n",
    "            self.cached_data_x.append(trajectory)\n",
    "            self.cached_data_y.append(self.labels[i])\n",
    "        \n",
    "        self.unique_labels = np.unique(self.cached_data_y)\n",
    "        self.label_to_new_id = {self.unique_labels[i] : i for i in range(len(self.unique_labels))}\n",
    "        self.new_id_to_label = {v : k for k,v in self.label_to_new_id.items()}\n",
    "        \n",
    "        for i in range(len(self.cached_data_y)):\n",
    "            self.cached_data_y[i] = self.label_to_new_id[self.cached_data_y[i]]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.cached_data_x)\n",
    "\n",
    "    def crop_video(self, trajectory : np.array) -> np.array:\n",
    "        cropped = trajectory\n",
    "        \n",
    "        if self.FRAME_SIZE != 0:\n",
    "            frame_size = len(trajectory)\n",
    "            start = 0 \n",
    "            \n",
    "            if frame_size > self.FRAME_SIZE:\n",
    "                start = np.random.randint(0, frame_size - self.FRAME_SIZE)\n",
    "            cropped = trajectory[start: (start + self.FRAME_SIZE)]\n",
    "                    \n",
    "            if len(cropped) < self.FRAME_SIZE:\n",
    "                necessary = self.FRAME_SIZE - len(cropped)\n",
    "                t, h, w, c = trajectory.shape\n",
    "                cropped = np.concatenate([cropped, np.zeros((necessary, h, w, c))], axis= 0)\n",
    "                \n",
    "            return cropped\n",
    "                        \n",
    "        return trajectory\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        trajectory, label = self.crop_video(self.cached_data_x[idx]), self.cached_data_y[idx]\n",
    "\n",
    "        if self.per_image_transform is not None:            \n",
    "            frames = {self.keywords[i] : frame for i, frame in enumerate(trajectory)}\n",
    "            processing = self.per_image_transform(**frames)       \n",
    "\n",
    "            return np.stack([processing[kw] for kw in self.keywords]), label    \n",
    "        return torch.Tensor(trajectory), label\n",
    "    \n",
    "\n",
    "transform = A.Compose(\n",
    "    [\n",
    "        A.Normalize(mean=CONFIG.mean, std=CONFIG.std),\n",
    "        A.HorizontalFlip(p=CONFIG.P_OF_TRANSFORM),\n",
    "        A.ShiftScaleRotate(p=CONFIG.P_OF_TRANSFORM, shift_limit=0.1, scale_limit=0.1, rotate_limit=10),\n",
    "        #A.RandomBrightnessContrast(p=0.2),\n",
    "        #A.RGBShift(p=0.2),\n",
    "        ToTensorV2()\n",
    "    ],\n",
    "    additional_targets={str(i) : \"image\" for i in range(CONFIG.FRAME_SIZE)}\n",
    ")\n",
    "    \n",
    "\n",
    "ds = SignRecognitionDatasetCached(max_start=1, max_end=150,\n",
    "                                  data_limit=CONFIG.DATA_LIMIT, per_image_transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T16:23:54.824185700Z",
     "start_time": "2024-04-07T16:23:54.809410100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from collections import Counter\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "class SignRecognitionDatasetCachedMHI(SignRecognitionDataset):\n",
    "\n",
    "    def __init__(self, max_start: int, max_end, \n",
    "                 per_image_transform=None,\n",
    "                 after_MHI_transform=None,\n",
    "                 scaled_resolution : Tuple[int]= (240, 240),\n",
    "                 frame_size:int=CONFIG.FRAME_SIZE,\n",
    "                 data_limit : int = CONFIG.DATA_LIMIT,\n",
    "                 decay : float = 0.7,\n",
    "                 threshold_method : str = \"regular\",\n",
    "                 threshold_val : int = 0.3 * 255,\n",
    "                 by_size=True) -> None:\n",
    "        super().__init__(max_start, max_end)\n",
    "        self.scaled_resolution = scaled_resolution\n",
    "        self.DATA_LIMIT = data_limit\n",
    "        self.by_size = by_size\n",
    "        self.cache_data()\n",
    "        self.per_image_transform = per_image_transform\n",
    "        self.after_MHI_transform = after_MHI_transform\n",
    "        self.FRAME_SIZE = frame_size\n",
    "        \n",
    "        self.keywords = [\"image\" ] + list(str(i) for i in range(frame_size-1))\n",
    "        \n",
    "    def preprocess_trajectory(self, traj : List[np.ndarray]):\n",
    "        return SignRecognitionDataset.rescale_video(traj, self.scaled_resolution)\n",
    "\n",
    "    def permutate(self):\n",
    "        l = len(self.videos_paths)\n",
    "        mask = np.arange(l)\n",
    "        np.random.shuffle(mask)\n",
    "        \n",
    "        self.videos_paths = np.array(self.videos_paths)[mask]\n",
    "        self.labels = np.array(self.labels)[mask]\n",
    "        self.start_frames = np.array(self.start_frames)[mask]\n",
    "        self.end_frames = np.array(self.end_frames)[mask]\n",
    "        \n",
    "    def sort_by_size(self):\n",
    "        c = Counter(self.labels)\n",
    "        _mask = sorted([ (10000 * c[l] + l, i) for i, l in enumerate(self.labels)])[::-1]\n",
    "        mask_by_size = np.array([el[1] for el in _mask])\n",
    "\n",
    "        self.video_paths        = np.array(self.video_paths)[mask_by_size]\n",
    "        self.labels             = np.array(self.labels)[mask_by_size]\n",
    "        self.start_frames       = np.array(self.start_frames)[mask_by_size]\n",
    "        self.end_frames         = np.array(self.end_frames)[mask_by_size]\n",
    "\n",
    " \n",
    "    def cache_data(self):\n",
    "        if not self.by_size:\n",
    "            self.permutate()\n",
    "        else:\n",
    "            self.sort_by_size()\n",
    "        \n",
    "        self.cached_data_x = []\n",
    "        self.cached_data_y = []\n",
    "        \n",
    "        pbar = tqdm(range(self.DATA_LIMIT))\n",
    "        pbar.set_description(\"Loading/scaling trajectories\")\n",
    "        for i in pbar:\n",
    "            trajectory = SignRecognitionDataset.get_video(self.videos_paths[i])            \n",
    "            trajectory = self.preprocess_trajectory(trajectory)\n",
    "\n",
    "            self.cached_data_x.append(trajectory)\n",
    "            self.cached_data_y.append(self.labels[i])\n",
    "        \n",
    "        self.unique_labels = np.unique(self.cached_data_y)\n",
    "        self.label_to_new_id = {self.unique_labels[i] : i for i in range(len(self.unique_labels))}\n",
    "        self.new_id_to_label = {v : k for k,v in self.label_to_new_id.items()}\n",
    "        \n",
    "        for i in range(len(self.cached_data_y)):\n",
    "            self.cached_data_y[i] = self.label_to_new_id[self.cached_data_y[i]]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.cached_data_x)\n",
    "\n",
    "    def crop_video(self, trajectory : np.array) -> np.array:\n",
    "        cropped = trajectory\n",
    "        \n",
    "        if self.FRAME_SIZE != 0:\n",
    "            frame_size = len(trajectory)\n",
    "            start = 0 \n",
    "            \n",
    "            if frame_size > self.FRAME_SIZE:\n",
    "                start = np.random.randint(0, frame_size - self.FRAME_SIZE)\n",
    "            cropped = trajectory[start: (start + self.FRAME_SIZE)]\n",
    "                    \n",
    "            if len(cropped) < self.FRAME_SIZE:\n",
    "                necessary = self.FRAME_SIZE - len(cropped)\n",
    "                t, h, w, c = trajectory.shape\n",
    "                cropped = np.concatenate([cropped, np.zeros((necessary, h, w, c))], axis= 0)\n",
    "                \n",
    "            return cropped\n",
    "                        \n",
    "        return trajectory\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        trajectory, label = self.crop_video(self.cached_data_x[idx]), self.cached_data_y[idx]\n",
    "\n",
    "        if self.per_image_transform is not None:            \n",
    "            frames = {self.keywords[i] : frame for i, frame in enumerate(trajectory)}\n",
    "            processing = self.per_image_transform(**frames)       \n",
    "\n",
    "            frames = np.array([processing[kw] for kw in self.keywords])    \n",
    "        else:\n",
    "            frames = torch.Tensor(trajectory)         \n",
    "            \n",
    "        return torch.Tensor(frames), label\n",
    "    \n",
    "\n",
    "transform = A.Compose(\n",
    "    [\n",
    "        A.Normalize(mean=CONFIG.mean, std=CONFIG.std),\n",
    "        A.HorizontalFlip(p=CONFIG.P_OF_TRANSFORM),\n",
    "        A.ShiftScaleRotate(p=CONFIG.P_OF_TRANSFORM, shift_limit=CONFIG.SHIFT_LIMIT, \n",
    "                           scale_limit=CONFIG.SCALE_LIMIT, rotate_limit=CONFIG.ROTATE_LIMIT),\n",
    "        #A.RandomBrightnessContrast(p=CONFIG.P_OF_TRANSFORM_COLOR),\n",
    "        #A.RGBShift(p=CONFIG.P_OF_TRANSFORM_COLOR),\n",
    "        ToTensorV2()\n",
    "    ],\n",
    "    additional_targets={str(i) : \"image\" for i in range(CONFIG.FRAME_SIZE)}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T16:24:15.090604Z",
     "start_time": "2024-04-07T16:23:54.821166200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2038/2038 [00:00<00:00, 43746.12it/s]\n",
      "Loading/scaling trajectories: 100%|██████████| 100/100 [00:19<00:00,  5.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting into : 80 20\n",
      "sizes of datasets : len(train)=80 len(val)=20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "ds = SignRecognitionDatasetCached(max_start=1, max_end=150, data_limit=CONFIG.DATA_LIMIT, per_image_transform=transform)\n",
    "\n",
    "# do train/val split\n",
    "dataset_size = len(ds)\n",
    "train_size = int(dataset_size * CONFIG.PORTION_OF_DATA_FOR_TRAINING)\n",
    "val_size = dataset_size - train_size\n",
    "\n",
    "print(f\"splitting into : {train_size} {val_size}\")   \n",
    "\n",
    "# splitting dataset \n",
    "train_dataset, val_dataset = random_split(ds, [train_size, val_size])\n",
    "\n",
    "print(f\"sizes of datasets : len(train)={len(train_dataset)} len(val)={len(val_dataset)}\")\n",
    "\n",
    "# Create Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, pin_memory=CONFIG.PIN_MEMORY, num_workers=CONFIG.NUM_WORKERS,  shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG.BATCH_SIZE, pin_memory=CONFIG.PIN_MEMORY, num_workers=CONFIG.NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T16:24:15.330943300Z",
     "start_time": "2024-04-07T16:24:15.323735300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test(model : nn.Module, val_dataset : DataLoader, cfg : CONFIG,   run = None):\n",
    "    # change the model to evaluation\n",
    "    model.eval()\n",
    "    \n",
    "    # get the number of datapoints\n",
    "    number_of_datapoints = len(val_dataset.dataset)    \n",
    "\n",
    "    # allocate the memory for these datapoints (no need to keep appending the data, which will make it slower)\n",
    "    predictions_prob = np.zeros((number_of_datapoints, len(val_dataset.dataset.dataset.unique_labels)))\n",
    "    predictions = np.zeros(number_of_datapoints)\n",
    "    true_values = np.zeros(number_of_datapoints) \n",
    "    \n",
    "    # get the number of batches\n",
    "    dataset_len = len(val_dataset)\n",
    "\n",
    "    # create the progreess bar \n",
    "    pbar = tqdm(val_dataset)\n",
    "\n",
    "    # variable that will track where we are in terms of all data (after iteration add batch size to it)\n",
    "    c = 0\n",
    "    \n",
    "    for i, (x,y) in enumerate(pbar): \n",
    "        # get the predictions\n",
    "        pred = model(x.to(cfg.DEVICE).float())\n",
    "    \n",
    "        # get the batch size\n",
    "        bs = x.shape[0]\n",
    "\n",
    "        true_values[c : (c + bs)] = y.detach().numpy()\n",
    "        predictions_prob[c : (c + bs)] = torch.softmax(pred.cpu().detach(), dim=1).numpy()\n",
    "        predictions[c : (c + bs)] = torch.argmax(pred, 1).cpu().detach().numpy()\n",
    "        c += bs \n",
    "            \n",
    "        if i % max(dataset_len//10, 1) == 0 or i == dataset_len -1:\n",
    "            acc = accuracy_score(predictions[:c], true_values[:c])\n",
    "            try:\n",
    "                roc_auc = roc_auc_score(true_values[:c], predictions_prob[:c, :], multi_class='ovr')            \n",
    "                \n",
    "            # It can happen at the beginning\n",
    "            except Exception as e:\n",
    "                roc_auc = 0\n",
    "\n",
    "            pbar.set_description(f\"examples seen so far : {c}, accuracy = {round(acc, cfg.ROUND_NUMBER)}, AUC ROC = {round(roc_auc, CONFIG.ROUND_NUMBER)}\")\n",
    "    \n",
    "    return {\"predition_prob\" : predictions_prob, \"predictions\" : predictions, \"true\" : true_values}\n",
    "\n",
    "def report_metrics(results : Dict, epoch : int, WANDB_ON : bool = True, prefix=\"val\", run=None) -> Dict:\n",
    "    predictions = results[\"predictions\"]\n",
    "    true_values = results[\"true\"]\n",
    "    predictions_prob = results[\"predition_prob\"]\n",
    "    \n",
    "    acc = accuracy_score(predictions, true_values)\n",
    "    roc_auc_ovr = roc_auc_score(true_values, predictions_prob, multi_class='ovr')            \n",
    "    roc_auc_ovo = roc_auc_score(true_values, predictions_prob, multi_class='ovo')  \n",
    "    \n",
    "    if WANDB_ON:\n",
    "        wandb.log({f\"{prefix}_acc\": acc, f\"{prefix}_ROC_AUC_ovr\": roc_auc_ovr, f\"{prefix}_ROC_AUC_ovo\" : roc_auc_ovo})\n",
    "        wandb.log({f\"{prefix}_ROC_epoch={epoch}\" : wandb.plot.roc_curve(true_values, predictions_prob)})\n",
    "    \n",
    "    return {\"accuracy\" : acc, \"ROC_AUC_OVR\" :  roc_auc_ovr, \"ROC_AUC_OVO\" : roc_auc_ovo}\n",
    "\n",
    "def save_model(model : nn.Module, metrics_results : Dict, metric_keyword : str, best_metric : float, savepath : str):\n",
    "    \n",
    "    if metrics_results[metric_keyword] > best_metric:\n",
    "        print(f\"Saving metric with {metric_keyword}={metrics_results[metric_keyword]} (previous : {best_metric})\")\n",
    "        torch.save(model.state_dict(), savepath)\n",
    "        \n",
    "    return max(metrics_results[metric_keyword], best_metric)\n",
    "\n",
    "def train(train_dataloader : torch.utils.data.DataLoader, \n",
    "          model : nn.Module, \n",
    "          optimizer : optim.Optimizer, \n",
    "          scheduler : lr_sched.LRScheduler, \n",
    "          criterion, \n",
    "          epoch : int, \n",
    "          cfg : CONFIG,\n",
    "          categorical_cast : bool  = True,\n",
    "          WANDB_ON : bool=True):\n",
    "    model.train()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    i = 1\n",
    "    train_len = len(train_dataloader)\n",
    "    \n",
    "    pb = tqdm(train_dataloader)\n",
    "    for inputs, labels in pb:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inputs = inputs.to(cfg.DEVICE).float()\n",
    "        if categorical_cast:\n",
    "            labels = labels.to(cfg.DEVICE).long()\n",
    "        else:\n",
    "            labels = labels.to(cfg.DEVICE).float()\n",
    "        \n",
    "        #with torch.autocast(device_type=\"cuda\"):\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "                        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                       \n",
    "        running_loss += loss.item()\n",
    "            \n",
    "        if (i-1) % (train_len//10) == 0 or i == train_len:   \n",
    "            lr = 0\n",
    "            cnt = 0\n",
    "            for param_group in optimizer.param_groups:\n",
    "                learning_rate = param_group['lr']\n",
    "                lr += learning_rate\n",
    "                    \n",
    "                cnt += 1\n",
    "                \n",
    "            pb.set_description(f\"EPOCH : {epoch}, average loss : {running_loss / i}, lr={lr / cnt}\")\n",
    "        i += 1\n",
    "    \n",
    "    if WANDB_ON:\n",
    "        wandb.log({\"loss\" : running_loss/len(train_dataloader)})\n",
    "\n",
    "def run_experiment(train_dataloader : torch.utils.data.DataLoader,\n",
    "                   val_dataloader : torch.utils.data.DataLoader,\n",
    "                   Model : nn.Module, \n",
    "                   run_name : str, \n",
    "                   model_parameters : dict, \n",
    "                   epochs : int, \n",
    "                   learning_rate : float, \n",
    "                   optimizer : str, \n",
    "                   savepath : str,\n",
    "                   cfg : CONFIG,\n",
    "                   saved_path_file : str = None,\n",
    "                   min_lr:float=1e-5, \n",
    "                   cosine_annealer_epochs=20,\n",
    "                   scheduler_en : bool = True,\n",
    "                   metric_keyword : str = \"acc\",\n",
    "                   lr_steps : int = 1000,\n",
    "                   WANDB_ON : bool = True):\n",
    "\n",
    "    try:\n",
    "        os.mkdir(\"models\") \n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    model = Model(**model_parameters).to(\"cuda\")\n",
    "    if saved_path_file is not None and os.path.exists(saved_path_file):\n",
    "        model.load_state_dict(torch.load(saved_path_file))\n",
    "        print(\"loaded state dict!\")\n",
    "    \n",
    "    config = {\"model name\" : model.__class__,\n",
    "              \"run name\" : run_name,\n",
    "              \"epochs\" : epochs,\n",
    "              \"learning rate\" : learning_rate,\n",
    "              \"optimizer\" : optimizer, \n",
    "              \"uses scheduler\" : scheduler_en,\n",
    "              \"min_lr\" : min_lr,\n",
    "              \"lr_steps\" : lr_steps}\n",
    "    \n",
    "    config.update(model_parameters)    \n",
    "            \n",
    "    if WANDB_ON:\n",
    "        run = wandb.init(project=cfg.TASK_NAME,\n",
    "                     name=f\"experiment_{run_name}\",\n",
    "                     notes=\"Model summary : \\n\" + str(model),\n",
    "                     config=config)\n",
    "\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Observe that all parameters are being optimized\n",
    "    if optimizer.lower() == \"adam\":\n",
    "        optimizer_ = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer.lower() == \"adamw\":\n",
    "        optimizer_ = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "        raise Exception(\"specify correctly the optimizer !\")\n",
    "\n",
    "    if scheduler_en:\n",
    "        scheduler = lr_sched.CosineAnnealingLR(optimizer_, cosine_annealer_epochs, eta_min=min_lr)\n",
    "\n",
    "    best_metric = 0 \n",
    "    class_labels = val_loader.dataset.dataset.label_to_new_id\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train(train_dataloader, model, optimizer_, scheduler, criterion, epoch=epoch, WANDB_ON=WANDB_ON, cfg=cfg)\n",
    "\n",
    "        test_res = test(model, train_dataloader, cfg=cfg)\n",
    "        evaluation = report_metrics(test_res, epoch=epoch, #class_labels=class_labels,\n",
    "                                    prefix=\"train\", WANDB_ON=WANDB_ON)\n",
    "            \n",
    "        test_res = test(model, val_dataloader, cfg=cfg)\n",
    "        evaluation = report_metrics(test_res, epoch=epoch, #class_labels=class_labels, \n",
    "                                    prefix=\"val\", WANDB_ON=WANDB_ON)\n",
    "\n",
    "        best_metric = save_model(model, evaluation, metric_keyword, best_metric, savepath)\n",
    "            \n",
    "        if scheduler_en:\n",
    "            scheduler.step()\n",
    "    \n",
    "    if WANDB_ON:\n",
    "        wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Resnet18 + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T16:24:15.478691100Z",
     "start_time": "2024-04-07T16:24:15.336943100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Resnet_plus_LSTM(\n",
       "  (backbone): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (lstm): LSTM(512, 64, batch_first=True)\n",
       "  (fc): Linear(in_features=64, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Resnet_plus_LSTM(nn.Module):\n",
    "    def __init__(self, num_classes, hid_size):\n",
    "        super().__init__() \n",
    "        self.backbone = torchvision.models.resnet18()\n",
    "        self.backbone.fc = nn.Identity() \n",
    "        self.lstm = nn.LSTM(input_size=512, hidden_size=hid_size, num_layers=1, batch_first=True)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Classifier layer\n",
    "        self.fc = nn.Linear(hid_size, num_classes)\n",
    "\n",
    "        \n",
    "    def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
    "        # assumed that the data is in format (batch, time, channels, height, width)\n",
    "        batch_size, frames, C, H, W = x.size()\n",
    "\n",
    "        x = x.view(batch_size * frames, C, H, W)\n",
    "\n",
    "        # Extract features for each frame using ResNet\n",
    "        # TODO: calculate gradient or not for backbone ? \n",
    "        with torch.no_grad(): \n",
    "            features = self.backbone(x)            \n",
    "        \n",
    "        features = features.view(batch_size, frames, -1)        \n",
    "        x, _ = self.lstm(features)\n",
    "\n",
    "        # Take the output of the last time step\n",
    "        x = x[:, -1, :]\n",
    "        \n",
    "        return self.fc(x)\n",
    "    \n",
    "m = Resnet_plus_LSTM(num_classes=len(ds.unique_labels), hid_size=64)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T16:40:07.701816700Z",
     "start_time": "2024-04-07T16:30:59.697554800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\PROJS\\sign-language-recognition\\notebooks\\wandb\\run-20240407_173100-2c9211tf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/WLASL_RGB/runs/2c9211tf/workspace' target=\"_blank\">experiment_resnet18+LSTM_hid=64</a></strong> to <a href='https://wandb.ai/mlewand7/WLASL_RGB' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/WLASL_RGB' target=\"_blank\">https://wandb.ai/mlewand7/WLASL_RGB</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/WLASL_RGB/runs/2c9211tf/workspace' target=\"_blank\">https://wandb.ai/mlewand7/WLASL_RGB/runs/2c9211tf/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH : 0, average loss : 1.9601110994815827, lr=0.0001: 100%|██████████| 20/20 [00:10<00:00,  1.92it/s]\n",
      "examples seen so far : 80, accuracy = 0.188, AUC ROC = 0.518: 100%|██████████| 20/20 [00:05<00:00,  3.85it/s]\n",
      "examples seen so far : 20, accuracy = 0.05, AUC ROC = 0.468: 100%|██████████| 5/5 [00:01<00:00,  3.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving metric with accuracy=0.05 (previous : 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH : 1, average loss : 1.9380579948425294, lr=9.99506682107068e-05: 100%|██████████| 20/20 [00:09<00:00,  2.02it/s]\n",
      "examples seen so far : 80, accuracy = 0.188, AUC ROC = 0.497: 100%|██████████| 20/20 [00:05<00:00,  3.77it/s]\n",
      "examples seen so far : 20, accuracy = 0.05, AUC ROC = 0.564: 100%|██████████| 5/5 [00:01<00:00,  3.69it/s] \n",
      "EPOCH : 2, average loss : 1.9351492941379547, lr=9.980286753286194e-05: 100%|██████████| 20/20 [00:10<00:00,  1.96it/s]\n",
      "examples seen so far : 80, accuracy = 0.175, AUC ROC = 0.514: 100%|██████████| 20/20 [00:05<00:00,  3.84it/s]\n",
      "examples seen so far : 20, accuracy = 0.05, AUC ROC = 0.466: 100%|██████████| 5/5 [00:01<00:00,  3.51it/s] \n",
      "EPOCH : 3, average loss : 1.9284413039684296, lr=9.955718126821722e-05: 100%|██████████| 20/20 [00:09<00:00,  2.04it/s]\n",
      "examples seen so far : 80, accuracy = 0.188, AUC ROC = 0.537: 100%|██████████| 20/20 [00:05<00:00,  3.67it/s]\n",
      "examples seen so far : 20, accuracy = 0.1, AUC ROC = 0.398: 100%|██████████| 5/5 [00:01<00:00,  3.54it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving metric with accuracy=0.1 (previous : 0.05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH : 4, average loss : 1.9258532106876374, lr=9.921457902821577e-05: 100%|██████████| 20/20 [00:09<00:00,  2.02it/s]\n",
      "examples seen so far : 80, accuracy = 0.188, AUC ROC = 0.603: 100%|██████████| 20/20 [00:05<00:00,  3.88it/s]\n",
      "examples seen so far : 20, accuracy = 0.05, AUC ROC = 0.432: 100%|██████████| 5/5 [00:01<00:00,  3.51it/s] \n",
      "EPOCH : 5, average loss : 1.9204324305057525, lr=9.877641290737884e-05: 100%|██████████| 20/20 [00:09<00:00,  2.04it/s]\n",
      "examples seen so far : 80, accuracy = 0.188, AUC ROC = 0.571: 100%|██████████| 20/20 [00:05<00:00,  3.82it/s]\n",
      "examples seen so far : 20, accuracy = 0.05, AUC ROC = 0.411: 100%|██████████| 5/5 [00:01<00:00,  3.80it/s] \n",
      "EPOCH : 6, average loss : 1.9123819291591644, lr=9.824441214720627e-05: 100%|██████████| 20/20 [00:09<00:00,  2.05it/s]\n",
      "examples seen so far : 80, accuracy = 0.2, AUC ROC = 0.605: 100%|██████████| 20/20 [00:05<00:00,  3.83it/s]  \n",
      "examples seen so far : 20, accuracy = 0.05, AUC ROC = 0.442: 100%|██████████| 5/5 [00:01<00:00,  3.73it/s] \n",
      "EPOCH : 7, average loss : 1.9227539360523225, lr=9.762067631165049e-05: 100%|██████████| 20/20 [00:09<00:00,  2.02it/s]\n",
      "examples seen so far : 80, accuracy = 0.2, AUC ROC = 0.632: 100%|██████████| 20/20 [00:05<00:00,  3.67it/s]  \n",
      "examples seen so far : 20, accuracy = 0.05, AUC ROC = 0.478: 100%|██████████| 5/5 [00:01<00:00,  3.80it/s] \n",
      "EPOCH : 8, average loss : 1.919302409887314, lr=9.690766700109658e-05: 100%|██████████| 20/20 [00:09<00:00,  2.00it/s] \n",
      "examples seen so far : 80, accuracy = 0.238, AUC ROC = 0.623: 100%|██████████| 20/20 [00:05<00:00,  3.83it/s]\n",
      "examples seen so far : 20, accuracy = 0.1, AUC ROC = 0.491: 100%|██████████| 5/5 [00:01<00:00,  3.87it/s]  \n",
      "EPOCH : 9, average loss : 1.9089115798473357, lr=9.610819813755036e-05: 100%|██████████| 20/20 [00:09<00:00,  2.03it/s]\n",
      "examples seen so far : 80, accuracy = 0.2, AUC ROC = 0.614: 100%|██████████| 20/20 [00:05<00:00,  3.75it/s]  \n",
      "examples seen so far : 20, accuracy = 0.05, AUC ROC = 0.461: 100%|██████████| 5/5 [00:01<00:00,  3.55it/s] \n",
      "EPOCH : 10, average loss : 1.9106158554553985, lr=9.522542485937367e-05: 100%|██████████| 20/20 [00:09<00:00,  2.05it/s]\n",
      "examples seen so far : 80, accuracy = 0.225, AUC ROC = 0.585: 100%|██████████| 20/20 [00:05<00:00,  3.81it/s]\n",
      "examples seen so far : 20, accuracy = 0.05, AUC ROC = 0.455: 100%|██████████| 5/5 [00:01<00:00,  3.82it/s] \n",
      "EPOCH : 11, average loss : 1.9043970346450805, lr=9.426283106939471e-05: 100%|██████████| 20/20 [00:11<00:00,  1.67it/s]\n",
      "examples seen so far : 80, accuracy = 0.225, AUC ROC = 0.613: 100%|██████████| 20/20 [00:09<00:00,  2.05it/s]\n",
      "examples seen so far : 20, accuracy = 0.05, AUC ROC = 0.503: 100%|██████████| 5/5 [00:02<00:00,  1.92it/s] \n",
      "EPOCH : 12, average loss : 1.9130578279495238, lr=9.322421568553527e-05: 100%|██████████| 20/20 [00:11<00:00,  1.80it/s]\n",
      "examples seen so far : 80, accuracy = 0.212, AUC ROC = 0.62: 100%|██████████| 20/20 [00:05<00:00,  3.85it/s] \n",
      "examples seen so far : 20, accuracy = 0.05, AUC ROC = 0.431: 100%|██████████| 5/5 [00:01<00:00,  3.77it/s] \n",
      "EPOCH : 13, average loss : 1.9281526803970337, lr=9.21136776482172e-05: 100%|██████████| 20/20 [00:09<00:00,  2.04it/s]\n",
      "examples seen so far : 80, accuracy = 0.238, AUC ROC = 0.619: 100%|██████████| 20/20 [00:05<00:00,  3.96it/s]\n",
      "examples seen so far : 20, accuracy = 0.05, AUC ROC = 0.505: 100%|██████████| 5/5 [00:01<00:00,  3.76it/s] \n",
      "EPOCH : 14, average loss : 1.909930121898651, lr=9.093559974371722e-05: 100%|██████████| 20/20 [00:09<00:00,  2.04it/s] \n",
      "examples seen so far : 80, accuracy = 0.238, AUC ROC = 0.625: 100%|██████████| 20/20 [00:05<00:00,  3.75it/s]\n",
      "examples seen so far : 20, accuracy = 0.05, AUC ROC = 0.489: 100%|██████████| 5/5 [00:01<00:00,  3.64it/s] \n",
      "EPOCH : 15, average loss : 1.9200922071933746, lr=8.969463130731182e-05: 100%|██████████| 20/20 [00:09<00:00,  2.06it/s]\n",
      "examples seen so far : 80, accuracy = 0.25, AUC ROC = 0.607: 100%|██████████| 20/20 [00:05<00:00,  3.84it/s] \n",
      "examples seen so far : 20, accuracy = 0.05, AUC ROC = 0.38: 100%|██████████| 5/5 [00:01<00:00,  3.93it/s]  \n",
      "EPOCH : 16, average loss : 1.8876539707183837, lr=8.83956698744749e-05: 100%|██████████| 20/20 [00:09<00:00,  2.03it/s]\n",
      "examples seen so far : 80, accuracy = 0.238, AUC ROC = 0.659: 100%|██████████| 20/20 [00:05<00:00,  3.74it/s]\n",
      "examples seen so far : 20, accuracy = 0.05, AUC ROC = 0.398: 100%|██████████| 5/5 [00:01<00:00,  3.49it/s] \n",
      "EPOCH : 17, average loss : 1.890685260295868, lr=8.704384185254286e-05: 100%|██████████| 20/20 [00:09<00:00,  2.01it/s] \n",
      "examples seen so far : 80, accuracy = 0.188, AUC ROC = 0.622: 100%|██████████| 20/20 [00:05<00:00,  3.74it/s]\n",
      "examples seen so far : 20, accuracy = 0.05, AUC ROC = 0.446: 100%|██████████| 5/5 [00:01<00:00,  3.52it/s] \n",
      "EPOCH : 18, average loss : 1.8991398751735686, lr=8.56444822891268e-05: 100%|██████████| 20/20 [00:09<00:00,  2.01it/s]\n",
      "examples seen so far : 80, accuracy = 0.225, AUC ROC = 0.641: 100%|██████████| 20/20 [00:05<00:00,  3.61it/s]\n",
      "examples seen so far : 20, accuracy = 0.05, AUC ROC = 0.459: 100%|██████████| 5/5 [00:01<00:00,  3.73it/s] \n",
      "EPOCH : 19, average loss : 1.9039463877677918, lr=8.420311381711694e-05: 100%|██████████| 20/20 [00:09<00:00,  2.09it/s]\n",
      "examples seen so far : 80, accuracy = 0.225, AUC ROC = 0.639: 100%|██████████| 20/20 [00:05<00:00,  3.67it/s]\n",
      "examples seen so far : 20, accuracy = 0.05, AUC ROC = 0.45: 100%|██████████| 5/5 [00:01<00:00,  3.48it/s]  \n",
      "EPOCH : 20, average loss : 1.9118046700954436, lr=8.272542485937367e-05: 100%|██████████| 20/20 [00:09<00:00,  2.06it/s]\n",
      "examples seen so far : 80, accuracy = 0.262, AUC ROC = 0.643: 100%|██████████| 20/20 [00:05<00:00,  3.73it/s]\n",
      "examples seen so far : 20, accuracy = 0.0, AUC ROC = 0.433: 100%|██████████| 5/5 [00:01<00:00,  3.62it/s]\n",
      "EPOCH : 21, average loss : 1.8722167909145355, lr=8.121724717912135e-05: 100%|██████████| 20/20 [00:09<00:00,  2.06it/s]\n",
      "examples seen so far : 80, accuracy = 0.212, AUC ROC = 0.627: 100%|██████████| 20/20 [00:05<00:00,  3.66it/s]\n",
      "examples seen so far : 20, accuracy = 0.0, AUC ROC = 0.426: 100%|██████████| 5/5 [00:01<00:00,  3.58it/s]\n",
      "EPOCH : 22, average loss : 1.8887228846549988, lr=7.96845328646431e-05: 100%|██████████| 20/20 [00:10<00:00,  1.98it/s]\n",
      "examples seen so far : 80, accuracy = 0.212, AUC ROC = 0.656: 100%|██████████| 20/20 [00:05<00:00,  3.92it/s]\n",
      "examples seen so far : 20, accuracy = 0.05, AUC ROC = 0.456: 100%|██████████| 5/5 [00:01<00:00,  3.72it/s]\n",
      "EPOCH : 23, average loss : 1.8975179314613342, lr=7.81333308391076e-05: 100%|██████████| 20/20 [00:09<00:00,  2.06it/s]\n",
      "examples seen so far : 80, accuracy = 0.262, AUC ROC = 0.653: 100%|██████████| 20/20 [00:05<00:00,  3.98it/s]\n",
      "examples seen so far : 20, accuracy = 0.05, AUC ROC = 0.454: 100%|██████████| 5/5 [00:01<00:00,  3.68it/s]\n",
      "EPOCH : 24, average loss : 1.886403787136078, lr=7.656976298823282e-05: 100%|██████████| 20/20 [00:10<00:00,  1.88it/s] \n",
      "examples seen so far : 80, accuracy = 0.262, AUC ROC = 0.662: 100%|██████████| 20/20 [00:09<00:00,  2.17it/s]\n",
      "examples seen so far : 20, accuracy = 0.05, AUC ROC = 0.433: 100%|██████████| 5/5 [00:02<00:00,  2.18it/s] \n",
      "EPOCH : 25, average loss : 1.902801162004471, lr=7.499999999999998e-05: 100%|██████████| 20/20 [00:12<00:00,  1.55it/s] \n",
      "examples seen so far : 80, accuracy = 0.25, AUC ROC = 0.666: 100%|██████████| 20/20 [00:05<00:00,  3.75it/s] \n",
      "examples seen so far : 20, accuracy = 0.05, AUC ROC = 0.426: 100%|██████████| 5/5 [00:01<00:00,  3.64it/s]\n",
      "EPOCH : 26, average loss : 1.8794211566448211, lr=7.343023701176715e-05: 100%|██████████| 20/20 [00:09<00:00,  2.07it/s]\n",
      "examples seen so far : 80, accuracy = 0.25, AUC ROC = 0.66: 100%|██████████| 20/20 [00:05<00:00,  3.84it/s]  \n",
      "examples seen so far : 20, accuracy = 0.05, AUC ROC = 0.468: 100%|██████████| 5/5 [00:01<00:00,  3.88it/s] \n",
      "EPOCH : 27, average loss : 1.8846480965614318, lr=7.186666916089238e-05: 100%|██████████| 20/20 [00:09<00:00,  2.07it/s]\n",
      "examples seen so far : 80, accuracy = 0.238, AUC ROC = 0.656: 100%|██████████| 20/20 [00:04<00:00,  4.00it/s]\n",
      "examples seen so far : 20, accuracy = 0.05, AUC ROC = 0.424: 100%|██████████| 5/5 [00:01<00:00,  3.72it/s] \n",
      "EPOCH : 28, average loss : 1.8755011899130685, lr=7.031546713535687e-05:  40%|████      | 8/20 [00:04<00:06,  1.86it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m run_experiment(Model\u001b[38;5;241m=\u001b[39mResnet_plus_LSTM, \n\u001b[0;32m      2\u001b[0m                run_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresnet18+LSTM_hid=64\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m      3\u001b[0m                model_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;28mlen\u001b[39m(ds\u001b[38;5;241m.\u001b[39munique_labels), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhid_size\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m64\u001b[39m},\n\u001b[0;32m      4\u001b[0m                epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m      5\u001b[0m                learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m,\n\u001b[0;32m      6\u001b[0m                optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m                train_dataloader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[0;32m      8\u001b[0m                val_dataloader\u001b[38;5;241m=\u001b[39mval_loader,\n\u001b[0;32m      9\u001b[0m                savepath\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresnet18.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     10\u001b[0m                min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m,\n\u001b[0;32m     11\u001b[0m                scheduler_en\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     12\u001b[0m             cosine_annealer_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m     13\u001b[0m                cfg\u001b[38;5;241m=\u001b[39mCONFIG,\n\u001b[0;32m     14\u001b[0m                metric_keyword\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m                WANDB_ON\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[11], line 185\u001b[0m, in \u001b[0;36mrun_experiment\u001b[1;34m(train_dataloader, val_dataloader, Model, run_name, model_parameters, epochs, learning_rate, optimizer, savepath, cfg, saved_path_file, min_lr, cosine_annealer_epochs, scheduler_en, metric_keyword, lr_steps, WANDB_ON)\u001b[0m\n\u001b[0;32m    182\u001b[0m class_labels \u001b[38;5;241m=\u001b[39m val_loader\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mlabel_to_new_id\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m--> 185\u001b[0m     train(train_dataloader, model, optimizer_, scheduler, criterion, epoch\u001b[38;5;241m=\u001b[39mepoch, WANDB_ON\u001b[38;5;241m=\u001b[39mWANDB_ON, cfg\u001b[38;5;241m=\u001b[39mcfg)\n\u001b[0;32m    187\u001b[0m     test_res \u001b[38;5;241m=\u001b[39m test(model, train_dataloader, cfg\u001b[38;5;241m=\u001b[39mcfg)\n\u001b[0;32m    188\u001b[0m     evaluation \u001b[38;5;241m=\u001b[39m report_metrics(test_res, epoch\u001b[38;5;241m=\u001b[39mepoch, \u001b[38;5;66;03m#class_labels=class_labels,\u001b[39;00m\n\u001b[0;32m    189\u001b[0m                                 prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, WANDB_ON\u001b[38;5;241m=\u001b[39mWANDB_ON)\n",
      "Cell \u001b[1;32mIn[11], line 105\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_dataloader, model, optimizer, scheduler, criterion, epoch, cfg, categorical_cast, WANDB_ON)\u001b[0m\n\u001b[0;32m    102\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    103\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 105\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m (train_len\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m10\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i \u001b[38;5;241m==\u001b[39m train_len:   \n\u001b[0;32m    108\u001b[0m     lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_experiment(Model=Resnet_plus_LSTM, \n",
    "               run_name=\"resnet18+LSTM_hid=64\", \n",
    "               model_parameters={\"num_classes\":len(ds.unique_labels), \"hid_size\":64},\n",
    "               epochs=100,\n",
    "               learning_rate=1e-4,\n",
    "               optimizer=\"Adam\",\n",
    "               train_dataloader=train_loader,\n",
    "               val_dataloader=val_loader,\n",
    "               savepath=os.path.join(\"models\", \"resnet18.pth\"),\n",
    "               min_lr=5e-5,\n",
    "               scheduler_en=True,\n",
    "            cosine_annealer_epochs=50,\n",
    "               cfg=CONFIG,\n",
    "               metric_keyword=\"accuracy\",\n",
    "               WANDB_ON=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1589971,
     "sourceId": 2632847,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
