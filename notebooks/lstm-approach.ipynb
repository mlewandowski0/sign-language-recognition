{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.11/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.14.0)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.9.0.80)\n",
      "Requirement already satisfied: mediapipe in /usr/local/lib/python3.11/dist-packages (0.10.11)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.4.1.post1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.1)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (15.0.2)\n",
      "Collecting wandb\n",
      "  Obtaining dependency information for wandb from https://files.pythonhosted.org/packages/8b/8d/bb05a4ecdeac6b2256d98ac10bae8723af5d7a8c1a4c2384b3ae0f80370e/wandb-0.16.6-py3-none-any.whl.metadata\n",
      "  Downloading wandb-0.16.6-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.8.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.58.0)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.14.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.14.0)\n",
      "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.14.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (23.1.0)\n",
      "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.26)\n",
      "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.26)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from mediapipe) (2.2.2)\n",
      "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.9.0.80)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.6)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.1)\n",
      "Collecting Click!=8.0.0,>=7.1 (from wandb)\n",
      "  Obtaining dependency information for Click!=8.0.0,>=7.1 from https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl.metadata\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Obtaining dependency information for GitPython!=3.1.29,>=1.0.0 from https://files.pythonhosted.org/packages/e9/bd/cc3a402a6439c15c3d4294333e13042b915bbeab54edc457c723931fed3f/GitPython-3.1.43-py3-none-any.whl.metadata\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Obtaining dependency information for sentry-sdk>=1.0.0 from https://files.pythonhosted.org/packages/b1/f8/2038661bc32579d0c11191fc1093e49db590bfb6e63d501d7995fb798d62/sentry_sdk-1.44.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading sentry_sdk-1.44.1-py2.py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Obtaining dependency information for docker-pycreds>=0.4.0 from https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.1)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Obtaining dependency information for setproctitle from https://files.pythonhosted.org/packages/fd/df/44b267cb8f073a4ae77e120f0705ab3a07165ad90cecd4881b34c7e1e37b/setproctitle-1.3.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading setproctitle-1.3.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
      "Collecting appdirs>=1.4.3 (from wandb)\n",
      "  Obtaining dependency information for appdirs>=1.4.3 from https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl.metadata\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Obtaining dependency information for gitdb<5,>=4.0.1 from https://files.pythonhosted.org/packages/fd/5b/8f0c4a5bb9fd491c277c21eff7ccae71b47d43c4446c9d0c6cff2fe8c2c4/gitdb-4.0.11-py3-none-any.whl.metadata\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
      "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.15.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.23.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.3.7)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe) (3.13.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->mediapipe) (12.4.127)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Obtaining dependency information for smmap<6,>=3.0.1 from https://files.pythonhosted.org/packages/a7/a5/10f97f73544edcdef54409f1d839f6049a0d79df68adbc1ceb24d1aaca42/smmap-5.0.1-py3-none-any.whl.metadata\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch->mediapipe) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.0)\n",
      "Downloading wandb-0.16.6-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-1.44.1-py2.py3-none-any.whl (266 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.1/266.1 kB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setproctitle-1.3.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
      "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: appdirs, smmap, setproctitle, sentry-sdk, docker-pycreds, Click, gitdb, GitPython, wandb\n",
      "Successfully installed Click-8.1.7 GitPython-3.1.43 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.44.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow opencv-python mediapipe scikit-learn matplotlib pandas pyarrow wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#Load parquet data into dataset_parquet for training.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.regularizers import l2\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import random\n",
    "import time\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    # root = os.path.join(\"/\", \"kaggle\", \"input\", \"asl-signs\") \n",
    "    root = os.path.join(\".\")\n",
    "    DATA_LIMIT = 100\n",
    "    BATCH_SIZE = 8\n",
    "    VIDEO_LENGTH = 60\n",
    "    TRAIN_VAL_SPLIT = 0.9\n",
    "    WANDB_RUN = \"mediapipe-asl-dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asl-signs.zip',\n",
       " 'lstm-approach.ipynb',\n",
       " '.ipynb_checkpoints',\n",
       " 'sign_to_prediction_index_map.json',\n",
       " 'train.csv',\n",
       " 'train_landmark_files']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(CONFIG.root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code sorts out a parquet files and rearrange the order to pose,face, left-hand, right-hand\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ids = None\n",
    "\n",
    "order_global = {\"pose\" : 10000, \"face\" : 1000, \"left_hand\" : 100, \"right_hand\" : 10}\n",
    "\n",
    "def process_parquet(ds):\n",
    "    ret = []\n",
    "    frames_unique = sorted(np.unique(ds[\"frame\"]))\n",
    "    for i,frame in enumerate(frames_unique):\n",
    "        frame_ds = ds[ds['frame'] == frame]\n",
    "        \n",
    "        order = []\n",
    "        for el in frame_ds[\"row_id\"]:\n",
    "            _frame, part, keypoint = el.split(\"-\")\n",
    "            order.append(order_global[part] - int(keypoint))\n",
    "\n",
    "        order = np.array(order)\n",
    "        frame_ds.iloc[:, 1] = order\n",
    "        frame_ds = frame_ds.sort_values(by=\"row_id\", ascending=False)\n",
    "    \n",
    "        vals = np.array(frame_ds[[\"x\", \"y\", \"z\"]]).flatten()\n",
    "\n",
    "        ret.append(vals)\n",
    "    return np.array(ret)\n",
    "        \n",
    "#process_parquet(\"79631423.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94477/94477 [00:00<00:00, 252266.18it/s]\n",
      "100%|██████████| 94477/94477 [00:00<00:00, 254670.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cardinality of train : 113, cardinality of validation : 13\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#custom class to load data from Parquet files for training ML models.\n",
    "class ParquetDataset(keras.utils.Sequence):\n",
    "    def __init__(self, dataset_folder, csv_file : str, batch_size=CONFIG.BATCH_SIZE, \n",
    "                 data_limit :int= CONFIG.DATA_LIMIT, check_if_file_exists = True, \n",
    "                 preprocessing_func=None, frame_length :int = CONFIG.VIDEO_LENGTH,\n",
    "                 split : str = \"train\", train_val_split : float = CONFIG.TRAIN_VAL_SPLIT,\n",
    "                 sort_by_counts : bool = True, **kwargs\n",
    "                ):\n",
    "        super().__init__(**kwargs)\n",
    "        #taking keras sequence for .fit(), .evaluate(), .predict() methods\n",
    "        #load csv - it has the path to parquet file, and another to store label\n",
    "        self.csv_path = csv_file\n",
    "        self.root_folder = dataset_folder\n",
    "        self.batch_size = batch_size\n",
    "        #optional pre-processing function to the parquet files.\n",
    "        self.preprocessing_func = preprocessing_func\n",
    "        \n",
    "        self.csv_data = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        self.all_files = []\n",
    "        self.not_exists = []\n",
    "        self.frame_length = frame_length\n",
    "\n",
    "        \n",
    "        for path, label in tqdm(list(zip(self.csv_data[\"path\"], self.csv_data[\"sign\"]))):\n",
    "            prop_path = os.path.join(self.root_folder, path)\n",
    "            \n",
    "            if check_if_file_exists:\n",
    "                if os.path.exists(prop_path):\n",
    "                    self.all_files.append((prop_path, label))\n",
    "                else:\n",
    "                    self.not_exists.append(prop_path)\n",
    "            else:\n",
    "                self.all_files.append((prop_path, label))\n",
    "                \n",
    "                    \n",
    "        self.all_files = np.array(self.all_files)\n",
    "        self.unique_labels = np.unique(self.all_files[:, 1])\n",
    "        self.label_2_id = { key : i for i, key in enumerate(self.unique_labels)}\n",
    "    \n",
    "        # sort the values by popularity\n",
    "        if sort_by_counts:\n",
    "            cnt = Counter(self.all_files[:, 1])\n",
    "            vals = []\n",
    "            \n",
    "            for i,row in enumerate(self.all_files):\n",
    "                vals.append((int(1e6 * cnt[row[1]] + self.label_2_id [row[1]]),i))\n",
    "            \n",
    "            vals = np.array(sorted(vals)[::-1])\n",
    "            self.all_files = self.all_files[vals[:,1]]\n",
    "\n",
    "        \n",
    "        if data_limit < 0:\n",
    "            train_ds, val_ds = train_test_split(self.all_files, train_size=train_val_split, random_state=42)\n",
    "        else:\n",
    "            train_ds, val_ds = train_test_split(self.all_files[:data_limit], train_size=train_val_split, random_state=42)\n",
    "            self.unique_labels = np.unique(self.all_files[:data_limit, 1])\n",
    "            self.label_2_id = { key : i for i, key in enumerate(self.unique_labels)}\n",
    "            \n",
    "        if split.lower() == \"train\":\n",
    "            self.dataset = train_ds\n",
    "            \n",
    "        elif split.lower() == \"val\":\n",
    "            self.dataset = val_ds \n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"please specify split to be either train or val\")\n",
    "            \n",
    "        np.random.shuffle(self.dataset)\n",
    "                   \n",
    "\n",
    "    def __len__(self):\n",
    "        # Assuming each Parquet file should be one batch; adjust if necessary\n",
    "        return math.ceil(len(self.dataset) / self.batch_size)\n",
    "    \n",
    "    def get_single(self, idx):\n",
    "        # Load one file per batch\n",
    "        #take the idx value, 1st label, \n",
    "        path, label = self.dataset[idx]\n",
    "        \n",
    "        df = pd.read_parquet( path)\n",
    "        \n",
    "        # Apply preprocessing if specified\n",
    "        if self.preprocessing_func:\n",
    "            df = self.preprocessing_func(df, self.frame_length)\n",
    "        \n",
    "        one_hot_encoded_label = np.zeros(len(self.unique_labels))\n",
    "        one_hot_encoded_label[self.label_2_id[label]] = 1  \n",
    "        \n",
    "        return df, one_hot_encoded_label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X, Y = [], []\n",
    "        \n",
    "        low = idx * self.batch_size\n",
    "        high = min(low + self.batch_size, len(self.dataset))\n",
    "        \n",
    "        for i in range(low, high):\n",
    "            x, y = self.get_single(i)\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "        \n",
    "        return np.array(X), np.array(Y)\n",
    "                \n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        # Shuffle files for the next epoch\n",
    "        np.random.shuffle(self.dataset)\n",
    "\n",
    "def my_preprocessing_func(df, frame_length):\n",
    "    \n",
    "    # Define your preprocessing steps here\n",
    "    # Example: normalize numerical features\n",
    "    frames_mediapipe = process_parquet(df)\n",
    "    \n",
    "    current_length, num_features = frames_mediapipe.shape\n",
    "\n",
    "    if current_length >= frame_length:\n",
    "            # TODO: a better than uniform value ? Could place gaussian in the middle\n",
    "            random_start = random.randint(0, current_length - frame_length)\n",
    "            return np.nan_to_num(frames_mediapipe[random_start : (random_start + frame_length)])\n",
    "        \n",
    "    # padd the video to contain zeros \n",
    "    return np.concatenate([np.nan_to_num(frames_mediapipe), np.zeros((frame_length - current_length, num_features))], axis=0)\n",
    "    \n",
    "# Usage example\n",
    "parquet_folder_path = CONFIG.root\n",
    "train_dataset_parquet = ParquetDataset(parquet_folder_path, csv_file = os.path.join(CONFIG.root, \"train.csv\"), \n",
    "                                 batch_size=CONFIG.BATCH_SIZE, data_limit=1000,\n",
    "                                 preprocessing_func=my_preprocessing_func,\n",
    "                                check_if_file_exists = True,\n",
    "                                split=\"train\")\n",
    "\n",
    "val_dataset_parquet = ParquetDataset(parquet_folder_path, csv_file = os.path.join(CONFIG.root, \"train.csv\"), \n",
    "                                 batch_size=CONFIG.BATCH_SIZE, data_limit=1000,\n",
    "                                 preprocessing_func=my_preprocessing_func,\n",
    "                                 check_if_file_exists= True,\n",
    "                                 split=\"val\")\n",
    "\n",
    "print(f\"cardinality of train : {len(train_dataset_parquet)}, cardinality of validation : {len(val_dataset_parquet)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(train_dataset_parquet.dataset[:, 1], bins=len(train_dataset_parquet.unique_labels))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(val_dataset_parquet.dataset[:, 1], bins=len(val_dataset_parquet.unique_labels))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/113 [00:00<00:46,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 60, 1629) (8, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:40<00:00,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating through dataset took : 40.6401s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "isnans =False\n",
    "\n",
    "f = True\n",
    "labels_batches = []\n",
    "for el in tqdm(train_dataset_parquet):\n",
    "    if f:\n",
    "        print(el[0].shape, el[1].shape)\n",
    "        f = False\n",
    "    labels_batches.append(el[1])\n",
    "        \n",
    "    isnans |= np.any(np.isnan(el[0]))\n",
    "    if isnans:\n",
    "        print(\"FOUND NAN!\")\n",
    "        break\n",
    "\n",
    "\n",
    "print(f\"Iterating through dataset took : {round( time.time() - start , 4)}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, LeakyReLU\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras import backend as K\n",
    "from wandb.keras import WandbMetricsLogger\n",
    "from keras.callbacks import Callback\n",
    "import tensorflow as tf\n",
    "import wandb\n",
    "\n",
    "\n",
    "class CosineAnnealingLearningRateScheduler(Callback):\n",
    "    def __init__(self, max_lr, min_lr, T_max):\n",
    "        super(CosineAnnealingLearningRateScheduler, self).__init__()\n",
    "        self.max_lr = max_lr  # Maximum learning rate (i.e., start learning rate)\n",
    "        self.min_lr = min_lr  # Minimum learning rate\n",
    "        self.T_max = T_max    # Specifies the number of epochs per cycle\n",
    "        self.t = 0            # Current epoch\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.t += 1\n",
    "        cos = np.cos(np.pi * (self.t % self.T_max) / self.T_max)\n",
    "        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + cos)\n",
    "\n",
    "        keras.backend.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "def keras_train(model, filepath : str, max_lr = 1e-4, min_lr = 5e-5, T_max=50, epochs=100, run_name=\"\", mediapipe_features = \"all\",): \n",
    "    \n",
    "    \n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
    "                                                 monitor=\"val_categorical_accuracy\",\n",
    "                                                 verbose=0,\n",
    "                                                 save_best_only=True,\n",
    "                                                 mode=\"max\",\n",
    "                                                 save_freq=\"epoch\")\n",
    "    \n",
    "    cosine_annealer = CosineAnnealingLearningRateScheduler(max_lr=max_lr,\n",
    "                                                           min_lr=min_lr,\n",
    "                                                           T_max=T_max)\n",
    "    \n",
    "    #Adam Optimizer - fixed learning rate.\n",
    "    adam_optimizer = tf.keras.optimizers.Adam(learning_rate=max_lr, clipnorm=1.)\n",
    "    lr_metric = get_lr_metric(adam_optimizer)\n",
    "\n",
    "    model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    \n",
    "    \n",
    "    wandb.init(project=CONFIG.WANDB_RUN,\n",
    "                     name=run_name,\n",
    "                     notes=\"Model summary : \\n\" + str(model),\n",
    "                     config={\"max_lr\" : max_lr, \n",
    "                             \"min_lr\" : 5e-5, \n",
    "                             \"scheduler\" : \"cosineAnnealer\", \n",
    "                             \"epochs\" : epochs, \n",
    "                             \"T_max\" : T_max, \n",
    "                             \"train_size\" : len(train_dataset_parquet.dataset),\n",
    "                             \"val_size\" : len(val_dataset_parquet.dataset),\n",
    "                             \"unique_classes\" : len(train_dataset_parquet.unique_labels), \n",
    "                             \"video_length\" : CONFIG.VIDEO_LENGTH,\n",
    "                             \"features\" : mediapipe_features\n",
    "                             })\n",
    "    history = model.fit(train_dataset_parquet, epochs=epochs, validation_data = val_dataset_parquet, \n",
    "                        batch_size = 8, callbacks=[WandbMetricsLogger(), checkpoint, cosine_annealer])\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:xt6menui) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.010 MB uploaded\\r'), FloatProgress(value=0.5386814200092208, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM128-Dense128-Dense256-allfeatures</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/xt6menui' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/xt6menui</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240408_001326-xt6menui/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:xt6menui). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60635e698f649bd918534f83e36cd45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113241604632802, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240408_001704-mma9yanu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/mma9yanu' target=\"_blank\">LSTM128-Dense128-Dense256-allfeatures</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/mma9yanu' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/mma9yanu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "113/113 [==============================] - 50s 420ms/step - loss: 1.1593 - categorical_accuracy: 0.4256 - val_loss: 1.1256 - val_categorical_accuracy: 0.4400\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - 47s 419ms/step - loss: 1.1167 - categorical_accuracy: 0.4033 - val_loss: 1.0833 - val_categorical_accuracy: 0.4100\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - 47s 418ms/step - loss: 1.0962 - categorical_accuracy: 0.3978 - val_loss: 1.0736 - val_categorical_accuracy: 0.4300\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - 47s 418ms/step - loss: 1.0673 - categorical_accuracy: 0.4167 - val_loss: 1.0816 - val_categorical_accuracy: 0.4300\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - 48s 429ms/step - loss: 1.0626 - categorical_accuracy: 0.4289 - val_loss: 1.0159 - val_categorical_accuracy: 0.5600\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - 47s 421ms/step - loss: 1.0114 - categorical_accuracy: 0.4800 - val_loss: 0.9312 - val_categorical_accuracy: 0.5400\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - 48s 425ms/step - loss: 0.9373 - categorical_accuracy: 0.4933 - val_loss: 0.8931 - val_categorical_accuracy: 0.5100\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - 48s 426ms/step - loss: 0.9902 - categorical_accuracy: 0.4778 - val_loss: 0.8592 - val_categorical_accuracy: 0.5300\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - 47s 419ms/step - loss: 0.8961 - categorical_accuracy: 0.5333 - val_loss: 0.8399 - val_categorical_accuracy: 0.5700\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - 48s 423ms/step - loss: 0.8905 - categorical_accuracy: 0.5378 - val_loss: 0.8394 - val_categorical_accuracy: 0.5400\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - 48s 422ms/step - loss: 0.8506 - categorical_accuracy: 0.5622 - val_loss: 0.8261 - val_categorical_accuracy: 0.5600\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - 48s 422ms/step - loss: 0.8818 - categorical_accuracy: 0.5222 - val_loss: 0.8491 - val_categorical_accuracy: 0.5300\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - 48s 427ms/step - loss: 0.8148 - categorical_accuracy: 0.5689 - val_loss: 0.7962 - val_categorical_accuracy: 0.5900\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - 47s 419ms/step - loss: 0.7931 - categorical_accuracy: 0.5833 - val_loss: 0.7738 - val_categorical_accuracy: 0.5500\n",
      "Epoch 15/100\n",
      "113/113 [==============================] - 48s 424ms/step - loss: 0.8367 - categorical_accuracy: 0.5689 - val_loss: 0.7863 - val_categorical_accuracy: 0.5400\n",
      "Epoch 16/100\n",
      "113/113 [==============================] - 48s 422ms/step - loss: 0.8055 - categorical_accuracy: 0.5878 - val_loss: 0.7690 - val_categorical_accuracy: 0.6600\n",
      "Epoch 17/100\n",
      "113/113 [==============================] - 48s 422ms/step - loss: 0.7794 - categorical_accuracy: 0.5944 - val_loss: 0.8129 - val_categorical_accuracy: 0.5900\n",
      "Epoch 18/100\n",
      "113/113 [==============================] - 48s 422ms/step - loss: 0.8122 - categorical_accuracy: 0.6067 - val_loss: 0.7385 - val_categorical_accuracy: 0.6200\n",
      "Epoch 19/100\n",
      "113/113 [==============================] - 48s 424ms/step - loss: 0.7484 - categorical_accuracy: 0.6544 - val_loss: 0.6947 - val_categorical_accuracy: 0.7200\n",
      "Epoch 20/100\n",
      "113/113 [==============================] - 48s 431ms/step - loss: 0.6762 - categorical_accuracy: 0.7278 - val_loss: 0.6666 - val_categorical_accuracy: 0.7300\n",
      "Epoch 21/100\n",
      "113/113 [==============================] - 48s 425ms/step - loss: 0.6974 - categorical_accuracy: 0.7189 - val_loss: 0.7052 - val_categorical_accuracy: 0.7200\n",
      "Epoch 22/100\n",
      "113/113 [==============================] - 48s 425ms/step - loss: 0.6419 - categorical_accuracy: 0.7478 - val_loss: 0.5823 - val_categorical_accuracy: 0.7900\n",
      "Epoch 23/100\n",
      "113/113 [==============================] - 48s 425ms/step - loss: 0.6484 - categorical_accuracy: 0.7378 - val_loss: 0.5613 - val_categorical_accuracy: 0.8000\n",
      "Epoch 24/100\n",
      "113/113 [==============================] - 48s 425ms/step - loss: 0.5821 - categorical_accuracy: 0.7722 - val_loss: 0.5581 - val_categorical_accuracy: 0.8000\n",
      "Epoch 25/100\n",
      "113/113 [==============================] - 48s 424ms/step - loss: 0.5418 - categorical_accuracy: 0.8011 - val_loss: 0.6032 - val_categorical_accuracy: 0.7900\n",
      "Epoch 26/100\n",
      "113/113 [==============================] - 48s 426ms/step - loss: 0.5472 - categorical_accuracy: 0.8067 - val_loss: 0.5418 - val_categorical_accuracy: 0.7900\n",
      "Epoch 27/100\n",
      "113/113 [==============================] - 48s 419ms/step - loss: 0.5336 - categorical_accuracy: 0.7989 - val_loss: 0.6143 - val_categorical_accuracy: 0.7700\n",
      "Epoch 28/100\n",
      "113/113 [==============================] - 48s 425ms/step - loss: 0.5005 - categorical_accuracy: 0.8256 - val_loss: 0.5685 - val_categorical_accuracy: 0.7900\n",
      "Epoch 29/100\n",
      "113/113 [==============================] - 49s 431ms/step - loss: 0.4811 - categorical_accuracy: 0.8400 - val_loss: 0.4959 - val_categorical_accuracy: 0.8500\n",
      "Epoch 30/100\n",
      "113/113 [==============================] - 48s 422ms/step - loss: 0.5266 - categorical_accuracy: 0.8200 - val_loss: 0.4777 - val_categorical_accuracy: 0.8400\n",
      "Epoch 31/100\n",
      "113/113 [==============================] - 48s 428ms/step - loss: 0.5450 - categorical_accuracy: 0.8022 - val_loss: 0.4906 - val_categorical_accuracy: 0.8500\n",
      "Epoch 32/100\n",
      "113/113 [==============================] - 48s 418ms/step - loss: 0.4958 - categorical_accuracy: 0.8311 - val_loss: 0.4617 - val_categorical_accuracy: 0.8500\n",
      "Epoch 33/100\n",
      "113/113 [==============================] - 47s 418ms/step - loss: 0.5106 - categorical_accuracy: 0.8133 - val_loss: 0.6085 - val_categorical_accuracy: 0.8000\n",
      "Epoch 34/100\n",
      "113/113 [==============================] - 47s 413ms/step - loss: 0.4859 - categorical_accuracy: 0.8367 - val_loss: 0.4656 - val_categorical_accuracy: 0.8300\n",
      "Epoch 35/100\n",
      "113/113 [==============================] - 47s 420ms/step - loss: 0.4826 - categorical_accuracy: 0.8311 - val_loss: 0.6898 - val_categorical_accuracy: 0.7400\n",
      "Epoch 36/100\n",
      "113/113 [==============================] - 48s 421ms/step - loss: 0.5441 - categorical_accuracy: 0.8089 - val_loss: 0.8643 - val_categorical_accuracy: 0.6800\n",
      "Epoch 37/100\n",
      "113/113 [==============================] - 47s 413ms/step - loss: 0.4600 - categorical_accuracy: 0.8511 - val_loss: 0.4421 - val_categorical_accuracy: 0.8700\n",
      "Epoch 38/100\n",
      "113/113 [==============================] - 47s 417ms/step - loss: 0.4399 - categorical_accuracy: 0.8667 - val_loss: 0.4483 - val_categorical_accuracy: 0.8700\n",
      "Epoch 39/100\n",
      "113/113 [==============================] - 47s 417ms/step - loss: 0.5153 - categorical_accuracy: 0.8256 - val_loss: 0.4638 - val_categorical_accuracy: 0.8600\n",
      "Epoch 40/100\n",
      "113/113 [==============================] - 47s 411ms/step - loss: 0.5243 - categorical_accuracy: 0.8122 - val_loss: 0.6970 - val_categorical_accuracy: 0.7400\n",
      "Epoch 41/100\n",
      "113/113 [==============================] - 47s 416ms/step - loss: 0.5487 - categorical_accuracy: 0.8144 - val_loss: 0.4104 - val_categorical_accuracy: 0.8600\n",
      "Epoch 42/100\n",
      "113/113 [==============================] - 48s 428ms/step - loss: 0.4268 - categorical_accuracy: 0.8578 - val_loss: 0.4271 - val_categorical_accuracy: 0.8600\n",
      "Epoch 43/100\n",
      "113/113 [==============================] - 48s 427ms/step - loss: 0.4736 - categorical_accuracy: 0.8467 - val_loss: 0.6445 - val_categorical_accuracy: 0.7500\n",
      "Epoch 44/100\n",
      "113/113 [==============================] - 48s 419ms/step - loss: 0.5092 - categorical_accuracy: 0.8422 - val_loss: 0.5641 - val_categorical_accuracy: 0.7900\n",
      "Epoch 45/100\n",
      "113/113 [==============================] - 48s 425ms/step - loss: 0.5190 - categorical_accuracy: 0.8311 - val_loss: 0.6782 - val_categorical_accuracy: 0.7500\n",
      "Epoch 46/100\n",
      "113/113 [==============================] - 48s 425ms/step - loss: 0.4052 - categorical_accuracy: 0.8800 - val_loss: 0.4169 - val_categorical_accuracy: 0.8800\n",
      "Epoch 47/100\n",
      "113/113 [==============================] - 48s 428ms/step - loss: 0.3865 - categorical_accuracy: 0.8856 - val_loss: 0.4572 - val_categorical_accuracy: 0.8700\n",
      "Epoch 48/100\n",
      "113/113 [==============================] - 48s 425ms/step - loss: 0.5065 - categorical_accuracy: 0.8478 - val_loss: 0.4465 - val_categorical_accuracy: 0.8400\n",
      "Epoch 49/100\n",
      "113/113 [==============================] - 47s 418ms/step - loss: 0.4052 - categorical_accuracy: 0.8800 - val_loss: 0.8905 - val_categorical_accuracy: 0.6700\n",
      "Epoch 50/100\n",
      "113/113 [==============================] - 47s 416ms/step - loss: 0.4923 - categorical_accuracy: 0.8456 - val_loss: 0.4294 - val_categorical_accuracy: 0.8500\n",
      "Epoch 51/100\n",
      "113/113 [==============================] - 47s 416ms/step - loss: 0.3835 - categorical_accuracy: 0.8911 - val_loss: 0.5039 - val_categorical_accuracy: 0.8400\n",
      "Epoch 52/100\n",
      "113/113 [==============================] - 47s 419ms/step - loss: 0.3715 - categorical_accuracy: 0.9011 - val_loss: 0.3427 - val_categorical_accuracy: 0.9000\n",
      "Epoch 53/100\n",
      "113/113 [==============================] - 47s 414ms/step - loss: 0.4253 - categorical_accuracy: 0.8689 - val_loss: 0.4801 - val_categorical_accuracy: 0.8500\n",
      "Epoch 54/100\n",
      "113/113 [==============================] - 48s 425ms/step - loss: 0.3497 - categorical_accuracy: 0.8967 - val_loss: 0.4864 - val_categorical_accuracy: 0.8500\n",
      "Epoch 55/100\n",
      "113/113 [==============================] - 47s 420ms/step - loss: 0.3871 - categorical_accuracy: 0.8856 - val_loss: 0.5752 - val_categorical_accuracy: 0.8400\n",
      "Epoch 56/100\n",
      "113/113 [==============================] - 50s 439ms/step - loss: 0.4073 - categorical_accuracy: 0.8789 - val_loss: 0.5532 - val_categorical_accuracy: 0.8400\n",
      "Epoch 57/100\n",
      "113/113 [==============================] - 50s 440ms/step - loss: 0.4104 - categorical_accuracy: 0.8811 - val_loss: 0.4483 - val_categorical_accuracy: 0.8600\n",
      "Epoch 58/100\n",
      "113/113 [==============================] - 49s 433ms/step - loss: 0.3848 - categorical_accuracy: 0.8956 - val_loss: 0.5916 - val_categorical_accuracy: 0.7800\n",
      "Epoch 59/100\n",
      "113/113 [==============================] - 50s 442ms/step - loss: 0.4618 - categorical_accuracy: 0.8511 - val_loss: 0.4506 - val_categorical_accuracy: 0.8500\n",
      "Epoch 60/100\n",
      "113/113 [==============================] - 49s 437ms/step - loss: 0.5616 - categorical_accuracy: 0.8122 - val_loss: 0.4002 - val_categorical_accuracy: 0.8800\n",
      "Epoch 61/100\n",
      "113/113 [==============================] - 48s 424ms/step - loss: 0.4067 - categorical_accuracy: 0.8722 - val_loss: 0.4151 - val_categorical_accuracy: 0.8700\n",
      "Epoch 62/100\n",
      "113/113 [==============================] - 48s 425ms/step - loss: 0.3999 - categorical_accuracy: 0.8844 - val_loss: 0.4294 - val_categorical_accuracy: 0.8800\n",
      "Epoch 63/100\n",
      "113/113 [==============================] - 49s 429ms/step - loss: 0.4069 - categorical_accuracy: 0.8822 - val_loss: 0.4492 - val_categorical_accuracy: 0.8600\n",
      "Epoch 64/100\n",
      " 20/113 [====>.........................] - ETA: 35s - loss: 0.3361 - categorical_accuracy: 0.9000"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(128, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, 1629),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"LSTM1.keras\"),\n",
    "            run_name=\"LSTM128-Dense128-Dense256-allfeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(64, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, 1629),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"LSTM1.tf\"),\n",
    "            run_name=\"LSTM64-Dense128-Dense256-allfeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(256, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, 1629),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"LSTM256-Dense128-Dense256-allfeatures.tf\"),\n",
    "            run_name=\"LSTM256-Dense128-Dense256-allfeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(128, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, 1629),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"LSTM128-Dense256-allfeatures.tf\"),\n",
    "            run_name=\"LSTM128-Dense256-allfeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(128, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, 1629),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"LSTM128-Dense128-allfeatures.tf\"),\n",
    "            run_name=\"LSTM128-Dense128-allfeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(128, return_sequences=False, activation='relu', input_shape=(CONFIG.VIDEO_LENGTH, 1629),\n",
    "               kernel_regularizer=l2(0.0000001), \n",
    "               activity_regularizer=l2(0.0000001)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "0keras_train(model, filepath=os.path.join(\"models\", \"LSTM128_l2-Dense128-Dense256-allfeatures.tf\"),\n",
    "            run_name=\"LSTM128_l2-Dense128-Dense256-allfeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:g1jztv1q) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁</td></tr><tr><td>epoch/epoch</td><td>▁</td></tr><tr><td>epoch/learning_rate</td><td>▁</td></tr><tr><td>epoch/loss</td><td>▁</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁</td></tr><tr><td>epoch/val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.36889</td></tr><tr><td>epoch/epoch</td><td>0</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>2.82214</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.45</td></tr><tr><td>epoch/val_loss</td><td>2.39018</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM64-Dense128-Dense256-allfeatures_bigger_reg</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/g1jztv1q' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/g1jztv1q</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240408_085721-g1jztv1q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:g1jztv1q). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae32d08edf64e04a8009a32b4a48b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113070862160788, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240408_085904-b2cvthn0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/b2cvthn0' target=\"_blank\">LSTM64-Dense128-Dense256-allfeatures_bigger_reg</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/b2cvthn0' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/b2cvthn0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 2.8411 - categorical_accuracy: 0.4000INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 60s 499ms/step - loss: 2.8411 - categorical_accuracy: 0.4000 - val_loss: 2.4088 - val_categorical_accuracy: 0.4200\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 2.1274 - categorical_accuracy: 0.4056INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 502ms/step - loss: 2.1274 - categorical_accuracy: 0.4056 - val_loss: 1.8624 - val_categorical_accuracy: 0.4300\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - 49s 439ms/step - loss: 1.7043 - categorical_accuracy: 0.4044 - val_loss: 1.5451 - val_categorical_accuracy: 0.4300\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 1.4585 - categorical_accuracy: 0.4056 - val_loss: 1.3563 - val_categorical_accuracy: 0.4400\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - 50s 441ms/step - loss: 1.3106 - categorical_accuracy: 0.4056 - val_loss: 1.2412 - val_categorical_accuracy: 0.4300\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.2187 - categorical_accuracy: 0.4167INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 503ms/step - loss: 1.2187 - categorical_accuracy: 0.4167 - val_loss: 1.1653 - val_categorical_accuracy: 0.4200\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.1668 - categorical_accuracy: 0.4211INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 505ms/step - loss: 1.1668 - categorical_accuracy: 0.4211 - val_loss: 1.0656 - val_categorical_accuracy: 0.5000\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - 50s 442ms/step - loss: 1.1402 - categorical_accuracy: 0.4167 - val_loss: 1.0608 - val_categorical_accuracy: 0.5100\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0638 - categorical_accuracy: 0.4833INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 505ms/step - loss: 1.0638 - categorical_accuracy: 0.4833 - val_loss: 0.9921 - val_categorical_accuracy: 0.5600\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0240 - categorical_accuracy: 0.4889INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 500ms/step - loss: 1.0240 - categorical_accuracy: 0.4889 - val_loss: 1.0163 - val_categorical_accuracy: 0.5200\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0002 - categorical_accuracy: 0.5100INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 512ms/step - loss: 1.0002 - categorical_accuracy: 0.5100 - val_loss: 0.9776 - val_categorical_accuracy: 0.5200\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.9623 - categorical_accuracy: 0.5144INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 512ms/step - loss: 0.9623 - categorical_accuracy: 0.5144 - val_loss: 0.9214 - val_categorical_accuracy: 0.5500\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.9209 - categorical_accuracy: 0.5544INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 505ms/step - loss: 0.9209 - categorical_accuracy: 0.5544 - val_loss: 0.8942 - val_categorical_accuracy: 0.5600\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.9315 - categorical_accuracy: 0.5333 - val_loss: 0.9100 - val_categorical_accuracy: 0.5400\n",
      "Epoch 15/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8799 - categorical_accuracy: 0.5733INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 512ms/step - loss: 0.8799 - categorical_accuracy: 0.5733 - val_loss: 0.8732 - val_categorical_accuracy: 0.5500\n",
      "Epoch 16/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.8731 - categorical_accuracy: 0.5589 - val_loss: 0.8565 - val_categorical_accuracy: 0.5900\n",
      "Epoch 17/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.8757 - categorical_accuracy: 0.5556 - val_loss: 0.8595 - val_categorical_accuracy: 0.5800\n",
      "Epoch 18/100\n",
      "113/113 [==============================] - 50s 444ms/step - loss: 0.8616 - categorical_accuracy: 0.5733 - val_loss: 0.8362 - val_categorical_accuracy: 0.6100\n",
      "Epoch 19/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8751 - categorical_accuracy: 0.5800INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 505ms/step - loss: 0.8751 - categorical_accuracy: 0.5800 - val_loss: 0.8408 - val_categorical_accuracy: 0.5700\n",
      "Epoch 20/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8239 - categorical_accuracy: 0.6022INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 56s 502ms/step - loss: 0.8239 - categorical_accuracy: 0.6022 - val_loss: 0.8111 - val_categorical_accuracy: 0.6000\n",
      "Epoch 21/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8207 - categorical_accuracy: 0.6067INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 511ms/step - loss: 0.8207 - categorical_accuracy: 0.6067 - val_loss: 0.8012 - val_categorical_accuracy: 0.6100\n",
      "Epoch 22/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7945 - categorical_accuracy: 0.6511INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 510ms/step - loss: 0.7945 - categorical_accuracy: 0.6511 - val_loss: 0.7536 - val_categorical_accuracy: 0.6500\n",
      "Epoch 23/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.7951 - categorical_accuracy: 0.6400 - val_loss: 0.9351 - val_categorical_accuracy: 0.6400\n",
      "Epoch 24/100\n",
      "113/113 [==============================] - 50s 449ms/step - loss: 0.8354 - categorical_accuracy: 0.6267 - val_loss: 1.3834 - val_categorical_accuracy: 0.3300\n",
      "Epoch 25/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7755 - categorical_accuracy: 0.6633INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 56s 499ms/step - loss: 0.7755 - categorical_accuracy: 0.6633 - val_loss: 0.7559 - val_categorical_accuracy: 0.6800\n",
      "Epoch 26/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7431 - categorical_accuracy: 0.7156INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 515ms/step - loss: 0.7431 - categorical_accuracy: 0.7156 - val_loss: 0.8453 - val_categorical_accuracy: 0.5700\n",
      "Epoch 27/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7037 - categorical_accuracy: 0.7278INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 56s 502ms/step - loss: 0.7037 - categorical_accuracy: 0.7278 - val_loss: 0.7106 - val_categorical_accuracy: 0.7100\n",
      "Epoch 28/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6386 - categorical_accuracy: 0.7778INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 502ms/step - loss: 0.6386 - categorical_accuracy: 0.7778 - val_loss: 0.7482 - val_categorical_accuracy: 0.6600\n",
      "Epoch 29/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.6493 - categorical_accuracy: 0.7622 - val_loss: 0.5469 - val_categorical_accuracy: 0.8400\n",
      "Epoch 30/100\n",
      "113/113 [==============================] - 50s 441ms/step - loss: 0.6398 - categorical_accuracy: 0.7711 - val_loss: 0.5749 - val_categorical_accuracy: 0.7700\n",
      "Epoch 31/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6062 - categorical_accuracy: 0.7833INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 508ms/step - loss: 0.6062 - categorical_accuracy: 0.7833 - val_loss: 0.7759 - val_categorical_accuracy: 0.6900\n",
      "Epoch 32/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5734 - categorical_accuracy: 0.8022INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 510ms/step - loss: 0.5734 - categorical_accuracy: 0.8022 - val_loss: 0.6057 - val_categorical_accuracy: 0.7800\n",
      "Epoch 33/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5675 - categorical_accuracy: 0.8044INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 506ms/step - loss: 0.5675 - categorical_accuracy: 0.8044 - val_loss: 0.7343 - val_categorical_accuracy: 0.7800\n",
      "Epoch 34/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.5910 - categorical_accuracy: 0.7911 - val_loss: 0.5328 - val_categorical_accuracy: 0.8200\n",
      "Epoch 35/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.6100 - categorical_accuracy: 0.7944 - val_loss: 0.9632 - val_categorical_accuracy: 0.7100\n",
      "Epoch 36/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.6669 - categorical_accuracy: 0.7811 - val_loss: 0.5713 - val_categorical_accuracy: 0.8100\n",
      "Epoch 37/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.5737 - categorical_accuracy: 0.8033 - val_loss: 0.8288 - val_categorical_accuracy: 0.6900\n",
      "Epoch 38/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5330 - categorical_accuracy: 0.8344INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 510ms/step - loss: 0.5330 - categorical_accuracy: 0.8344 - val_loss: 0.8726 - val_categorical_accuracy: 0.6300\n",
      "Epoch 39/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.5579 - categorical_accuracy: 0.8111 - val_loss: 0.7435 - val_categorical_accuracy: 0.7500\n",
      "Epoch 40/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.5458 - categorical_accuracy: 0.8233 - val_loss: 0.8340 - val_categorical_accuracy: 0.7000\n",
      "Epoch 41/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.6143 - categorical_accuracy: 0.7856 - val_loss: 0.7396 - val_categorical_accuracy: 0.7700\n",
      "Epoch 42/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.5929 - categorical_accuracy: 0.7967 - val_loss: 0.7475 - val_categorical_accuracy: 0.6900\n",
      "Epoch 43/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.5422 - categorical_accuracy: 0.8333 - val_loss: 0.6000 - val_categorical_accuracy: 0.8200\n",
      "Epoch 44/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4798 - categorical_accuracy: 0.8567INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 507ms/step - loss: 0.4798 - categorical_accuracy: 0.8567 - val_loss: 0.6378 - val_categorical_accuracy: 0.7800\n",
      "Epoch 45/100\n",
      "113/113 [==============================] - 49s 438ms/step - loss: 0.5517 - categorical_accuracy: 0.8100 - val_loss: 0.6610 - val_categorical_accuracy: 0.8000\n",
      "Epoch 46/100\n",
      "113/113 [==============================] - 50s 444ms/step - loss: 0.5207 - categorical_accuracy: 0.8422 - val_loss: 0.5422 - val_categorical_accuracy: 0.8600\n",
      "Epoch 47/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.5088 - categorical_accuracy: 0.8489 - val_loss: 0.5806 - val_categorical_accuracy: 0.7900\n",
      "Epoch 48/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.5605 - categorical_accuracy: 0.8167 - val_loss: 0.4904 - val_categorical_accuracy: 0.8400\n",
      "Epoch 49/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.5473 - categorical_accuracy: 0.8278 - val_loss: 0.6565 - val_categorical_accuracy: 0.7600\n",
      "Epoch 50/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.5422 - categorical_accuracy: 0.8378 - val_loss: 0.6482 - val_categorical_accuracy: 0.8100\n",
      "Epoch 51/100\n",
      "113/113 [==============================] - 50s 444ms/step - loss: 0.4920 - categorical_accuracy: 0.8533 - val_loss: 0.9195 - val_categorical_accuracy: 0.7200\n",
      "Epoch 52/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.5176 - categorical_accuracy: 0.8456 - val_loss: 0.4050 - val_categorical_accuracy: 0.8900\n",
      "Epoch 53/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.5224 - categorical_accuracy: 0.8367 - val_loss: 0.6238 - val_categorical_accuracy: 0.8300\n",
      "Epoch 54/100\n",
      "113/113 [==============================] - 51s 446ms/step - loss: 0.4998 - categorical_accuracy: 0.8511 - val_loss: 0.7593 - val_categorical_accuracy: 0.6900\n",
      "Epoch 55/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4259 - categorical_accuracy: 0.8800INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 505ms/step - loss: 0.4259 - categorical_accuracy: 0.8800 - val_loss: 0.6574 - val_categorical_accuracy: 0.7800\n",
      "Epoch 56/100\n",
      "113/113 [==============================] - 51s 446ms/step - loss: 0.5203 - categorical_accuracy: 0.8444 - val_loss: 0.5315 - val_categorical_accuracy: 0.8300\n",
      "Epoch 57/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.5094 - categorical_accuracy: 0.8478 - val_loss: 0.6249 - val_categorical_accuracy: 0.8100\n",
      "Epoch 58/100\n",
      "113/113 [==============================] - 51s 447ms/step - loss: 0.5697 - categorical_accuracy: 0.8200 - val_loss: 0.4367 - val_categorical_accuracy: 0.9000\n",
      "Epoch 59/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.4575 - categorical_accuracy: 0.8767 - val_loss: 0.6292 - val_categorical_accuracy: 0.8000\n",
      "Epoch 65/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.4987 - categorical_accuracy: 0.8511 - val_loss: 0.5974 - val_categorical_accuracy: 0.8300\n",
      "Epoch 66/100\n",
      "113/113 [==============================] - 50s 448ms/step - loss: 0.5852 - categorical_accuracy: 0.8289 - val_loss: 0.6959 - val_categorical_accuracy: 0.7500\n",
      "Epoch 67/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.5027 - categorical_accuracy: 0.8533 - val_loss: 0.5093 - val_categorical_accuracy: 0.8500\n",
      "Epoch 68/100\n",
      "113/113 [==============================] - 50s 441ms/step - loss: 0.4957 - categorical_accuracy: 0.8622 - val_loss: 0.4846 - val_categorical_accuracy: 0.8500\n",
      "Epoch 69/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.5055 - categorical_accuracy: 0.8533 - val_loss: 0.5980 - val_categorical_accuracy: 0.8600\n",
      "Epoch 70/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.5464 - categorical_accuracy: 0.8367 - val_loss: 0.5365 - val_categorical_accuracy: 0.8400\n",
      "Epoch 71/100\n",
      "113/113 [==============================] - 50s 448ms/step - loss: 0.4602 - categorical_accuracy: 0.8689 - val_loss: 0.4441 - val_categorical_accuracy: 0.8800\n",
      "Epoch 72/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.5463 - categorical_accuracy: 0.8189 - val_loss: 0.5223 - val_categorical_accuracy: 0.8000\n",
      "Epoch 73/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4312 - categorical_accuracy: 0.8878INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 513ms/step - loss: 0.4312 - categorical_accuracy: 0.8878 - val_loss: 0.4676 - val_categorical_accuracy: 0.8600\n",
      "Epoch 74/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.4691 - categorical_accuracy: 0.8589 - val_loss: 0.4604 - val_categorical_accuracy: 0.8700\n",
      "Epoch 75/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.4864 - categorical_accuracy: 0.8567 - val_loss: 0.4465 - val_categorical_accuracy: 0.9000\n",
      "Epoch 76/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.4778 - categorical_accuracy: 0.8678 - val_loss: 0.4316 - val_categorical_accuracy: 0.8800\n",
      "Epoch 77/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.4969 - categorical_accuracy: 0.8444 - val_loss: 0.4481 - val_categorical_accuracy: 0.8800\n",
      "Epoch 78/100\n",
      "113/113 [==============================] - 50s 448ms/step - loss: 0.4584 - categorical_accuracy: 0.8644 - val_loss: 0.4431 - val_categorical_accuracy: 0.9100\n",
      "Epoch 79/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4070 - categorical_accuracy: 0.8900INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 505ms/step - loss: 0.4070 - categorical_accuracy: 0.8900 - val_loss: 0.4321 - val_categorical_accuracy: 0.8800\n",
      "Epoch 80/100\n",
      "113/113 [==============================] - 50s 442ms/step - loss: 0.4880 - categorical_accuracy: 0.8633 - val_loss: 0.4699 - val_categorical_accuracy: 0.8800\n",
      "Epoch 81/100\n",
      "113/113 [==============================] - 50s 439ms/step - loss: 0.4324 - categorical_accuracy: 0.8822 - val_loss: 0.4674 - val_categorical_accuracy: 0.8600\n",
      "Epoch 82/100\n",
      "113/113 [==============================] - 50s 439ms/step - loss: 0.4607 - categorical_accuracy: 0.8722 - val_loss: 0.4195 - val_categorical_accuracy: 0.9000\n",
      "Epoch 83/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.5813 - categorical_accuracy: 0.8144 - val_loss: 0.7107 - val_categorical_accuracy: 0.8300\n",
      "Epoch 84/100\n",
      "113/113 [==============================] - 50s 440ms/step - loss: 0.5578 - categorical_accuracy: 0.8311 - val_loss: 0.4452 - val_categorical_accuracy: 0.8800\n",
      "Epoch 85/100\n",
      "113/113 [==============================] - 50s 439ms/step - loss: 0.4656 - categorical_accuracy: 0.8722 - val_loss: 0.4972 - val_categorical_accuracy: 0.8600\n",
      "Epoch 86/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.4155 - categorical_accuracy: 0.8800 - val_loss: 0.7409 - val_categorical_accuracy: 0.8000\n",
      "Epoch 87/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.5258 - categorical_accuracy: 0.8367 - val_loss: 0.4980 - val_categorical_accuracy: 0.8300\n",
      "Epoch 88/100\n",
      "113/113 [==============================] - 50s 442ms/step - loss: 0.4853 - categorical_accuracy: 0.8689 - val_loss: 0.5915 - val_categorical_accuracy: 0.8200\n",
      "Epoch 89/100\n",
      "113/113 [==============================] - 50s 439ms/step - loss: 0.4376 - categorical_accuracy: 0.8811 - val_loss: 0.4558 - val_categorical_accuracy: 0.8700\n",
      "Epoch 90/100\n",
      "113/113 [==============================] - 50s 441ms/step - loss: 0.4365 - categorical_accuracy: 0.8822 - val_loss: 0.5814 - val_categorical_accuracy: 0.8300\n",
      "Epoch 91/100\n",
      "113/113 [==============================] - 50s 440ms/step - loss: 0.5081 - categorical_accuracy: 0.8378 - val_loss: 0.7659 - val_categorical_accuracy: 0.7900\n",
      "Epoch 92/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.4477 - categorical_accuracy: 0.8767 - val_loss: 0.4377 - val_categorical_accuracy: 0.8800\n",
      "Epoch 93/100\n",
      "113/113 [==============================] - 50s 438ms/step - loss: 0.4153 - categorical_accuracy: 0.8900 - val_loss: 0.3688 - val_categorical_accuracy: 0.9100\n",
      "Epoch 94/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4063 - categorical_accuracy: 0.8922INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 56s 498ms/step - loss: 0.4063 - categorical_accuracy: 0.8922 - val_loss: 0.4536 - val_categorical_accuracy: 0.8800\n",
      "Epoch 95/100\n",
      "113/113 [==============================] - 51s 444ms/step - loss: 0.4751 - categorical_accuracy: 0.8656 - val_loss: 0.5127 - val_categorical_accuracy: 0.8400\n",
      "Epoch 96/100\n",
      "113/113 [==============================] - 50s 442ms/step - loss: 0.4397 - categorical_accuracy: 0.8767 - val_loss: 0.4737 - val_categorical_accuracy: 0.8700\n",
      "Epoch 97/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.4376 - categorical_accuracy: 0.8722 - val_loss: 0.4013 - val_categorical_accuracy: 0.8800\n",
      "Epoch 98/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.4417 - categorical_accuracy: 0.8722 - val_loss: 0.4582 - val_categorical_accuracy: 0.8600\n",
      "Epoch 99/100\n",
      "113/113 [==============================] - 50s 441ms/step - loss: 0.4192 - categorical_accuracy: 0.8867 - val_loss: 0.4077 - val_categorical_accuracy: 0.9000\n",
      "Epoch 100/100\n",
      "113/113 [==============================] - 50s 444ms/step - loss: 0.5048 - categorical_accuracy: 0.8467 - val_loss: 0.6696 - val_categorical_accuracy: 0.8000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁▁▁▁▃▃▃▃▄▄▅▆▆▇▆▇▆▇▇▇▇▇▇█▇█▇▇▇█▇██▇▇████▇</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▅▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁▁▁▂▂▃▃▄▄▄▃▅▅▇▇▆▆▆▇▆▅▅▇▇▇▇▇▇▇█████▇████▇</td></tr><tr><td>epoch/val_loss</td><td>█▅▄▃▃▃▃▃▂▃▃▂▂▁▂▂▂▂▁▂▃▂▁▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.84667</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>0.50475</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.8</td></tr><tr><td>epoch/val_loss</td><td>0.66957</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM64-Dense128-Dense256-allfeatures_bigger_reg</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/b2cvthn0' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/b2cvthn0</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240408_085904-b2cvthn0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fda6bc02690>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(64, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, 1629),\n",
    "               kernel_regularizer=l2(0.005), \n",
    "               activity_regularizer=l2(0.005)))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"LSTM-L64-D128-D256-reg=0.005.tf\"),\n",
    "            run_name=\"LSTM64-Dense128-Dense256-allfeatures_bigger_reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ehqx0ymo) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.010 MB uploaded\\r'), FloatProgress(value=0.5380860274477296, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM128-Dense128-Dense256-allfeatures</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/ehqx0ymo' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/ehqx0ymo</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240408_102806-ehqx0ymo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ehqx0ymo). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02c443cf9c24d67afce99e0dd73feaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111298907134268, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240408_102839-tfbow8kv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/tfbow8kv' target=\"_blank\">LSTM128-Dense128-Dense256-allfeatures</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/tfbow8kv' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/tfbow8kv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.1593 - categorical_accuracy: 0.4111INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 59s 497ms/step - loss: 1.1593 - categorical_accuracy: 0.4111 - val_loss: 1.1264 - val_categorical_accuracy: 0.4300\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - 50s 440ms/step - loss: 1.1159 - categorical_accuracy: 0.4122 - val_loss: 1.0794 - val_categorical_accuracy: 0.4300\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - 50s 444ms/step - loss: 1.0872 - categorical_accuracy: 0.4156 - val_loss: 1.0639 - val_categorical_accuracy: 0.4300\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0789 - categorical_accuracy: 0.4222INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 504ms/step - loss: 1.0789 - categorical_accuracy: 0.4222 - val_loss: 1.0364 - val_categorical_accuracy: 0.4500\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0382 - categorical_accuracy: 0.4522INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 504ms/step - loss: 1.0382 - categorical_accuracy: 0.4522 - val_loss: 0.9582 - val_categorical_accuracy: 0.4800\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.9295 - categorical_accuracy: 0.5167INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 56s 500ms/step - loss: 0.9295 - categorical_accuracy: 0.5167 - val_loss: 0.8747 - val_categorical_accuracy: 0.5400\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.8825 - categorical_accuracy: 0.5411 - val_loss: 0.8312 - val_categorical_accuracy: 0.5300\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8269 - categorical_accuracy: 0.5622INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 509ms/step - loss: 0.8269 - categorical_accuracy: 0.5622 - val_loss: 0.8254 - val_categorical_accuracy: 0.5500\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8556 - categorical_accuracy: 0.5433INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 507ms/step - loss: 0.8556 - categorical_accuracy: 0.5433 - val_loss: 0.8159 - val_categorical_accuracy: 0.6200\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.8348 - categorical_accuracy: 0.5756 - val_loss: 0.7958 - val_categorical_accuracy: 0.5500\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - 50s 442ms/step - loss: 0.8885 - categorical_accuracy: 0.5233 - val_loss: 0.9448 - val_categorical_accuracy: 0.5100\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.8151 - categorical_accuracy: 0.5722 - val_loss: 0.8031 - val_categorical_accuracy: 0.5500\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.7978 - categorical_accuracy: 0.5844 - val_loss: 0.8093 - val_categorical_accuracy: 0.5400\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - 50s 444ms/step - loss: 0.7737 - categorical_accuracy: 0.5944 - val_loss: 0.7904 - val_categorical_accuracy: 0.5900\n",
      "Epoch 15/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.7715 - categorical_accuracy: 0.6011 - val_loss: 0.7445 - val_categorical_accuracy: 0.6100\n",
      "Epoch 16/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.7729 - categorical_accuracy: 0.6289 - val_loss: 0.7602 - val_categorical_accuracy: 0.6000\n",
      "Epoch 17/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7333 - categorical_accuracy: 0.6744INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 506ms/step - loss: 0.7333 - categorical_accuracy: 0.6744 - val_loss: 0.7445 - val_categorical_accuracy: 0.6900\n",
      "Epoch 18/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6788 - categorical_accuracy: 0.7311INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 503ms/step - loss: 0.6788 - categorical_accuracy: 0.7311 - val_loss: 0.6787 - val_categorical_accuracy: 0.7300\n",
      "Epoch 19/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6468 - categorical_accuracy: 0.7200INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 508ms/step - loss: 0.6468 - categorical_accuracy: 0.7200 - val_loss: 0.5513 - val_categorical_accuracy: 0.8400\n",
      "Epoch 20/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.6418 - categorical_accuracy: 0.7500 - val_loss: 0.5769 - val_categorical_accuracy: 0.7900\n",
      "Epoch 21/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.5473 - categorical_accuracy: 0.8067 - val_loss: 0.5113 - val_categorical_accuracy: 0.8200\n",
      "Epoch 22/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.5497 - categorical_accuracy: 0.8022 - val_loss: 0.4657 - val_categorical_accuracy: 0.8200\n",
      "Epoch 23/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5541 - categorical_accuracy: 0.7900INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 509ms/step - loss: 0.5541 - categorical_accuracy: 0.7900 - val_loss: 0.5049 - val_categorical_accuracy: 0.8500\n",
      "Epoch 24/100\n",
      "113/113 [==============================] - 50s 444ms/step - loss: 0.5154 - categorical_accuracy: 0.7989 - val_loss: 0.5346 - val_categorical_accuracy: 0.7900\n",
      "Epoch 25/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.5237 - categorical_accuracy: 0.8133 - val_loss: 0.4664 - val_categorical_accuracy: 0.8300\n",
      "Epoch 26/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.5831 - categorical_accuracy: 0.8033 - val_loss: 0.5966 - val_categorical_accuracy: 0.8000\n",
      "Epoch 27/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.5225 - categorical_accuracy: 0.8033 - val_loss: 0.7948 - val_categorical_accuracy: 0.7300\n",
      "Epoch 28/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.5959 - categorical_accuracy: 0.7822 - val_loss: 0.6207 - val_categorical_accuracy: 0.7900\n",
      "Epoch 29/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4713 - categorical_accuracy: 0.8367INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 512ms/step - loss: 0.4713 - categorical_accuracy: 0.8367 - val_loss: 0.4706 - val_categorical_accuracy: 0.8600\n",
      "Epoch 30/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.4891 - categorical_accuracy: 0.8333 - val_loss: 0.4781 - val_categorical_accuracy: 0.8400\n",
      "Epoch 31/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.5176 - categorical_accuracy: 0.8200 - val_loss: 0.5093 - val_categorical_accuracy: 0.8400\n",
      "Epoch 32/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.4791 - categorical_accuracy: 0.8400 - val_loss: 0.4815 - val_categorical_accuracy: 0.8500\n",
      "Epoch 33/100\n",
      "113/113 [==============================] - 50s 448ms/step - loss: 0.4326 - categorical_accuracy: 0.8689 - val_loss: 0.5219 - val_categorical_accuracy: 0.8000\n",
      "Epoch 34/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4221 - categorical_accuracy: 0.8611INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 509ms/step - loss: 0.4221 - categorical_accuracy: 0.8611 - val_loss: 0.4338 - val_categorical_accuracy: 0.8700\n",
      "Epoch 35/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.4176 - categorical_accuracy: 0.8600 - val_loss: 0.5111 - val_categorical_accuracy: 0.8100\n",
      "Epoch 36/100\n",
      "113/113 [==============================] - 50s 441ms/step - loss: 0.3978 - categorical_accuracy: 0.8722 - val_loss: 0.4874 - val_categorical_accuracy: 0.8100\n",
      "Epoch 37/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4454 - categorical_accuracy: 0.8511INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 504ms/step - loss: 0.4454 - categorical_accuracy: 0.8511 - val_loss: 0.3816 - val_categorical_accuracy: 0.8900\n",
      "Epoch 38/100\n",
      "113/113 [==============================] - 49s 437ms/step - loss: 0.3786 - categorical_accuracy: 0.8889 - val_loss: 0.4801 - val_categorical_accuracy: 0.8500\n",
      "Epoch 39/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.3902 - categorical_accuracy: 0.8922 - val_loss: 0.5937 - val_categorical_accuracy: 0.8000\n",
      "Epoch 40/100\n",
      "113/113 [==============================] - 50s 441ms/step - loss: 0.3852 - categorical_accuracy: 0.8889 - val_loss: 0.4676 - val_categorical_accuracy: 0.8500\n",
      "Epoch 41/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.3807 - categorical_accuracy: 0.8856 - val_loss: 0.4871 - val_categorical_accuracy: 0.8300\n",
      "Epoch 42/100\n",
      "113/113 [==============================] - 50s 444ms/step - loss: 0.3750 - categorical_accuracy: 0.8822 - val_loss: 0.4893 - val_categorical_accuracy: 0.8600\n",
      "Epoch 43/100\n",
      "113/113 [==============================] - 50s 441ms/step - loss: 0.4044 - categorical_accuracy: 0.8911 - val_loss: 0.6310 - val_categorical_accuracy: 0.7900\n",
      "Epoch 44/100\n",
      "113/113 [==============================] - 50s 442ms/step - loss: 0.3771 - categorical_accuracy: 0.8867 - val_loss: 0.4822 - val_categorical_accuracy: 0.8500\n",
      "Epoch 45/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.3903 - categorical_accuracy: 0.8844INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 56s 499ms/step - loss: 0.3903 - categorical_accuracy: 0.8844 - val_loss: 0.3689 - val_categorical_accuracy: 0.9000\n",
      "Epoch 46/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.3459 - categorical_accuracy: 0.9078 - val_loss: 0.3768 - val_categorical_accuracy: 0.9000\n",
      "Epoch 47/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3704 - categorical_accuracy: 0.8956 - val_loss: 0.4289 - val_categorical_accuracy: 0.8800\n",
      "Epoch 48/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.3126 - categorical_accuracy: 0.9122 - val_loss: 0.4029 - val_categorical_accuracy: 0.8800\n",
      "Epoch 49/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3530 - categorical_accuracy: 0.8922 - val_loss: 0.4759 - val_categorical_accuracy: 0.8700\n",
      "Epoch 50/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.4171 - categorical_accuracy: 0.8767 - val_loss: 0.6324 - val_categorical_accuracy: 0.7900\n",
      "Epoch 51/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.4903 - categorical_accuracy: 0.8400 - val_loss: 0.4580 - val_categorical_accuracy: 0.8600\n",
      "Epoch 52/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.5004 - categorical_accuracy: 0.8322 - val_loss: 0.7386 - val_categorical_accuracy: 0.7800\n",
      "Epoch 53/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.5495 - categorical_accuracy: 0.8144 - val_loss: 0.5707 - val_categorical_accuracy: 0.8200\n",
      "Epoch 54/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.4326 - categorical_accuracy: 0.8522 - val_loss: 0.4949 - val_categorical_accuracy: 0.8600\n",
      "Epoch 55/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.4168 - categorical_accuracy: 0.8711 - val_loss: 0.3718 - val_categorical_accuracy: 0.8700\n",
      "Epoch 56/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.4029 - categorical_accuracy: 0.8656 - val_loss: 0.3996 - val_categorical_accuracy: 0.8900\n",
      "Epoch 57/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.3701 - categorical_accuracy: 0.8833 - val_loss: 0.4646 - val_categorical_accuracy: 0.8700\n",
      "Epoch 58/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.3946 - categorical_accuracy: 0.8789 - val_loss: 0.7404 - val_categorical_accuracy: 0.7200\n",
      "Epoch 59/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.3892 - categorical_accuracy: 0.8789 - val_loss: 0.4167 - val_categorical_accuracy: 0.8900\n",
      "Epoch 60/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.3286 - categorical_accuracy: 0.8978 - val_loss: 0.4500 - val_categorical_accuracy: 0.8700\n",
      "Epoch 61/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.4057 - categorical_accuracy: 0.8778 - val_loss: 0.5199 - val_categorical_accuracy: 0.8400\n",
      "Epoch 62/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.4515 - categorical_accuracy: 0.8611 - val_loss: 0.4424 - val_categorical_accuracy: 0.8700\n",
      "Epoch 63/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.4480 - categorical_accuracy: 0.8656 - val_loss: 0.4351 - val_categorical_accuracy: 0.8900\n",
      "Epoch 64/100\n",
      "113/113 [==============================] - 51s 447ms/step - loss: 0.3490 - categorical_accuracy: 0.8967 - val_loss: 0.5264 - val_categorical_accuracy: 0.8400\n",
      "Epoch 65/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3577 - categorical_accuracy: 0.9011 - val_loss: 0.4346 - val_categorical_accuracy: 0.8300\n",
      "Epoch 66/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.3784 - categorical_accuracy: 0.8800 - val_loss: 0.4018 - val_categorical_accuracy: 0.8700\n",
      "Epoch 67/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.3576 - categorical_accuracy: 0.8956 - val_loss: 0.5138 - val_categorical_accuracy: 0.8200\n",
      "Epoch 68/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.3531 - categorical_accuracy: 0.8933 - val_loss: 0.4497 - val_categorical_accuracy: 0.8500\n",
      "Epoch 69/100\n",
      "113/113 [==============================] - 51s 446ms/step - loss: 0.3818 - categorical_accuracy: 0.8944 - val_loss: 0.5373 - val_categorical_accuracy: 0.8200\n",
      "Epoch 70/100\n",
      "113/113 [==============================] - 51s 447ms/step - loss: 0.3838 - categorical_accuracy: 0.8922 - val_loss: 0.5568 - val_categorical_accuracy: 0.8300\n",
      "Epoch 71/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.2919 - categorical_accuracy: 0.9233 - val_loss: 0.4551 - val_categorical_accuracy: 0.8700\n",
      "Epoch 72/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3211 - categorical_accuracy: 0.9078 - val_loss: 0.4649 - val_categorical_accuracy: 0.8700\n",
      "Epoch 73/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.3066 - categorical_accuracy: 0.9122 - val_loss: 0.4409 - val_categorical_accuracy: 0.8600\n",
      "Epoch 74/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.3514 - categorical_accuracy: 0.8922 - val_loss: 0.4000 - val_categorical_accuracy: 0.8900\n",
      "Epoch 75/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.3193 - categorical_accuracy: 0.9122 - val_loss: 0.3849 - val_categorical_accuracy: 0.8700\n",
      "Epoch 76/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.3559 - categorical_accuracy: 0.8922 - val_loss: 0.4093 - val_categorical_accuracy: 0.8900\n",
      "Epoch 77/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.2585 - categorical_accuracy: 0.9344 - val_loss: 0.3891 - val_categorical_accuracy: 0.8900\n",
      "Epoch 78/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.2845 - categorical_accuracy: 0.9200INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 506ms/step - loss: 0.2845 - categorical_accuracy: 0.9200 - val_loss: 0.3175 - val_categorical_accuracy: 0.9200\n",
      "Epoch 79/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.3147 - categorical_accuracy: 0.9156 - val_loss: 0.3852 - val_categorical_accuracy: 0.8900\n",
      "Epoch 80/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.2673 - categorical_accuracy: 0.9200 - val_loss: 0.3926 - val_categorical_accuracy: 0.9000\n",
      "Epoch 81/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.2707 - categorical_accuracy: 0.9256 - val_loss: 0.3431 - val_categorical_accuracy: 0.9000\n",
      "Epoch 82/100\n",
      "  6/113 [>.............................] - ETA: 38s - loss: 0.2516 - categorical_accuracy: 0.9375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(128, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, 1629),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"LSTM128-Dense128-Dense256-allfeatures.tf\"),\n",
    "            run_name=\"LSTM128-Dense128-Dense256-allfeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:vxwg2tiq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁</td></tr><tr><td>epoch/epoch</td><td>▁</td></tr><tr><td>epoch/learning_rate</td><td>▁</td></tr><tr><td>epoch/loss</td><td>▁</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁</td></tr><tr><td>epoch/val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.41667</td></tr><tr><td>epoch/epoch</td><td>0</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>1.13316</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.44</td></tr><tr><td>epoch/val_loss</td><td>1.11618</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM64-Dense64-allfeatures</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/vxwg2tiq' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/vxwg2tiq</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240408_124116-vxwg2tiq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:vxwg2tiq). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a843215891450f8950dbbea742785b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112615217765172, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240408_124402-tyh5w2ae</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/tyh5w2ae' target=\"_blank\">LSTM64-Dense64-allfeatures</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/tyh5w2ae' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/tyh5w2ae</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.1343 - categorical_accuracy: 0.4444INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 59s 497ms/step - loss: 1.1343 - categorical_accuracy: 0.4444 - val_loss: 1.1175 - val_categorical_accuracy: 0.4300\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 1.1170 - categorical_accuracy: 0.4256 - val_loss: 1.0923 - val_categorical_accuracy: 0.4300\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - 52s 454ms/step - loss: 1.0976 - categorical_accuracy: 0.4200 - val_loss: 1.0688 - val_categorical_accuracy: 0.4300\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 1.0772 - categorical_accuracy: 0.4267 - val_loss: 1.0476 - val_categorical_accuracy: 0.4300\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 1.0660 - categorical_accuracy: 0.4222 - val_loss: 1.0532 - val_categorical_accuracy: 0.4300\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 1.0527 - categorical_accuracy: 0.4122 - val_loss: 1.0162 - val_categorical_accuracy: 0.4300\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0104 - categorical_accuracy: 0.4322INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 518ms/step - loss: 1.0104 - categorical_accuracy: 0.4322 - val_loss: 0.9230 - val_categorical_accuracy: 0.5300\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.9788 - categorical_accuracy: 0.4911 - val_loss: 1.0028 - val_categorical_accuracy: 0.4700\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.9554 - categorical_accuracy: 0.5089INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 512ms/step - loss: 0.9554 - categorical_accuracy: 0.5089 - val_loss: 0.9205 - val_categorical_accuracy: 0.5400\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8856 - categorical_accuracy: 0.5389INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 514ms/step - loss: 0.8856 - categorical_accuracy: 0.5389 - val_loss: 0.8232 - val_categorical_accuracy: 0.5600\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.8843 - categorical_accuracy: 0.5400 - val_loss: 0.8389 - val_categorical_accuracy: 0.5600\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.8609 - categorical_accuracy: 0.5311 - val_loss: 0.8181 - val_categorical_accuracy: 0.5600\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.8422 - categorical_accuracy: 0.5622 - val_loss: 0.8075 - val_categorical_accuracy: 0.5500\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.8400 - categorical_accuracy: 0.5556 - val_loss: 0.8344 - val_categorical_accuracy: 0.5600\n",
      "Epoch 15/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.8375 - categorical_accuracy: 0.5533 - val_loss: 0.8198 - val_categorical_accuracy: 0.5400\n",
      "Epoch 16/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.8186 - categorical_accuracy: 0.5600 - val_loss: 0.8137 - val_categorical_accuracy: 0.5500\n",
      "Epoch 17/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.8218 - categorical_accuracy: 0.5478 - val_loss: 0.8052 - val_categorical_accuracy: 0.5300\n",
      "Epoch 18/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.8168 - categorical_accuracy: 0.5567 - val_loss: 0.8125 - val_categorical_accuracy: 0.5600\n",
      "Epoch 19/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7975 - categorical_accuracy: 0.5889INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 514ms/step - loss: 0.7975 - categorical_accuracy: 0.5889 - val_loss: 0.7677 - val_categorical_accuracy: 0.5700\n",
      "Epoch 20/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7678 - categorical_accuracy: 0.5900INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 510ms/step - loss: 0.7678 - categorical_accuracy: 0.5900 - val_loss: 0.7368 - val_categorical_accuracy: 0.6100\n",
      "Epoch 21/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.7924 - categorical_accuracy: 0.5856 - val_loss: 0.8150 - val_categorical_accuracy: 0.5900\n",
      "Epoch 22/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.7752 - categorical_accuracy: 0.6200 - val_loss: 0.8004 - val_categorical_accuracy: 0.5900\n",
      "Epoch 23/100\n",
      "113/113 [==============================] - 51s 446ms/step - loss: 0.7590 - categorical_accuracy: 0.6222 - val_loss: 0.7709 - val_categorical_accuracy: 0.6000\n",
      "Epoch 24/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7470 - categorical_accuracy: 0.6400INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 503ms/step - loss: 0.7470 - categorical_accuracy: 0.6400 - val_loss: 0.7407 - val_categorical_accuracy: 0.6800\n",
      "Epoch 25/100\n",
      "113/113 [==============================] - 51s 456ms/step - loss: 0.6949 - categorical_accuracy: 0.6856 - val_loss: 0.6845 - val_categorical_accuracy: 0.6800\n",
      "Epoch 26/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7088 - categorical_accuracy: 0.7156INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 515ms/step - loss: 0.7088 - categorical_accuracy: 0.7156 - val_loss: 0.6359 - val_categorical_accuracy: 0.7800\n",
      "Epoch 27/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.6057 - categorical_accuracy: 0.7789 - val_loss: 0.6122 - val_categorical_accuracy: 0.7800\n",
      "Epoch 28/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.5726 - categorical_accuracy: 0.7922 - val_loss: 0.5490 - val_categorical_accuracy: 0.7800\n",
      "Epoch 29/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.5773 - categorical_accuracy: 0.7700 - val_loss: 0.6536 - val_categorical_accuracy: 0.7500\n",
      "Epoch 30/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.6203 - categorical_accuracy: 0.7578 - val_loss: 0.5957 - val_categorical_accuracy: 0.7600\n",
      "Epoch 31/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5099 - categorical_accuracy: 0.8278INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 59s 525ms/step - loss: 0.5099 - categorical_accuracy: 0.8278 - val_loss: 0.5750 - val_categorical_accuracy: 0.7900\n",
      "Epoch 32/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.5296 - categorical_accuracy: 0.8056 - val_loss: 0.6380 - val_categorical_accuracy: 0.7300\n",
      "Epoch 33/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.5025 - categorical_accuracy: 0.8167 - val_loss: 0.6393 - val_categorical_accuracy: 0.7700\n",
      "Epoch 34/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.5316 - categorical_accuracy: 0.8089 - val_loss: 0.6577 - val_categorical_accuracy: 0.7400\n",
      "Epoch 35/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5025 - categorical_accuracy: 0.8278INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 503ms/step - loss: 0.5025 - categorical_accuracy: 0.8278 - val_loss: 0.5503 - val_categorical_accuracy: 0.8000\n",
      "Epoch 36/100\n",
      "113/113 [==============================] - 52s 460ms/step - loss: 0.4798 - categorical_accuracy: 0.8322 - val_loss: 0.5658 - val_categorical_accuracy: 0.7900\n",
      "Epoch 37/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4456 - categorical_accuracy: 0.8544INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 510ms/step - loss: 0.4456 - categorical_accuracy: 0.8544 - val_loss: 0.5746 - val_categorical_accuracy: 0.8100\n",
      "Epoch 38/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.4612 - categorical_accuracy: 0.8422 - val_loss: 0.6220 - val_categorical_accuracy: 0.7800\n",
      "Epoch 39/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.4847 - categorical_accuracy: 0.8411 - val_loss: 0.5608 - val_categorical_accuracy: 0.7900\n",
      "Epoch 40/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.4923 - categorical_accuracy: 0.8311 - val_loss: 0.5668 - val_categorical_accuracy: 0.7900\n",
      "Epoch 41/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.4842 - categorical_accuracy: 0.8278 - val_loss: 0.7195 - val_categorical_accuracy: 0.7000\n",
      "Epoch 42/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.4352 - categorical_accuracy: 0.8600 - val_loss: 0.5334 - val_categorical_accuracy: 0.8000\n",
      "Epoch 43/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4023 - categorical_accuracy: 0.8844INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 59s 523ms/step - loss: 0.4023 - categorical_accuracy: 0.8844 - val_loss: 0.4396 - val_categorical_accuracy: 0.8700\n",
      "Epoch 44/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3983 - categorical_accuracy: 0.8722 - val_loss: 0.6895 - val_categorical_accuracy: 0.7600\n",
      "Epoch 45/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.4343 - categorical_accuracy: 0.8678 - val_loss: 0.4985 - val_categorical_accuracy: 0.8100\n",
      "Epoch 46/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.3535 - categorical_accuracy: 0.8922 - val_loss: 0.7079 - val_categorical_accuracy: 0.7500\n",
      "Epoch 47/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.4056 - categorical_accuracy: 0.8678 - val_loss: 0.4280 - val_categorical_accuracy: 0.8600\n",
      "Epoch 48/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.3670 - categorical_accuracy: 0.8867 - val_loss: 0.6978 - val_categorical_accuracy: 0.7400\n",
      "Epoch 49/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.4121 - categorical_accuracy: 0.8656 - val_loss: 0.7472 - val_categorical_accuracy: 0.7600\n",
      "Epoch 50/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.3920 - categorical_accuracy: 0.8778 - val_loss: 0.4649 - val_categorical_accuracy: 0.8300\n",
      "Epoch 51/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.3776 - categorical_accuracy: 0.8800 - val_loss: 0.5739 - val_categorical_accuracy: 0.8000\n",
      "Epoch 52/100\n",
      "113/113 [==============================] - 51s 456ms/step - loss: 0.3885 - categorical_accuracy: 0.8822 - val_loss: 0.6278 - val_categorical_accuracy: 0.7800\n",
      "Epoch 53/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.3540 - categorical_accuracy: 0.8956 - val_loss: 0.5146 - val_categorical_accuracy: 0.8400\n",
      "Epoch 54/100\n",
      "113/113 [==============================] - 52s 463ms/step - loss: 0.3355 - categorical_accuracy: 0.9022 - val_loss: 0.4200 - val_categorical_accuracy: 0.8600\n",
      "Epoch 55/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.3524 - categorical_accuracy: 0.8911 - val_loss: 0.4268 - val_categorical_accuracy: 0.8500\n",
      "Epoch 56/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.3640 - categorical_accuracy: 0.8911 - val_loss: 0.4965 - val_categorical_accuracy: 0.8400\n",
      "Epoch 57/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.3157 - categorical_accuracy: 0.9111 - val_loss: 0.4252 - val_categorical_accuracy: 0.8500\n",
      "Epoch 58/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.3352 - categorical_accuracy: 0.9033 - val_loss: 0.7226 - val_categorical_accuracy: 0.7800\n",
      "Epoch 59/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.3047 - categorical_accuracy: 0.9144 - val_loss: 0.4384 - val_categorical_accuracy: 0.8700\n",
      "Epoch 60/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.3595 - categorical_accuracy: 0.8933 - val_loss: 0.4577 - val_categorical_accuracy: 0.8600\n",
      "Epoch 61/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.2885 - categorical_accuracy: 0.9200INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 508ms/step - loss: 0.2885 - categorical_accuracy: 0.9200 - val_loss: 0.4066 - val_categorical_accuracy: 0.8900\n",
      "Epoch 62/100\n",
      "113/113 [==============================] - 52s 460ms/step - loss: 0.3012 - categorical_accuracy: 0.9133 - val_loss: 0.4599 - val_categorical_accuracy: 0.8600\n",
      "Epoch 63/100\n",
      "113/113 [==============================] - 51s 457ms/step - loss: 0.2946 - categorical_accuracy: 0.9133 - val_loss: 0.5087 - val_categorical_accuracy: 0.8200\n",
      "Epoch 64/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.2807 - categorical_accuracy: 0.9178 - val_loss: 0.4382 - val_categorical_accuracy: 0.8700\n",
      "Epoch 65/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.2937 - categorical_accuracy: 0.9189 - val_loss: 0.4842 - val_categorical_accuracy: 0.8600\n",
      "Epoch 66/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.3033 - categorical_accuracy: 0.9200 - val_loss: 0.4485 - val_categorical_accuracy: 0.8700\n",
      "Epoch 67/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.3023 - categorical_accuracy: 0.9122 - val_loss: 0.4437 - val_categorical_accuracy: 0.8600\n",
      "Epoch 68/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.2777 - categorical_accuracy: 0.9244 - val_loss: 0.4381 - val_categorical_accuracy: 0.8700\n",
      "Epoch 69/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.2983 - categorical_accuracy: 0.9200 - val_loss: 0.3792 - val_categorical_accuracy: 0.8900\n",
      "Epoch 70/100\n",
      "113/113 [==============================] - 51s 456ms/step - loss: 0.3051 - categorical_accuracy: 0.9122 - val_loss: 0.4292 - val_categorical_accuracy: 0.8800\n",
      "Epoch 71/100\n",
      "113/113 [==============================] - 52s 457ms/step - loss: 0.3089 - categorical_accuracy: 0.9144 - val_loss: 0.4018 - val_categorical_accuracy: 0.8800\n",
      "Epoch 72/100\n",
      "113/113 [==============================] - 52s 457ms/step - loss: 0.2753 - categorical_accuracy: 0.9244 - val_loss: 0.4442 - val_categorical_accuracy: 0.8700\n",
      "Epoch 73/100\n",
      "113/113 [==============================] - 52s 459ms/step - loss: 0.3125 - categorical_accuracy: 0.9044 - val_loss: 0.5365 - val_categorical_accuracy: 0.8400\n",
      "Epoch 74/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.2727 - categorical_accuracy: 0.9278 - val_loss: 0.4091 - val_categorical_accuracy: 0.8700\n",
      "Epoch 75/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.4557 - categorical_accuracy: 0.8600 - val_loss: 0.4815 - val_categorical_accuracy: 0.8500\n",
      "Epoch 76/100\n",
      "113/113 [==============================] - 51s 456ms/step - loss: 0.4240 - categorical_accuracy: 0.8578 - val_loss: 0.4209 - val_categorical_accuracy: 0.8900\n",
      "Epoch 77/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.4365 - categorical_accuracy: 0.8600 - val_loss: 0.6409 - val_categorical_accuracy: 0.8000\n",
      "Epoch 78/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.4681 - categorical_accuracy: 0.8456 - val_loss: 0.7268 - val_categorical_accuracy: 0.7500\n",
      "Epoch 79/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.4132 - categorical_accuracy: 0.8644 - val_loss: 0.4595 - val_categorical_accuracy: 0.8700\n",
      "Epoch 80/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.4216 - categorical_accuracy: 0.8600 - val_loss: 0.9288 - val_categorical_accuracy: 0.6800\n",
      "Epoch 81/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.4877 - categorical_accuracy: 0.8344 - val_loss: 0.6403 - val_categorical_accuracy: 0.7500\n",
      "Epoch 82/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.4549 - categorical_accuracy: 0.8511 - val_loss: 0.4718 - val_categorical_accuracy: 0.8200\n",
      "Epoch 83/100\n",
      "113/113 [==============================] - 52s 459ms/step - loss: 0.4132 - categorical_accuracy: 0.8622 - val_loss: 0.5724 - val_categorical_accuracy: 0.8000\n",
      "Epoch 84/100\n",
      "113/113 [==============================] - 52s 461ms/step - loss: 0.4099 - categorical_accuracy: 0.8733 - val_loss: 0.3596 - val_categorical_accuracy: 0.8900\n",
      "Epoch 85/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.3584 - categorical_accuracy: 0.8867 - val_loss: 0.5718 - val_categorical_accuracy: 0.8200\n",
      "Epoch 86/100\n",
      "113/113 [==============================] - 52s 457ms/step - loss: 0.3383 - categorical_accuracy: 0.8967 - val_loss: 0.5884 - val_categorical_accuracy: 0.8100\n",
      "Epoch 87/100\n",
      "113/113 [==============================] - 52s 459ms/step - loss: 0.3716 - categorical_accuracy: 0.8844 - val_loss: 0.4360 - val_categorical_accuracy: 0.8800\n",
      "Epoch 88/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.3714 - categorical_accuracy: 0.8889 - val_loss: 0.4109 - val_categorical_accuracy: 0.8600\n",
      "Epoch 89/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.3686 - categorical_accuracy: 0.8833 - val_loss: 0.4547 - val_categorical_accuracy: 0.8600\n",
      "Epoch 90/100\n",
      "113/113 [==============================] - 52s 462ms/step - loss: 0.3794 - categorical_accuracy: 0.8800 - val_loss: 0.4483 - val_categorical_accuracy: 0.8600\n",
      "Epoch 91/100\n",
      "113/113 [==============================] - 52s 460ms/step - loss: 0.3467 - categorical_accuracy: 0.8967 - val_loss: 0.4917 - val_categorical_accuracy: 0.8300\n",
      "Epoch 92/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4276 - categorical_accuracy: 0.8656INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 514ms/step - loss: 0.4276 - categorical_accuracy: 0.8656 - val_loss: 0.3346 - val_categorical_accuracy: 0.9000\n",
      "Epoch 93/100\n",
      "113/113 [==============================] - 52s 459ms/step - loss: 0.3803 - categorical_accuracy: 0.8844 - val_loss: 0.5051 - val_categorical_accuracy: 0.8300\n",
      "Epoch 94/100\n",
      "113/113 [==============================] - 51s 456ms/step - loss: 0.3907 - categorical_accuracy: 0.8700 - val_loss: 0.4872 - val_categorical_accuracy: 0.8200\n",
      "Epoch 95/100\n",
      "113/113 [==============================] - 52s 460ms/step - loss: 0.3270 - categorical_accuracy: 0.9056 - val_loss: 0.4560 - val_categorical_accuracy: 0.8700\n",
      "Epoch 96/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.3558 - categorical_accuracy: 0.8944 - val_loss: 0.3986 - val_categorical_accuracy: 0.8800\n",
      "Epoch 97/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.3054 - categorical_accuracy: 0.9033 - val_loss: 0.4988 - val_categorical_accuracy: 0.8500\n",
      "Epoch 98/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.3453 - categorical_accuracy: 0.8889 - val_loss: 0.5048 - val_categorical_accuracy: 0.8400\n",
      "Epoch 99/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.3556 - categorical_accuracy: 0.8967 - val_loss: 0.3912 - val_categorical_accuracy: 0.8700\n",
      "Epoch 100/100\n",
      "113/113 [==============================] - 52s 457ms/step - loss: 0.3030 - categorical_accuracy: 0.9111 - val_loss: 0.7505 - val_categorical_accuracy: 0.7400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁▁▁▂▃▃▃▃▃▄▅▆▇▆▇▇▇▇█▇▇█████████▇▇▇▇▇▇▇▇██</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>█████▇▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁█████▇▇▇▇▆</td></tr><tr><td>epoch/loss</td><td>██▇▇▆▆▅▅▅▅▅▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▁▁</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁▁▁▂▃▃▃▃▃▄▆▆▆▆▆▆▅▆▆▆▇▇▇███▇███▇█▇██▇█▇▇▆</td></tr><tr><td>epoch/val_loss</td><td>██▇▇▆▅▅▅▅▅▄▃▃▄▃▃▄▄▄▅▃▂▂▂▂▂▂▁▂▂▄▂▂▁▂▂▁▂▂▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.91111</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>8e-05</td></tr><tr><td>epoch/loss</td><td>0.30305</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.74</td></tr><tr><td>epoch/val_loss</td><td>0.75053</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM64-Dense64-allfeatures</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/tyh5w2ae' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/tyh5w2ae</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240408_124402-tyh5w2ae/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fda527cdad0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(64, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, 1629),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"LSTM64-Dense64-allfeatures.tf\"),\n",
    "            run_name=\"LSTM64-Dense64-allfeatures\", T_max=75, epochs=100, \n",
    "            max_lr = 1e-4, min_lr = 2.5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240408_141317-jhepc329</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/jhepc329' target=\"_blank\">LSTM64-Dense128-allfeatures</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/jhepc329' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/jhepc329</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.1320 - categorical_accuracy: 0.3978INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 67s 557ms/step - loss: 1.1320 - categorical_accuracy: 0.3978 - val_loss: 1.1157 - val_categorical_accuracy: 0.4100\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.1137 - categorical_accuracy: 0.4156INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 517ms/step - loss: 1.1137 - categorical_accuracy: 0.4156 - val_loss: 1.0914 - val_categorical_accuracy: 0.4600\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 1.0936 - categorical_accuracy: 0.4167 - val_loss: 1.0644 - val_categorical_accuracy: 0.4300\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0683 - categorical_accuracy: 0.3889INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 56s 501ms/step - loss: 1.0683 - categorical_accuracy: 0.3889 - val_loss: 1.0305 - val_categorical_accuracy: 0.4700\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - 52s 457ms/step - loss: 1.0345 - categorical_accuracy: 0.4256 - val_loss: 0.9855 - val_categorical_accuracy: 0.4300\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0035 - categorical_accuracy: 0.4456INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 59s 520ms/step - loss: 1.0035 - categorical_accuracy: 0.4456 - val_loss: 0.9437 - val_categorical_accuracy: 0.4900\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0103 - categorical_accuracy: 0.4544INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 513ms/step - loss: 1.0103 - categorical_accuracy: 0.4544 - val_loss: 0.9660 - val_categorical_accuracy: 0.5200\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.9675 - categorical_accuracy: 0.4767 - val_loss: 1.0216 - val_categorical_accuracy: 0.4900\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.9119 - categorical_accuracy: 0.5267INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 59s 517ms/step - loss: 0.9119 - categorical_accuracy: 0.5267 - val_loss: 0.8566 - val_categorical_accuracy: 0.5600\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.9029 - categorical_accuracy: 0.5400INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 59s 518ms/step - loss: 0.9029 - categorical_accuracy: 0.5400 - val_loss: 0.8066 - val_categorical_accuracy: 0.5800\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8586 - categorical_accuracy: 0.5511INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 511ms/step - loss: 0.8586 - categorical_accuracy: 0.5511 - val_loss: 0.8088 - val_categorical_accuracy: 0.5900\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - 52s 457ms/step - loss: 0.8127 - categorical_accuracy: 0.5533 - val_loss: 0.7929 - val_categorical_accuracy: 0.5800\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.8108 - categorical_accuracy: 0.5578 - val_loss: 0.7927 - val_categorical_accuracy: 0.5700\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.8790 - categorical_accuracy: 0.5344 - val_loss: 0.8307 - val_categorical_accuracy: 0.5500\n",
      "Epoch 15/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.7909 - categorical_accuracy: 0.5622 - val_loss: 0.8010 - val_categorical_accuracy: 0.5500\n",
      "Epoch 16/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.8140 - categorical_accuracy: 0.5711 - val_loss: 0.8049 - val_categorical_accuracy: 0.5600\n",
      "Epoch 17/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8103 - categorical_accuracy: 0.5778INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 508ms/step - loss: 0.8103 - categorical_accuracy: 0.5778 - val_loss: 0.8149 - val_categorical_accuracy: 0.6000\n",
      "Epoch 18/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.8039 - categorical_accuracy: 0.5756 - val_loss: 0.7945 - val_categorical_accuracy: 0.5800\n",
      "Epoch 19/100\n",
      "113/113 [==============================] - 52s 457ms/step - loss: 0.7993 - categorical_accuracy: 0.5856 - val_loss: 0.7896 - val_categorical_accuracy: 0.5500\n",
      "Epoch 20/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.7795 - categorical_accuracy: 0.5911 - val_loss: 0.8051 - val_categorical_accuracy: 0.5300\n",
      "Epoch 21/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.7706 - categorical_accuracy: 0.6022 - val_loss: 0.7732 - val_categorical_accuracy: 0.5700\n",
      "Epoch 22/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.7396 - categorical_accuracy: 0.6156 - val_loss: 0.8178 - val_categorical_accuracy: 0.5900\n",
      "Epoch 23/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.7452 - categorical_accuracy: 0.6011 - val_loss: 0.7757 - val_categorical_accuracy: 0.5900\n",
      "Epoch 24/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.7296 - categorical_accuracy: 0.6189 - val_loss: 0.7378 - val_categorical_accuracy: 0.6000\n",
      "Epoch 25/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.7152 - categorical_accuracy: 0.6333 - val_loss: 0.7172 - val_categorical_accuracy: 0.5900\n",
      "Epoch 26/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6897 - categorical_accuracy: 0.6811INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 515ms/step - loss: 0.6897 - categorical_accuracy: 0.6811 - val_loss: 0.6801 - val_categorical_accuracy: 0.6400\n",
      "Epoch 27/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6367 - categorical_accuracy: 0.7233INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 512ms/step - loss: 0.6367 - categorical_accuracy: 0.7233 - val_loss: 0.6453 - val_categorical_accuracy: 0.7200\n",
      "Epoch 28/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5524 - categorical_accuracy: 0.7744INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 515ms/step - loss: 0.5524 - categorical_accuracy: 0.7744 - val_loss: 0.5173 - val_categorical_accuracy: 0.8400\n",
      "Epoch 29/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.5915 - categorical_accuracy: 0.7633 - val_loss: 0.6240 - val_categorical_accuracy: 0.7300\n",
      "Epoch 30/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.5347 - categorical_accuracy: 0.7911 - val_loss: 0.4735 - val_categorical_accuracy: 0.8400\n",
      "Epoch 31/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.5165 - categorical_accuracy: 0.7944 - val_loss: 0.4232 - val_categorical_accuracy: 0.8100\n",
      "Epoch 32/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.4915 - categorical_accuracy: 0.8178 - val_loss: 0.4979 - val_categorical_accuracy: 0.8100\n",
      "Epoch 33/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4810 - categorical_accuracy: 0.8333INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 518ms/step - loss: 0.4810 - categorical_accuracy: 0.8333 - val_loss: 0.4634 - val_categorical_accuracy: 0.8500\n",
      "Epoch 34/100\n",
      "105/113 [==========================>...] - ETA: 3s - loss: 0.4591 - categorical_accuracy: 0.8421"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(64, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, 1629),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"LSTM64-Dense128allfeatures.tf\"),\n",
    "            run_name=\"LSTM64-Dense128-allfeatures\", T_max=75, epochs=100, \n",
    "            max_lr = 1e-4, min_lr = 2.5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = ops.shape(x)[-1]\n",
    "        positions = ops.arange(start=0, stop=maxlen, step=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "class PositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = ops.shape(x)[-1]\n",
    "        positions = ops.arange(start=0, stop=maxlen, step=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        return x + positions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 64  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "\n",
    "inputs = layers.Input(shape=(CONFIG.VIDEO_LENGTH,))\n",
    "embedding_layer = TokenAndPositionEmbedding(CONFIG.VIDEO_LENGTH, 1629, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(len(train_dataset_parquet.unique_labels), activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 5087314,
     "sourceId": 46105,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
