{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":46105,"databundleVersionId":5087314,"sourceType":"competition"}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install tensorflow opencv-python mediapipe scikit-learn matplotlib pandas pyarrow","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-06T11:41:56.860405Z","iopub.execute_input":"2024-04-06T11:41:56.860988Z","iopub.status.idle":"2024-04-06T11:42:19.705553Z","shell.execute_reply.started":"2024-04-06T11:41:56.860951Z","shell.execute_reply":"2024-04-06T11:42:19.704055Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.15.0)\nRequirement already satisfied: opencv-python in /opt/conda/lib/python3.10/site-packages (4.9.0.80)\nCollecting mediapipe\n  Downloading mediapipe-0.10.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.7.5)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.1)\nRequirement already satisfied: pyarrow in /opt/conda/lib/python3.10/site-packages (15.0.2)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.10.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (16.0.6)\nRequirement already satisfied: ml-dtypes~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.26.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (69.0.3)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.9.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.35.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.60.0)\nRequirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.15.1)\nRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.15.0)\nCollecting keras<2.16,>=2.15.0 (from tensorflow)\n  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: attrs>=19.1.0 in /opt/conda/lib/python3.10/site-packages (from mediapipe) (23.2.0)\nRequirement already satisfied: jax in /opt/conda/lib/python3.10/site-packages (from mediapipe) (0.4.25)\nRequirement already satisfied: jaxlib in /opt/conda/lib/python3.10/site-packages (from mediapipe) (0.4.25)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from mediapipe) (2.1.2+cpu)\nRequirement already satisfied: opencv-contrib-python in /opt/conda/lib/python3.10/site-packages (from mediapipe) (4.9.0.80)\nCollecting sounddevice>=0.4.4 (from mediapipe)\n  Downloading sounddevice-0.4.6-py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.4)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\nRequirement already satisfied: CFFI>=1.0 in /opt/conda/lib/python3.10/site-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->mediapipe) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->mediapipe) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->mediapipe) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->mediapipe) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->mediapipe) (2024.3.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->mediapipe) (1.3.0)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\nDownloading mediapipe-0.10.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading sounddevice-0.4.6-py3-none-any.whl (31 kB)\nInstalling collected packages: keras, sounddevice, mediapipe\n  Attempting uninstall: keras\n    Found existing installation: keras 3.0.5\n    Uninstalling keras-3.0.5:\n      Successfully uninstalled keras-3.0.5\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed keras-2.15.0 mediapipe-0.10.11 sounddevice-0.4.6\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport math\nimport pandas as pd\nimport numpy as np\n#Load parquet data into dataset_parquet for training.\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom glob import glob\nfrom tqdm import tqdm\nimport random\nimport time","metadata":{"execution":{"iopub.status.busy":"2024-04-06T14:24:56.817202Z","iopub.execute_input":"2024-04-06T14:24:56.817640Z","iopub.status.idle":"2024-04-06T14:24:56.823910Z","shell.execute_reply.started":"2024-04-06T14:24:56.817605Z","shell.execute_reply":"2024-04-06T14:24:56.822333Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"class CONFIG:\n    root = os.path.join(\"/\", \"kaggle\", \"input\", \"asl-signs\") \n    DATA_LIMIT = 1000\n    BATCH_SIZE = 8\n    VIDEO_LENGTH = 60\n    TRAIN_VAL_SPLIT = 0.8","metadata":{"execution":{"iopub.status.busy":"2024-04-06T14:45:00.019725Z","iopub.execute_input":"2024-04-06T14:45:00.021123Z","iopub.status.idle":"2024-04-06T14:45:00.028394Z","shell.execute_reply.started":"2024-04-06T14:45:00.021070Z","shell.execute_reply":"2024-04-06T14:45:00.026975Z"},"trusted":true},"execution_count":167,"outputs":[]},{"cell_type":"code","source":"os.listdir(CONFIG.root)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T11:42:19.730457Z","iopub.execute_input":"2024-04-06T11:42:19.730952Z","iopub.status.idle":"2024-04-06T11:42:19.747021Z","shell.execute_reply.started":"2024-04-06T11:42:19.730909Z","shell.execute_reply":"2024-04-06T11:42:19.745893Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"['sign_to_prediction_index_map.json', 'train.csv', 'train_landmark_files']"},"metadata":{}}]},{"cell_type":"code","source":"#simple code to read parquet files\n# Path to your Parquet file\nparquet_file_path = os.path.join(os.path.join(CONFIG.root,'train_landmark_files/2044/3127189.parquet'))\n\n# Read the Parquet file\ndf = pd.read_parquet(parquet_file_path)\n\n# Display the first few rows of the dataframe to view the landmarks\nprint(df.head())","metadata":{"execution":{"iopub.status.busy":"2024-04-06T11:42:35.632215Z","iopub.execute_input":"2024-04-06T11:42:35.632617Z","iopub.status.idle":"2024-04-06T11:42:35.853331Z","shell.execute_reply.started":"2024-04-06T11:42:35.632586Z","shell.execute_reply":"2024-04-06T11:42:35.852141Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"   frame     row_id  type  landmark_index         x         y         z\n0     21  21-face-0  face               0  0.479847  0.478954 -0.044969\n1     21  21-face-1  face               1  0.469082  0.439374 -0.069958\n2     21  21-face-2  face               2  0.472504  0.453134 -0.040229\n3     21  21-face-3  face               3  0.458673  0.407925 -0.045351\n4     21  21-face-4  face               4  0.468096  0.428457 -0.072840\n","output_type":"stream"}]},{"cell_type":"code","source":"#this code sorts out a parquet files and rearrange the order to pose,face, left-hand, right-hand\nimport os\nimport pandas as pd\nimport numpy as np\n\nids = None\n\norder_global = {\"pose\" : 10000, \"face\" : 1000, \"left_hand\" : 100, \"right_hand\" : 10}\n\ndef process_parquet(ds):\n    ret = []\n    frames_unique = sorted(np.unique(ds[\"frame\"]))\n    for i,frame in enumerate(frames_unique):\n        frame_ds = ds[ds['frame'] == frame]\n        \n        order = []\n        for el in frame_ds[\"row_id\"]:\n            _frame, part, keypoint = el.split(\"-\")\n            order.append(order_global[part] - int(keypoint))\n\n        order = np.array(order)\n        frame_ds.iloc[:, 1] = order\n        frame_ds = frame_ds.sort_values(by=\"row_id\", ascending=False)\n    \n        vals = np.array(frame_ds[[\"x\", \"y\", \"z\"]]).flatten()\n\n        ret.append(vals)\n    return np.array(ret)\n        \n#process_parquet(\"79631423.parquet\")","metadata":{"execution":{"iopub.status.busy":"2024-04-06T11:42:38.200337Z","iopub.execute_input":"2024-04-06T11:42:38.203081Z","iopub.status.idle":"2024-04-06T11:42:38.214876Z","shell.execute_reply.started":"2024-04-06T11:42:38.203027Z","shell.execute_reply":"2024-04-06T11:42:38.213639Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n#custom class to load data from Parquet files for training ML models.\nclass ParquetDataset(keras.utils.Sequence):\n    def __init__(self, dataset_folder, csv_file : str, batch_size=CONFIG.BATCH_SIZE, \n                 data_limit :int= CONFIG.DATA_LIMIT, check_if_file_exists = True, \n                 preprocessing_func=None, frame_length :int = CONFIG.VIDEO_LENGTH,\n                 split : str = \"train\", train_val_split : float = CONFIG.TRAIN_VAL_SPLIT,\n                 sort_by_counts : bool = True\n                ):\n        #taking keras sequence for .fit(), .evaluate(), .predict() methods\n        #load csv - it has the path to parquet file, and another to store label\n        self.csv_path = csv_file\n        self.root_folder = dataset_folder\n        self.batch_size = batch_size\n        #optional pre-processing function to the parquet files.\n        self.preprocessing_func = preprocessing_func\n        \n        self.csv_data = pd.read_csv(self.csv_path)\n        \n        self.all_files = []\n        self.not_exists = []\n        self.frame_length = frame_length\n\n        \n        for path, label in tqdm(list(zip(self.csv_data[\"path\"], self.csv_data[\"sign\"]))):\n            prop_path = os.path.join(self.root_folder, path)\n            \n            if check_if_file_exists:\n                if os.path.exists(prop_path):\n                    self.all_files.append((prop_path, label))\n                else:\n                    self.not_exists.append(prop_path)\n            else:\n                self.all_files.append((prop_path, label))\n                \n                    \n        self.all_files = np.array(self.all_files)\n        self.unique_labels = np.unique(self.all_files[:, 1])\n        self.label_2_id = { key : i for i, key in enumerate(self.unique_labels)}\n    \n        # sort the values by popularity\n        if sort_by_counts:\n            cnt = Counter(self.all_files[:, 1])\n            vals = []\n            \n            for i,row in enumerate(self.all_files):\n                vals.append((int(1e6 * cnt[row[1]] + self.label_2_id [row[1]]),i))\n            \n            vals = np.array(sorted(vals)[::-1])\n            self.all_files = self.all_files[vals[:,1]]\n\n        \n        if data_limit < 0:\n            train_ds, val_ds = train_test_split(self.all_files, train_size=train_val_split, random_state=42)\n        else:\n            train_ds, val_ds = train_test_split(self.all_files[:data_limit], train_size=train_val_split, random_state=42)\n            self.unique_labels = np.unique(self.all_files[:data_limit, 1])\n            self.label_2_id = { key : i for i, key in enumerate(self.unique_labels)}\n            \n        if split.lower() == \"train\":\n            self.dataset = train_ds\n            \n        elif split.lower() == \"val\":\n            self.dataset = val_ds \n            \n        else:\n            raise Exception(\"please specify split to be either train or val\")\n            \n        np.random.shuffle(self.dataset)\n                   \n\n    def __len__(self):\n        # Assuming each Parquet file should be one batch; adjust if necessary\n        return math.ceil(len(self.dataset) / self.batch_size)\n    \n    def get_single(self, idx):\n        # Load one file per batch\n        #take the idx value, 1st label, \n        path, label = self.dataset[idx]\n        \n        df = pd.read_parquet( path)\n        \n        # Apply preprocessing if specified\n        if self.preprocessing_func:\n            df = self.preprocessing_func(df, self.frame_length)\n        \n        one_hot_encoded_label = np.zeros(len(self.unique_labels))\n        one_hot_encoded_label[self.label_2_id[label]] = 1  \n        \n        return df, one_hot_encoded_label\n\n    def __getitem__(self, idx):\n        X, Y = [], []\n        \n        low = idx * self.batch_size\n        high = min(low + self.batch_size, len(self.dataset))\n        \n        for i in range(low, high):\n            x, y = self.get_single(i)\n            X.append(x)\n            Y.append(y)\n        \n        return np.array(X), np.array(Y)\n                \n        \n    def on_epoch_end(self):\n        # Shuffle files for the next epoch\n        np.random.shuffle(self.dataset)\n\ndef my_preprocessing_func(df, frame_length):\n    \n    # Define your preprocessing steps here\n    # Example: normalize numerical features\n    frames_mediapipe = process_parquet(df)\n    \n    current_length, num_features = frames_mediapipe.shape\n\n    if current_length >= frame_length:\n            # TODO: a better than uniform value ? Could place gaussian in the middle\n            random_start = random.randint(0, current_length - frame_length)\n            return np.nan_to_num(frames_mediapipe[random_start : (random_start + frame_length)])\n        \n    # padd the video to contain zeros \n    return np.concatenate([np.nan_to_num(frames_mediapipe), np.zeros((frame_length - current_length, num_features))], axis=0)\n    \n# Usage example\nparquet_folder_path = CONFIG.root\ntrain_dataset_parquet = ParquetDataset(parquet_folder_path, csv_file = os.path.join(CONFIG.root, \"train.csv\"), \n                                 batch_size=CONFIG.BATCH_SIZE, data_limit=CONFIG.DATA_LIMIT,\n                                 preprocessing_func=my_preprocessing_func,\n                                check_if_file_exists = False,\n                                split=\"train\")\n\nval_dataset_parquet = ParquetDataset(parquet_folder_path, csv_file = os.path.join(CONFIG.root, \"train.csv\"), \n                                 batch_size=CONFIG.BATCH_SIZE, data_limit=CONFIG.DATA_LIMIT,\n                                 preprocessing_func=my_preprocessing_func,\n                                 check_if_file_exists= False,\n                                 split=\"val\")\n\nprint(f\"cardinality of train : {len(train_dataset_parquet)}, cardinality of validation : {len(val_dataset_parquet)}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-06T14:45:02.912195Z","iopub.execute_input":"2024-04-06T14:45:02.912597Z","iopub.status.idle":"2024-04-06T14:45:05.493425Z","shell.execute_reply.started":"2024-04-06T14:45:02.912562Z","shell.execute_reply":"2024-04-06T14:45:05.492164Z"},"trusted":true},"execution_count":168,"outputs":[{"name":"stderr","text":"100%|██████████| 94477/94477 [00:00<00:00, 307004.59it/s]\n100%|██████████| 94477/94477 [00:00<00:00, 307440.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"cardinality of train : 100, cardinality of validation : 25\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt \n\n\nplt.figure(figsize=(10, 3))\nplt.subplot(1, 2, 1)\nplt.hist(train_dataset_parquet.dataset[:, 1], bins=len(dataset_parquet.unique_labels))\n\nplt.subplot(1, 2, 2)\nplt.hist(val_dataset_parquet.dataset[:, 1], bins=len(dataset_parquet.unique_labels))\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T14:45:05.495501Z","iopub.execute_input":"2024-04-06T14:45:05.495957Z","iopub.status.idle":"2024-04-06T14:45:06.070055Z","shell.execute_reply.started":"2024-04-06T14:45:05.495918Z","shell.execute_reply":"2024-04-06T14:45:06.068863Z"},"trusted":true},"execution_count":169,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x300 with 2 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"start = time.time()\nisnans =False\n\nf = True\nlabels_batches = []\nfor el in tqdm(train_dataset_parquet):\n    if f:\n        print(el[0].shape, el[1].shape)\n        f = False\n    labels_batches.append(el[1])\n        \n    isnans |= np.any(np.isnan(el[0]))\n    if isnans:\n        print(\"FOUND NAN!\")\n        break\n\n\nprint(f\"Iterating through dataset took : {round( time.time() - start , 4)}s\")","metadata":{"execution":{"iopub.status.busy":"2024-04-06T14:45:07.748199Z","iopub.execute_input":"2024-04-06T14:45:07.748709Z","iopub.status.idle":"2024-04-06T14:46:03.489544Z","shell.execute_reply.started":"2024-04-06T14:45:07.748654Z","shell.execute_reply":"2024-04-06T14:46:03.487308Z"},"trusted":true},"execution_count":170,"outputs":[{"name":"stderr","text":"  1%|          | 1/100 [00:00<01:05,  1.52it/s]","output_type":"stream"},{"name":"stdout","text":"(8, 60, 1629) (8, 3)\n","output_type":"stream"},{"name":"stderr","text":"  6%|▌         | 6/100 [00:05<01:27,  1.07it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[170], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      5\u001b[0m labels_batches \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dataset_parquet):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f:\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28mprint\u001b[39m(el[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, el[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/data_utils.py:567\u001b[0m, in \u001b[0;36mSequence.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    566\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a generator that iterate over the Sequence.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mself\u001b[39m[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))):\n\u001b[1;32m    568\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m item\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/data_utils.py:567\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    566\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a generator that iterate over the Sequence.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))):\n\u001b[1;32m    568\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m item\n","Cell \u001b[0;32mIn[168], line 100\u001b[0m, in \u001b[0;36mParquetDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     97\u001b[0m high \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(low \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset))\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(low, high):\n\u001b[0;32m--> 100\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m     X\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m    102\u001b[0m     Y\u001b[38;5;241m.\u001b[39mappend(y)\n","Cell \u001b[0;32mIn[168], line 86\u001b[0m, in \u001b[0;36mParquetDataset.get_single\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Apply preprocessing if specified\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessing_func:\n\u001b[0;32m---> 86\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocessing_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m one_hot_encoded_label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munique_labels))\n\u001b[1;32m     89\u001b[0m one_hot_encoded_label[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_2_id[label]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \n","Cell \u001b[0;32mIn[168], line 115\u001b[0m, in \u001b[0;36mmy_preprocessing_func\u001b[0;34m(df, frame_length)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmy_preprocessing_func\u001b[39m(df, frame_length):\n\u001b[1;32m    112\u001b[0m     \n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# Define your preprocessing steps here\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# Example: normalize numerical features\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     frames_mediapipe \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     current_length, num_features \u001b[38;5;241m=\u001b[39m frames_mediapipe\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m current_length \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m frame_length:\n\u001b[1;32m    120\u001b[0m             \u001b[38;5;66;03m# TODO: a better than uniform value ? Could place gaussian in the middle\u001b[39;00m\n","Cell \u001b[0;32mIn[8], line 23\u001b[0m, in \u001b[0;36mprocess_parquet\u001b[0;34m(ds)\u001b[0m\n\u001b[1;32m     21\u001b[0m order \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(order)\n\u001b[1;32m     22\u001b[0m frame_ds\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m order\n\u001b[0;32m---> 23\u001b[0m frame_ds \u001b[38;5;241m=\u001b[39m \u001b[43mframe_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mby\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrow_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mascending\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m vals \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(frame_ds[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     27\u001b[0m ret\u001b[38;5;241m.\u001b[39mappend(vals)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:7206\u001b[0m, in \u001b[0;36mDataFrame.sort_values\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   7203\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   7204\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 7206\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   7207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   7208\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   7210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_index:\n\u001b[1;32m   7211\u001b[0m     new_data\u001b[38;5;241m.\u001b[39mset_axis(\n\u001b[1;32m   7212\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_block_manager_axis(axis), default_index(\u001b[38;5;28mlen\u001b[39m(indexer))\n\u001b[1;32m   7213\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/internals/managers.py:891\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[0;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;66;03m# Caller is responsible for ensuring indexer annotation is accurate\u001b[39;00m\n\u001b[1;32m    890\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[axis]\n\u001b[0;32m--> 891\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_convert_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    893\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreindex_indexer(\n\u001b[1;32m    895\u001b[0m     new_axis\u001b[38;5;241m=\u001b[39mnew_labels,\n\u001b[1;32m    896\u001b[0m     indexer\u001b[38;5;241m=\u001b[39mindexer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    899\u001b[0m     copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    900\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexers/utils.py:275\u001b[0m, in \u001b[0;36mmaybe_convert_indices\u001b[0;34m(indices, n, verify)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp)\n\u001b[1;32m    274\u001b[0m mask \u001b[38;5;241m=\u001b[39m indices \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    276\u001b[0m     indices \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    277\u001b[0m     indices[mask] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m n\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:58\u001b[0m, in \u001b[0;36m_any\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_any\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Parsing keyword arguments is currently fairly slow, so avoid it for now\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m where \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_any\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_any(a, axis, dtype, out, keepdims, where\u001b[38;5;241m=\u001b[39mwhere)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"# Train simple LSTM","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.callbacks import TensorBoard\nfrom tensorflow.keras import backend as K\nimport tensorflow as tf\n\nmodel = Sequential()\n#x,y,z -> y,z as the input shape\nmodel.add(LSTM(128, return_sequences=False, activation='relu', input_shape=(CONFIG.VIDEO_LENGTH, 1629)))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n\n#Adam Optimizer - fixed learning rate.\nadam_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n\n\nmodel.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n\nprint(model.summary())\n\nmodel.fit(train_dataset_parquet, epochs=100, validation_data = val_dataset_parquet, batch_size = 8, callbacks=[])","metadata":{"execution":{"iopub.status.busy":"2024-04-06T14:59:33.363116Z","iopub.execute_input":"2024-04-06T14:59:33.363569Z","iopub.status.idle":"2024-04-06T18:45:22.190682Z","shell.execute_reply.started":"2024-04-06T14:59:33.363530Z","shell.execute_reply":"2024-04-06T18:45:22.189372Z"},"trusted":true},"execution_count":174,"outputs":[{"name":"stdout","text":"Model: \"sequential_11\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_25 (LSTM)              (None, 128)               900096    \n                                                                 \n dense_32 (Dense)            (None, 128)               16512     \n                                                                 \n dense_33 (Dense)            (None, 256)               33024     \n                                                                 \n dense_34 (Dense)            (None, 3)                 771       \n                                                                 \n=================================================================\nTotal params: 950403 (3.63 MB)\nTrainable params: 950403 (3.63 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\nEpoch 1/100\n100/100 [==============================] - 113s 1s/step - loss: 1.0960 - categorical_accuracy: 0.4112 - val_loss: 1.0782 - val_categorical_accuracy: 0.4150\nEpoch 2/100\n100/100 [==============================] - 110s 1s/step - loss: 1.0678 - categorical_accuracy: 0.4137 - val_loss: 1.0555 - val_categorical_accuracy: 0.4100\nEpoch 3/100\n100/100 [==============================] - 110s 1s/step - loss: 1.0345 - categorical_accuracy: 0.4025 - val_loss: 1.4280 - val_categorical_accuracy: 0.4050\nEpoch 4/100\n100/100 [==============================] - 111s 1s/step - loss: 1299.4078 - categorical_accuracy: 0.4087 - val_loss: 1.0479 - val_categorical_accuracy: 0.4050\nEpoch 5/100\n100/100 [==============================] - 112s 1s/step - loss: 1.0275 - categorical_accuracy: 0.4137 - val_loss: 1.0382 - val_categorical_accuracy: 0.4200\nEpoch 6/100\n100/100 [==============================] - 111s 1s/step - loss: 1.0165 - categorical_accuracy: 0.4313 - val_loss: 1.0287 - val_categorical_accuracy: 0.4300\nEpoch 7/100\n100/100 [==============================] - 111s 1s/step - loss: 1.0121 - categorical_accuracy: 0.4325 - val_loss: 1.0217 - val_categorical_accuracy: 0.4550\nEpoch 8/100\n100/100 [==============================] - 111s 1s/step - loss: 0.9965 - categorical_accuracy: 0.4475 - val_loss: 1.0350 - val_categorical_accuracy: 0.4250\nEpoch 9/100\n100/100 [==============================] - 110s 1s/step - loss: 0.9986 - categorical_accuracy: 0.4338 - val_loss: 1.0359 - val_categorical_accuracy: 0.4300\nEpoch 10/100\n100/100 [==============================] - 110s 1s/step - loss: 0.9968 - categorical_accuracy: 0.4462 - val_loss: 1.0097 - val_categorical_accuracy: 0.4800\nEpoch 11/100\n100/100 [==============================] - 112s 1s/step - loss: 0.9895 - categorical_accuracy: 0.4450 - val_loss: 0.9942 - val_categorical_accuracy: 0.4850\nEpoch 12/100\n100/100 [==============================] - 114s 1s/step - loss: 0.9791 - categorical_accuracy: 0.4737 - val_loss: 0.9869 - val_categorical_accuracy: 0.5000\nEpoch 13/100\n100/100 [==============================] - 117s 1s/step - loss: 0.9809 - categorical_accuracy: 0.4762 - val_loss: 0.9966 - val_categorical_accuracy: 0.4800\nEpoch 14/100\n100/100 [==============================] - 110s 1s/step - loss: 1.0003 - categorical_accuracy: 0.4525 - val_loss: 1.0325 - val_categorical_accuracy: 0.4550\nEpoch 15/100\n100/100 [==============================] - 109s 1s/step - loss: 0.9875 - categorical_accuracy: 0.4625 - val_loss: 1.0103 - val_categorical_accuracy: 0.4400\nEpoch 16/100\n100/100 [==============================] - 110s 1s/step - loss: 0.9707 - categorical_accuracy: 0.4875 - val_loss: 0.9767 - val_categorical_accuracy: 0.5050\nEpoch 17/100\n100/100 [==============================] - 112s 1s/step - loss: 0.9819 - categorical_accuracy: 0.4913 - val_loss: 0.9874 - val_categorical_accuracy: 0.5050\nEpoch 18/100\n100/100 [==============================] - 110s 1s/step - loss: 0.9472 - categorical_accuracy: 0.5163 - val_loss: 0.9820 - val_categorical_accuracy: 0.5250\nEpoch 19/100\n100/100 [==============================] - 110s 1s/step - loss: 0.9982 - categorical_accuracy: 0.4875 - val_loss: 1.0341 - val_categorical_accuracy: 0.4050\nEpoch 20/100\n100/100 [==============================] - 112s 1s/step - loss: 1.0052 - categorical_accuracy: 0.4450 - val_loss: 1.0306 - val_categorical_accuracy: 0.4300\nEpoch 21/100\n100/100 [==============================] - 110s 1s/step - loss: 0.9879 - categorical_accuracy: 0.4725 - val_loss: 1.0102 - val_categorical_accuracy: 0.4500\nEpoch 22/100\n100/100 [==============================] - 110s 1s/step - loss: 0.9710 - categorical_accuracy: 0.4850 - val_loss: 1.0072 - val_categorical_accuracy: 0.4600\nEpoch 23/100\n100/100 [==============================] - 112s 1s/step - loss: 0.9502 - categorical_accuracy: 0.4975 - val_loss: 1.0032 - val_categorical_accuracy: 0.4650\nEpoch 24/100\n100/100 [==============================] - 110s 1s/step - loss: 0.9483 - categorical_accuracy: 0.5075 - val_loss: 0.9726 - val_categorical_accuracy: 0.4500\nEpoch 25/100\n100/100 [==============================] - 109s 1s/step - loss: 0.9307 - categorical_accuracy: 0.5100 - val_loss: 0.9588 - val_categorical_accuracy: 0.5600\nEpoch 26/100\n100/100 [==============================] - 111s 1s/step - loss: 0.9019 - categorical_accuracy: 0.5475 - val_loss: 0.9240 - val_categorical_accuracy: 0.5850\nEpoch 27/100\n100/100 [==============================] - 111s 1s/step - loss: 0.9242 - categorical_accuracy: 0.5587 - val_loss: 0.9578 - val_categorical_accuracy: 0.5200\nEpoch 28/100\n100/100 [==============================] - 109s 1s/step - loss: 0.8851 - categorical_accuracy: 0.6187 - val_loss: 1.1413 - val_categorical_accuracy: 0.5800\nEpoch 29/100\n100/100 [==============================] - 109s 1s/step - loss: 1.4352 - categorical_accuracy: 0.5888 - val_loss: 1.0008 - val_categorical_accuracy: 0.4150\nEpoch 30/100\n100/100 [==============================] - 110s 1s/step - loss: 0.9544 - categorical_accuracy: 0.4725 - val_loss: 0.9978 - val_categorical_accuracy: 0.4250\nEpoch 31/100\n100/100 [==============================] - 113s 1s/step - loss: 0.9164 - categorical_accuracy: 0.5075 - val_loss: 0.9552 - val_categorical_accuracy: 0.5200\nEpoch 32/100\n100/100 [==============================] - 114s 1s/step - loss: 0.8890 - categorical_accuracy: 0.5250 - val_loss: 0.9266 - val_categorical_accuracy: 0.5100\nEpoch 33/100\n100/100 [==============================] - 110s 1s/step - loss: 0.8651 - categorical_accuracy: 0.5713 - val_loss: 0.9084 - val_categorical_accuracy: 0.5800\nEpoch 34/100\n100/100 [==============================] - 110s 1s/step - loss: 0.8288 - categorical_accuracy: 0.6250 - val_loss: 0.8849 - val_categorical_accuracy: 0.6050\nEpoch 35/100\n100/100 [==============================] - 109s 1s/step - loss: 0.8042 - categorical_accuracy: 0.6450 - val_loss: 0.8688 - val_categorical_accuracy: 0.5900\nEpoch 36/100\n100/100 [==============================] - 114s 1s/step - loss: 0.7923 - categorical_accuracy: 0.6338 - val_loss: 0.8467 - val_categorical_accuracy: 0.6200\nEpoch 37/100\n100/100 [==============================] - 115s 1s/step - loss: 0.7443 - categorical_accuracy: 0.6612 - val_loss: 0.8380 - val_categorical_accuracy: 0.6300\nEpoch 38/100\n100/100 [==============================] - 113s 1s/step - loss: 0.7188 - categorical_accuracy: 0.6550 - val_loss: 0.8050 - val_categorical_accuracy: 0.6300\nEpoch 39/100\n100/100 [==============================] - 114s 1s/step - loss: 0.7000 - categorical_accuracy: 0.6712 - val_loss: 0.7784 - val_categorical_accuracy: 0.5950\nEpoch 40/100\n100/100 [==============================] - 135s 1s/step - loss: 0.6665 - categorical_accuracy: 0.6775 - val_loss: 0.8297 - val_categorical_accuracy: 0.6250\nEpoch 41/100\n100/100 [==============================] - 112s 1s/step - loss: 0.6589 - categorical_accuracy: 0.6875 - val_loss: 0.8402 - val_categorical_accuracy: 0.6150\nEpoch 42/100\n100/100 [==============================] - 111s 1s/step - loss: 0.6465 - categorical_accuracy: 0.7038 - val_loss: 0.7909 - val_categorical_accuracy: 0.6300\nEpoch 43/100\n100/100 [==============================] - 111s 1s/step - loss: 0.5982 - categorical_accuracy: 0.7075 - val_loss: 1.0760 - val_categorical_accuracy: 0.6400\nEpoch 44/100\n100/100 [==============================] - 113s 1s/step - loss: 0.6200 - categorical_accuracy: 0.7150 - val_loss: 0.6712 - val_categorical_accuracy: 0.6900\nEpoch 45/100\n100/100 [==============================] - 110s 1s/step - loss: 0.5590 - categorical_accuracy: 0.7525 - val_loss: 0.7064 - val_categorical_accuracy: 0.7050\nEpoch 46/100\n100/100 [==============================] - 110s 1s/step - loss: 0.6166 - categorical_accuracy: 0.7150 - val_loss: 0.6527 - val_categorical_accuracy: 0.7050\nEpoch 47/100\n100/100 [==============================] - 109s 1s/step - loss: 0.5773 - categorical_accuracy: 0.7200 - val_loss: 0.5891 - val_categorical_accuracy: 0.7550\nEpoch 48/100\n100/100 [==============================] - 109s 1s/step - loss: 0.5237 - categorical_accuracy: 0.7600 - val_loss: 0.7052 - val_categorical_accuracy: 0.6700\nEpoch 49/100\n100/100 [==============================] - 111s 1s/step - loss: 0.5480 - categorical_accuracy: 0.7575 - val_loss: 0.6927 - val_categorical_accuracy: 0.6600\nEpoch 50/100\n100/100 [==============================] - 110s 1s/step - loss: 0.5202 - categorical_accuracy: 0.7763 - val_loss: 0.6563 - val_categorical_accuracy: 0.7400\nEpoch 51/100\n100/100 [==============================] - 111s 1s/step - loss: 0.5672 - categorical_accuracy: 0.7287 - val_loss: 0.7809 - val_categorical_accuracy: 0.5650\nEpoch 52/100\n100/100 [==============================] - 110s 1s/step - loss: 0.5792 - categorical_accuracy: 0.7325 - val_loss: 0.6705 - val_categorical_accuracy: 0.7000\nEpoch 53/100\n100/100 [==============================] - 110s 1s/step - loss: 0.5274 - categorical_accuracy: 0.7675 - val_loss: 0.7247 - val_categorical_accuracy: 0.6550\nEpoch 54/100\n100/100 [==============================] - 111s 1s/step - loss: 0.5124 - categorical_accuracy: 0.7700 - val_loss: 0.5943 - val_categorical_accuracy: 0.7850\nEpoch 55/100\n100/100 [==============================] - 109s 1s/step - loss: 0.5104 - categorical_accuracy: 0.7700 - val_loss: 0.7204 - val_categorical_accuracy: 0.7350\nEpoch 56/100\n100/100 [==============================] - 109s 1s/step - loss: 0.4895 - categorical_accuracy: 0.7887 - val_loss: 0.6001 - val_categorical_accuracy: 0.7950\nEpoch 57/100\n100/100 [==============================] - 110s 1s/step - loss: 0.4588 - categorical_accuracy: 0.8000 - val_loss: 0.8428 - val_categorical_accuracy: 0.7200\nEpoch 58/100\n100/100 [==============================] - 111s 1s/step - loss: 0.4327 - categorical_accuracy: 0.8163 - val_loss: 0.7577 - val_categorical_accuracy: 0.7100\nEpoch 59/100\n100/100 [==============================] - 110s 1s/step - loss: 0.4274 - categorical_accuracy: 0.8112 - val_loss: 0.5036 - val_categorical_accuracy: 0.8000\nEpoch 60/100\n100/100 [==============================] - 110s 1s/step - loss: 0.3966 - categorical_accuracy: 0.8363 - val_loss: 0.6273 - val_categorical_accuracy: 0.7750\nEpoch 61/100\n100/100 [==============================] - 111s 1s/step - loss: 0.4285 - categorical_accuracy: 0.8112 - val_loss: 0.5094 - val_categorical_accuracy: 0.8000\nEpoch 62/100\n100/100 [==============================] - 113s 1s/step - loss: 0.4438 - categorical_accuracy: 0.8200 - val_loss: 0.5065 - val_categorical_accuracy: 0.7900\nEpoch 63/100\n100/100 [==============================] - 110s 1s/step - loss: 0.3869 - categorical_accuracy: 0.8288 - val_loss: 0.4688 - val_categorical_accuracy: 0.7850\nEpoch 64/100\n100/100 [==============================] - 110s 1s/step - loss: 0.3627 - categorical_accuracy: 0.8475 - val_loss: 0.4777 - val_categorical_accuracy: 0.8100\nEpoch 65/100\n100/100 [==============================] - 110s 1s/step - loss: 0.3324 - categorical_accuracy: 0.8662 - val_loss: 0.5550 - val_categorical_accuracy: 0.7900\nEpoch 66/100\n100/100 [==============================] - 110s 1s/step - loss: 0.3505 - categorical_accuracy: 0.8525 - val_loss: 0.5402 - val_categorical_accuracy: 0.8000\nEpoch 67/100\n100/100 [==============================] - 111s 1s/step - loss: 0.3451 - categorical_accuracy: 0.8550 - val_loss: 0.6072 - val_categorical_accuracy: 0.8100\nEpoch 68/100\n100/100 [==============================] - 109s 1s/step - loss: 0.3403 - categorical_accuracy: 0.8487 - val_loss: 0.6118 - val_categorical_accuracy: 0.7750\nEpoch 69/100\n100/100 [==============================] - 111s 1s/step - loss: 0.3364 - categorical_accuracy: 0.8500 - val_loss: 0.4940 - val_categorical_accuracy: 0.8250\nEpoch 70/100\n100/100 [==============================] - 109s 1s/step - loss: 0.3381 - categorical_accuracy: 0.8512 - val_loss: 0.4824 - val_categorical_accuracy: 0.8350\nEpoch 71/100\n100/100 [==============================] - 110s 1s/step - loss: 0.3454 - categorical_accuracy: 0.8575 - val_loss: 0.4311 - val_categorical_accuracy: 0.8400\nEpoch 72/100\n100/100 [==============================] - 109s 1s/step - loss: 0.3437 - categorical_accuracy: 0.8662 - val_loss: 0.4943 - val_categorical_accuracy: 0.8450\nEpoch 73/100\n100/100 [==============================] - 109s 1s/step - loss: 0.2918 - categorical_accuracy: 0.8875 - val_loss: 0.5411 - val_categorical_accuracy: 0.8050\nEpoch 74/100\n100/100 [==============================] - 109s 1s/step - loss: 0.3121 - categorical_accuracy: 0.8675 - val_loss: 0.4652 - val_categorical_accuracy: 0.8450\nEpoch 75/100\n100/100 [==============================] - 109s 1s/step - loss: 0.2864 - categorical_accuracy: 0.8800 - val_loss: 0.5134 - val_categorical_accuracy: 0.8100\nEpoch 76/100\n100/100 [==============================] - 109s 1s/step - loss: 0.2804 - categorical_accuracy: 0.8863 - val_loss: 0.4984 - val_categorical_accuracy: 0.8400\nEpoch 77/100\n100/100 [==============================] - 110s 1s/step - loss: 0.3006 - categorical_accuracy: 0.8662 - val_loss: 0.4925 - val_categorical_accuracy: 0.8050\nEpoch 78/100\n100/100 [==============================] - 110s 1s/step - loss: 0.2835 - categorical_accuracy: 0.8813 - val_loss: 0.5111 - val_categorical_accuracy: 0.8350\nEpoch 79/100\n100/100 [==============================] - 110s 1s/step - loss: 0.3262 - categorical_accuracy: 0.8550 - val_loss: 0.4960 - val_categorical_accuracy: 0.8600\nEpoch 80/100\n100/100 [==============================] - 111s 1s/step - loss: 0.2724 - categorical_accuracy: 0.8875 - val_loss: 0.4853 - val_categorical_accuracy: 0.8000\nEpoch 81/100\n100/100 [==============================] - 110s 1s/step - loss: 0.2442 - categorical_accuracy: 0.9050 - val_loss: 0.5668 - val_categorical_accuracy: 0.8350\nEpoch 82/100\n100/100 [==============================] - 111s 1s/step - loss: 0.2578 - categorical_accuracy: 0.8950 - val_loss: 0.5129 - val_categorical_accuracy: 0.8300\nEpoch 83/100\n100/100 [==============================] - 110s 1s/step - loss: 0.2474 - categorical_accuracy: 0.9013 - val_loss: 0.4740 - val_categorical_accuracy: 0.8450\nEpoch 84/100\n100/100 [==============================] - 110s 1s/step - loss: 0.2584 - categorical_accuracy: 0.8963 - val_loss: 0.4882 - val_categorical_accuracy: 0.8200\nEpoch 85/100\n100/100 [==============================] - 111s 1s/step - loss: 0.3075 - categorical_accuracy: 0.8687 - val_loss: 0.4211 - val_categorical_accuracy: 0.8250\nEpoch 86/100\n100/100 [==============================] - 111s 1s/step - loss: 0.2379 - categorical_accuracy: 0.9087 - val_loss: 0.4819 - val_categorical_accuracy: 0.8500\nEpoch 87/100\n100/100 [==============================] - 110s 1s/step - loss: 0.2550 - categorical_accuracy: 0.9038 - val_loss: 0.4832 - val_categorical_accuracy: 0.8250\nEpoch 88/100\n100/100 [==============================] - 112s 1s/step - loss: 0.2213 - categorical_accuracy: 0.9087 - val_loss: 0.4847 - val_categorical_accuracy: 0.8200\nEpoch 89/100\n100/100 [==============================] - 111s 1s/step - loss: 0.2564 - categorical_accuracy: 0.8913 - val_loss: 0.4636 - val_categorical_accuracy: 0.8400\nEpoch 90/100\n100/100 [==============================] - 112s 1s/step - loss: 0.2116 - categorical_accuracy: 0.9075 - val_loss: 0.5058 - val_categorical_accuracy: 0.8200\nEpoch 91/100\n100/100 [==============================] - 110s 1s/step - loss: 0.2280 - categorical_accuracy: 0.9075 - val_loss: 0.4817 - val_categorical_accuracy: 0.8350\nEpoch 92/100\n100/100 [==============================] - 115s 1s/step - loss: 0.2159 - categorical_accuracy: 0.9212 - val_loss: 0.4433 - val_categorical_accuracy: 0.8250\nEpoch 93/100\n100/100 [==============================] - 116s 1s/step - loss: 0.2187 - categorical_accuracy: 0.9175 - val_loss: 0.4808 - val_categorical_accuracy: 0.8150\nEpoch 94/100\n100/100 [==============================] - 120s 1s/step - loss: 0.2371 - categorical_accuracy: 0.9062 - val_loss: 0.5326 - val_categorical_accuracy: 0.8000\nEpoch 95/100\n100/100 [==============================] - 117s 1s/step - loss: 0.2527 - categorical_accuracy: 0.9075 - val_loss: 0.6023 - val_categorical_accuracy: 0.7700\nEpoch 96/100\n100/100 [==============================] - 117s 1s/step - loss: 0.1956 - categorical_accuracy: 0.9300 - val_loss: 0.5735 - val_categorical_accuracy: 0.8450\nEpoch 97/100\n100/100 [==============================] - 117s 1s/step - loss: 0.2465 - categorical_accuracy: 0.9013 - val_loss: 0.4723 - val_categorical_accuracy: 0.8300\nEpoch 98/100\n100/100 [==============================] - 110s 1s/step - loss: 0.1934 - categorical_accuracy: 0.9237 - val_loss: 0.7244 - val_categorical_accuracy: 0.7350\nEpoch 99/100\n100/100 [==============================] - 115s 1s/step - loss: 0.2157 - categorical_accuracy: 0.9150 - val_loss: 0.6189 - val_categorical_accuracy: 0.7950\nEpoch 100/100\n100/100 [==============================] - 112s 1s/step - loss: 0.2530 - categorical_accuracy: 0.8988 - val_loss: 0.4532 - val_categorical_accuracy: 0.8350\n","output_type":"stream"},{"execution_count":174,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.History at 0x7ee332e0a980>"},"metadata":{}}]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.callbacks import TensorBoard\nfrom tensorflow.keras import backend as K\nimport tensorflow as tf\n\nmodel = Sequential()\n#x,y,z -> y,z as the input shape\nmodel.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(CONFIG.VIDEO_LENGTH, 1629)))\nmodel.add(LSTM(128, return_sequences=True, activation='relu'))\nmodel.add(LSTM(64, return_sequences=False, activation='relu'))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n\n\n#Adam Optimizer - fixed learning rate.\nadam_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n\n\nmodel.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n\nprint(model.summary())\n\nmodel.fit(train_dataset_parquet, epochs=100, validation_data = val_dataset_parquet, batch_size = 8, callbacks=[])","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:57:24.782965Z","iopub.execute_input":"2024-04-06T19:57:24.783730Z","iopub.status.idle":"2024-04-06T22:31:18.351910Z","shell.execute_reply.started":"2024-04-06T19:57:24.783671Z","shell.execute_reply":"2024-04-06T22:31:18.332639Z"},"trusted":true},"execution_count":176,"outputs":[{"name":"stdout","text":"Model: \"sequential_13\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_29 (LSTM)              (None, 60, 64)            433664    \n                                                                 \n lstm_30 (LSTM)              (None, 60, 128)           98816     \n                                                                 \n lstm_31 (LSTM)              (None, 64)                49408     \n                                                                 \n dense_38 (Dense)            (None, 128)               8320      \n                                                                 \n dense_39 (Dense)            (None, 256)               33024     \n                                                                 \n dense_40 (Dense)            (None, 3)                 771       \n                                                                 \n=================================================================\nTotal params: 624003 (2.38 MB)\nTrainable params: 624003 (2.38 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\nEpoch 1/100\n100/100 [==============================] - 116s 1s/step - loss: 1.3551 - categorical_accuracy: 0.4000 - val_loss: 1.0955 - val_categorical_accuracy: 0.4050\nEpoch 2/100\n100/100 [==============================] - 111s 1s/step - loss: 1.1271 - categorical_accuracy: 0.3963 - val_loss: 1.0884 - val_categorical_accuracy: 0.4050\nEpoch 3/100\n100/100 [==============================] - 111s 1s/step - loss: 1.0864 - categorical_accuracy: 0.4238 - val_loss: 1.0543 - val_categorical_accuracy: 0.4350\nEpoch 4/100\n100/100 [==============================] - 111s 1s/step - loss: 1.0682 - categorical_accuracy: 0.4075 - val_loss: 1.0443 - val_categorical_accuracy: 0.4450\nEpoch 5/100\n100/100 [==============================] - 112s 1s/step - loss: 1.0504 - categorical_accuracy: 0.4013 - val_loss: 1.0406 - val_categorical_accuracy: 0.4350\nEpoch 6/100\n100/100 [==============================] - 109s 1s/step - loss: 1.0411 - categorical_accuracy: 0.4162 - val_loss: 1.0436 - val_categorical_accuracy: 0.4300\nEpoch 7/100\n100/100 [==============================] - 108s 1s/step - loss: 1.0337 - categorical_accuracy: 0.4250 - val_loss: 1.0348 - val_categorical_accuracy: 0.4400\nEpoch 8/100\n100/100 [==============================] - 111s 1s/step - loss: 1.0261 - categorical_accuracy: 0.4363 - val_loss: 1.0357 - val_categorical_accuracy: 0.4550\nEpoch 9/100\n100/100 [==============================] - 112s 1s/step - loss: 1.0195 - categorical_accuracy: 0.4300 - val_loss: 1.0335 - val_categorical_accuracy: 0.4250\nEpoch 10/100\n100/100 [==============================] - 112s 1s/step - loss: 1.0148 - categorical_accuracy: 0.4525 - val_loss: 1.0359 - val_categorical_accuracy: 0.4150\nEpoch 11/100\n100/100 [==============================] - 135s 1s/step - loss: 1.0107 - categorical_accuracy: 0.4200 - val_loss: 1.0408 - val_categorical_accuracy: 0.4100\nEpoch 12/100\n100/100 [==============================] - 114s 1s/step - loss: 1.0097 - categorical_accuracy: 0.4500 - val_loss: 1.0417 - val_categorical_accuracy: 0.4100\nEpoch 13/100\n100/100 [==============================] - 112s 1s/step - loss: 1.0040 - categorical_accuracy: 0.4550 - val_loss: 1.0449 - val_categorical_accuracy: 0.4150\nEpoch 14/100\n100/100 [==============================] - 114s 1s/step - loss: 1.0013 - categorical_accuracy: 0.4588 - val_loss: 1.0502 - val_categorical_accuracy: 0.3950\nEpoch 15/100\n100/100 [==============================] - 118s 1s/step - loss: 1.0061 - categorical_accuracy: 0.4563 - val_loss: 1.0500 - val_categorical_accuracy: 0.4050\nEpoch 16/100\n100/100 [==============================] - 111s 1s/step - loss: 1.0102 - categorical_accuracy: 0.4512 - val_loss: 1.0598 - val_categorical_accuracy: 0.4050\nEpoch 17/100\n100/100 [==============================] - 115s 1s/step - loss: 0.9966 - categorical_accuracy: 0.4588 - val_loss: 1.0473 - val_categorical_accuracy: 0.4050\nEpoch 18/100\n100/100 [==============================] - 113s 1s/step - loss: 0.9953 - categorical_accuracy: 0.4525 - val_loss: 1.0517 - val_categorical_accuracy: 0.4050\nEpoch 19/100\n100/100 [==============================] - 112s 1s/step - loss: 0.9936 - categorical_accuracy: 0.4625 - val_loss: 1.0601 - val_categorical_accuracy: 0.3850\nEpoch 20/100\n100/100 [==============================] - 110s 1s/step - loss: 0.9979 - categorical_accuracy: 0.4563 - val_loss: 1.0582 - val_categorical_accuracy: 0.3950\nEpoch 21/100\n100/100 [==============================] - 110s 1s/step - loss: 0.9901 - categorical_accuracy: 0.4600 - val_loss: 1.0656 - val_categorical_accuracy: 0.4600\nEpoch 22/100\n100/100 [==============================] - 113s 1s/step - loss: 0.9845 - categorical_accuracy: 0.4650 - val_loss: 1.0643 - val_categorical_accuracy: 0.4200\nEpoch 23/100\n100/100 [==============================] - 112s 1s/step - loss: 1.0045 - categorical_accuracy: 0.4725 - val_loss: 2.0736 - val_categorical_accuracy: 0.3400\nEpoch 24/100\n100/100 [==============================] - 111s 1s/step - loss: 1.2533 - categorical_accuracy: 0.4275 - val_loss: 1.0367 - val_categorical_accuracy: 0.4250\nEpoch 25/100\n100/100 [==============================] - 111s 1s/step - loss: 1.0239 - categorical_accuracy: 0.4363 - val_loss: 1.0353 - val_categorical_accuracy: 0.4300\nEpoch 26/100\n100/100 [==============================] - 111s 1s/step - loss: 1.0224 - categorical_accuracy: 0.4412 - val_loss: 1.0354 - val_categorical_accuracy: 0.4250\nEpoch 27/100\n100/100 [==============================] - 109s 1s/step - loss: 1.0204 - categorical_accuracy: 0.4350 - val_loss: 1.0355 - val_categorical_accuracy: 0.4250\nEpoch 28/100\n100/100 [==============================] - 110s 1s/step - loss: 1.0190 - categorical_accuracy: 0.4462 - val_loss: 1.0357 - val_categorical_accuracy: 0.4300\nEpoch 29/100\n100/100 [==============================] - 112s 1s/step - loss: 1.0177 - categorical_accuracy: 0.4313 - val_loss: 1.0347 - val_categorical_accuracy: 0.4400\nEpoch 30/100\n100/100 [==============================] - 111s 1s/step - loss: 1.0162 - categorical_accuracy: 0.4363 - val_loss: 1.0329 - val_categorical_accuracy: 0.4400\nEpoch 31/100\n100/100 [==============================] - 115s 1s/step - loss: 1.0146 - categorical_accuracy: 0.4375 - val_loss: 1.0318 - val_categorical_accuracy: 0.4450\nEpoch 32/100\n100/100 [==============================] - 110s 1s/step - loss: 1.0119 - categorical_accuracy: 0.4350 - val_loss: 1.0294 - val_categorical_accuracy: 0.4450\nEpoch 33/100\n100/100 [==============================] - 111s 1s/step - loss: 1.0115 - categorical_accuracy: 0.4238 - val_loss: 1.0279 - val_categorical_accuracy: 0.4250\nEpoch 34/100\n100/100 [==============================] - 109s 1s/step - loss: 1.0099 - categorical_accuracy: 0.4412 - val_loss: 1.0266 - val_categorical_accuracy: 0.4450\nEpoch 35/100\n100/100 [==============================] - 110s 1s/step - loss: 1.0085 - categorical_accuracy: 0.4412 - val_loss: 1.0265 - val_categorical_accuracy: 0.4700\nEpoch 36/100\n100/100 [==============================] - 110s 1s/step - loss: 1.0080 - categorical_accuracy: 0.4425 - val_loss: 1.0234 - val_categorical_accuracy: 0.4350\nEpoch 37/100\n100/100 [==============================] - 111s 1s/step - loss: 1.0092 - categorical_accuracy: 0.4512 - val_loss: 1.0257 - val_categorical_accuracy: 0.4450\nEpoch 38/100\n100/100 [==============================] - 110s 1s/step - loss: 1.0097 - categorical_accuracy: 0.4525 - val_loss: 1.0240 - val_categorical_accuracy: 0.4450\nEpoch 39/100\n100/100 [==============================] - 111s 1s/step - loss: 1.0051 - categorical_accuracy: 0.4437 - val_loss: 1.0217 - val_categorical_accuracy: 0.4750\nEpoch 40/100\n100/100 [==============================] - 131s 1s/step - loss: 1.0049 - categorical_accuracy: 0.4487 - val_loss: 1.0184 - val_categorical_accuracy: 0.4650\nEpoch 41/100\n100/100 [==============================] - 113s 1s/step - loss: 1.0020 - categorical_accuracy: 0.4712 - val_loss: 1.0841 - val_categorical_accuracy: 0.4500\nEpoch 42/100\n100/100 [==============================] - 112s 1s/step - loss: 1.0842 - categorical_accuracy: 0.4350 - val_loss: 1.0236 - val_categorical_accuracy: 0.4350\nEpoch 43/100\n100/100 [==============================] - 114s 1s/step - loss: 1.0035 - categorical_accuracy: 0.4487 - val_loss: 1.0216 - val_categorical_accuracy: 0.4550\nEpoch 44/100\n100/100 [==============================] - 112s 1s/step - loss: 1.0005 - categorical_accuracy: 0.4650 - val_loss: 1.0184 - val_categorical_accuracy: 0.4450\nEpoch 45/100\n100/100 [==============================] - 114s 1s/step - loss: 1.0010 - categorical_accuracy: 0.4512 - val_loss: 1.0188 - val_categorical_accuracy: 0.4550\nEpoch 46/100\n100/100 [==============================] - 111s 1s/step - loss: 0.9980 - categorical_accuracy: 0.4625 - val_loss: 1.0235 - val_categorical_accuracy: 0.4450\nEpoch 47/100\n100/100 [==============================] - 110s 1s/step - loss: 1.0060 - categorical_accuracy: 0.4512 - val_loss: 1.0240 - val_categorical_accuracy: 0.4450\nEpoch 48/100\n100/100 [==============================] - 110s 1s/step - loss: 1.0032 - categorical_accuracy: 0.4475 - val_loss: 1.0227 - val_categorical_accuracy: 0.4150\nEpoch 49/100\n100/100 [==============================] - 112s 1s/step - loss: 0.9962 - categorical_accuracy: 0.4563 - val_loss: 1.0225 - val_categorical_accuracy: 0.4250\nEpoch 50/100\n100/100 [==============================] - 115s 1s/step - loss: 0.9931 - categorical_accuracy: 0.4563 - val_loss: 1.0197 - val_categorical_accuracy: 0.4400\nEpoch 51/100\n100/100 [==============================] - 112s 1s/step - loss: 0.9894 - categorical_accuracy: 0.4875 - val_loss: 1.0261 - val_categorical_accuracy: 0.4300\nEpoch 52/100\n100/100 [==============================] - 113s 1s/step - loss: 0.9895 - categorical_accuracy: 0.4812 - val_loss: 1.0171 - val_categorical_accuracy: 0.4400\nEpoch 53/100\n100/100 [==============================] - 112s 1s/step - loss: 26.4741 - categorical_accuracy: 0.4525 - val_loss: 14847.0635 - val_categorical_accuracy: 0.3900\nEpoch 54/100\n100/100 [==============================] - 111s 1s/step - loss: 95963.8828 - categorical_accuracy: 0.3800 - val_loss: 6021.8511 - val_categorical_accuracy: 0.4200\nEpoch 55/100\n100/100 [==============================] - 110s 1s/step - loss: 183948.2812 - categorical_accuracy: 0.4050 - val_loss: 176169.5625 - val_categorical_accuracy: 0.3900\nEpoch 56/100\n100/100 [==============================] - 112s 1s/step - loss: 64993.3711 - categorical_accuracy: 0.3800 - val_loss: 5770.7036 - val_categorical_accuracy: 0.3750\nEpoch 57/100\n100/100 [==============================] - 111s 1s/step - loss: 2911.3303 - categorical_accuracy: 0.3475 - val_loss: 3278.1450 - val_categorical_accuracy: 0.3650\nEpoch 58/100\n100/100 [==============================] - 112s 1s/step - loss: 8553.1572 - categorical_accuracy: 0.3738 - val_loss: 14601.6387 - val_categorical_accuracy: 0.3050\nEpoch 59/100\n100/100 [==============================] - 110s 1s/step - loss: 27734.3867 - categorical_accuracy: 0.3500 - val_loss: 13635.7002 - val_categorical_accuracy: 0.4100\nEpoch 60/100\n100/100 [==============================] - 111s 1s/step - loss: 8254.9648 - categorical_accuracy: 0.3675 - val_loss: 57810.8281 - val_categorical_accuracy: 0.3650\nEpoch 61/100\n100/100 [==============================] - 118s 1s/step - loss: 35802.4766 - categorical_accuracy: 0.3850 - val_loss: 32765.6934 - val_categorical_accuracy: 0.3050\nEpoch 62/100\n100/100 [==============================] - 110s 1s/step - loss: 27947.1055 - categorical_accuracy: 0.4038 - val_loss: 61810.8906 - val_categorical_accuracy: 0.3650\nEpoch 63/100\n100/100 [==============================] - 109s 1s/step - loss: 61083.3281 - categorical_accuracy: 0.3812 - val_loss: 87150.2969 - val_categorical_accuracy: 0.3950\nEpoch 64/100\n100/100 [==============================] - 112s 1s/step - loss: 46122.1289 - categorical_accuracy: 0.3613 - val_loss: 50374.7539 - val_categorical_accuracy: 0.3250\nEpoch 65/100\n100/100 [==============================] - 112s 1s/step - loss: 42049.1602 - categorical_accuracy: 0.4150 - val_loss: 37090.0664 - val_categorical_accuracy: 0.4450\nEpoch 66/100\n100/100 [==============================] - 118s 1s/step - loss: 34922.7539 - categorical_accuracy: 0.4062 - val_loss: 49658.7031 - val_categorical_accuracy: 0.4100\nEpoch 67/100\n100/100 [==============================] - 111s 1s/step - loss: 42884.7383 - categorical_accuracy: 0.4062 - val_loss: 57952.4297 - val_categorical_accuracy: 0.3800\nEpoch 68/100\n100/100 [==============================] - 111s 1s/step - loss: 49711.4609 - categorical_accuracy: 0.3600 - val_loss: 97314.6484 - val_categorical_accuracy: 0.3800\nEpoch 69/100\n100/100 [==============================] - 109s 1s/step - loss: 52074.4219 - categorical_accuracy: 0.3812 - val_loss: 105636.9609 - val_categorical_accuracy: 0.2950\nEpoch 70/100\n 82/100 [=======================>......] - ETA: 15s - loss: 78367.1641 - categorical_accuracy: 0.3643","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[176], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39madam_optimizer, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39msummary())\n\u001b[0;32m---> 25\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset_parquet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_dataset_parquet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.callbacks import TensorBoard\nfrom tensorflow.keras import backend as K\nimport tensorflow as tf\n\nmodel = Sequential()\n#x,y,z -> y,z as the input shape\nmodel.add(LSTM(64, return_sequences=False, activation='relu', input_shape=(CONFIG.VIDEO_LENGTH, 1629)))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n\n#Adam Optimizer - fixed learning rate.\nadam_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n\n\nmodel.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n\nprint(model.summary())\n\nmodel.fit(train_dataset_parquet, epochs=100, validation_data = val_dataset_parquet, batch_size = 16, callbacks=[])","metadata":{"execution":{"iopub.status.busy":"2024-04-06T22:31:19.060788Z","iopub.execute_input":"2024-04-06T22:31:19.061290Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Model: \"sequential_14\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_32 (LSTM)              (None, 64)                433664    \n                                                                 \n dense_41 (Dense)            (None, 128)               8320      \n                                                                 \n dense_42 (Dense)            (None, 256)               33024     \n                                                                 \n dense_43 (Dense)            (None, 3)                 771       \n                                                                 \n=================================================================\nTotal params: 475779 (1.81 MB)\nTrainable params: 475779 (1.81 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\nEpoch 1/100\n100/100 [==============================] - 113s 1s/step - loss: 1.0939 - categorical_accuracy: 0.4225 - val_loss: 1.0831 - val_categorical_accuracy: 0.4150\nEpoch 2/100\n100/100 [==============================] - 109s 1s/step - loss: 1.0714 - categorical_accuracy: 0.4288 - val_loss: 1.0627 - val_categorical_accuracy: 0.4050\nEpoch 3/100\n100/100 [==============================] - 112s 1s/step - loss: 1.0544 - categorical_accuracy: 0.4313 - val_loss: 1.0452 - val_categorical_accuracy: 0.4050\nEpoch 4/100\n100/100 [==============================] - 109s 1s/step - loss: 1.0701 - categorical_accuracy: 0.4225 - val_loss: 1.0403 - val_categorical_accuracy: 0.4100\nEpoch 5/100\n100/100 [==============================] - 108s 1s/step - loss: 1.0325 - categorical_accuracy: 0.4313 - val_loss: 1.0372 - val_categorical_accuracy: 0.4050\nEpoch 6/100\n100/100 [==============================] - 107s 1s/step - loss: 1.0281 - categorical_accuracy: 0.4238 - val_loss: 1.0359 - val_categorical_accuracy: 0.4100\nEpoch 7/100\n100/100 [==============================] - 107s 1s/step - loss: 1.0247 - categorical_accuracy: 0.4363 - val_loss: 1.0348 - val_categorical_accuracy: 0.4100\nEpoch 8/100\n100/100 [==============================] - 107s 1s/step - loss: 1.0207 - categorical_accuracy: 0.4412 - val_loss: 1.0336 - val_categorical_accuracy: 0.4200\nEpoch 9/100\n100/100 [==============================] - 110s 1s/step - loss: 1.0112 - categorical_accuracy: 0.4512 - val_loss: 1.0279 - val_categorical_accuracy: 0.4100\nEpoch 10/100\n100/100 [==============================] - 109s 1s/step - loss: 1.0100 - categorical_accuracy: 0.4425 - val_loss: 1.0255 - val_categorical_accuracy: 0.4250\nEpoch 11/100\n100/100 [==============================] - 110s 1s/step - loss: 1.0016 - categorical_accuracy: 0.4512 - val_loss: 1.1090 - val_categorical_accuracy: 0.4300\nEpoch 12/100\n100/100 [==============================] - 109s 1s/step - loss: 1.0294 - categorical_accuracy: 0.4387 - val_loss: 1.0384 - val_categorical_accuracy: 0.4050\nEpoch 13/100\n100/100 [==============================] - 108s 1s/step - loss: 1.0058 - categorical_accuracy: 0.4512 - val_loss: 1.0322 - val_categorical_accuracy: 0.4100\nEpoch 14/100\n100/100 [==============================] - 107s 1s/step - loss: 0.9963 - categorical_accuracy: 0.4600 - val_loss: 1.0261 - val_categorical_accuracy: 0.4150\nEpoch 15/100\n100/100 [==============================] - 107s 1s/step - loss: 1.0034 - categorical_accuracy: 0.4550 - val_loss: 1.0404 - val_categorical_accuracy: 0.4100\nEpoch 16/100\n100/100 [==============================] - 111s 1s/step - loss: 0.9973 - categorical_accuracy: 0.4500 - val_loss: 1.0318 - val_categorical_accuracy: 0.4100\nEpoch 17/100\n100/100 [==============================] - 110s 1s/step - loss: 0.9914 - categorical_accuracy: 0.4613 - val_loss: 1.0283 - val_categorical_accuracy: 0.4150\nEpoch 18/100\n100/100 [==============================] - 113s 1s/step - loss: 0.9863 - categorical_accuracy: 0.4613 - val_loss: 1.0274 - val_categorical_accuracy: 0.4200\nEpoch 19/100\n100/100 [==============================] - 115s 1s/step - loss: 0.9778 - categorical_accuracy: 0.4650 - val_loss: 1.0178 - val_categorical_accuracy: 0.4200\nEpoch 20/100\n100/100 [==============================] - 114s 1s/step - loss: 0.9684 - categorical_accuracy: 0.4700 - val_loss: 1.0071 - val_categorical_accuracy: 0.4300\nEpoch 21/100\n100/100 [==============================] - 111s 1s/step - loss: 2.1197 - categorical_accuracy: 0.4462 - val_loss: 1.0426 - val_categorical_accuracy: 0.4050\nEpoch 22/100\n100/100 [==============================] - 110s 1s/step - loss: 1.0163 - categorical_accuracy: 0.4425 - val_loss: 1.0364 - val_categorical_accuracy: 0.4250\nEpoch 23/100\n100/100 [==============================] - 111s 1s/step - loss: 1.0116 - categorical_accuracy: 0.4475 - val_loss: 1.0278 - val_categorical_accuracy: 0.4100\nEpoch 24/100\n100/100 [==============================] - 113s 1s/step - loss: 1.0046 - categorical_accuracy: 0.4462 - val_loss: 1.0313 - val_categorical_accuracy: 0.4150\nEpoch 25/100\n100/100 [==============================] - 114s 1s/step - loss: 1.0009 - categorical_accuracy: 0.4538 - val_loss: 1.0299 - val_categorical_accuracy: 0.4100\nEpoch 26/100\n100/100 [==============================] - 110s 1s/step - loss: 0.9962 - categorical_accuracy: 0.4600 - val_loss: 1.0293 - val_categorical_accuracy: 0.4200\nEpoch 27/100\n100/100 [==============================] - 110s 1s/step - loss: 0.9892 - categorical_accuracy: 0.4500 - val_loss: 1.0091 - val_categorical_accuracy: 0.4450\nEpoch 28/100\n100/100 [==============================] - 108s 1s/step - loss: 8.8788 - categorical_accuracy: 0.4375 - val_loss: 41.5329 - val_categorical_accuracy: 0.3650\nEpoch 29/100\n100/100 [==============================] - 108s 1s/step - loss: 4.4051 - categorical_accuracy: 0.3925 - val_loss: 1.0869 - val_categorical_accuracy: 0.3700\nEpoch 30/100\n100/100 [==============================] - 111s 1s/step - loss: 1.0726 - categorical_accuracy: 0.3850 - val_loss: 1.0559 - val_categorical_accuracy: 0.3900\nEpoch 31/100\n100/100 [==============================] - 113s 1s/step - loss: 1.0501 - categorical_accuracy: 0.4075 - val_loss: 1.0526 - val_categorical_accuracy: 0.3850\nEpoch 32/100\n100/100 [==============================] - 110s 1s/step - loss: 1.0397 - categorical_accuracy: 0.4187 - val_loss: 1.0427 - val_categorical_accuracy: 0.4100\nEpoch 33/100\n 79/100 [======================>.......] - ETA: 18s - loss: 1.0301 - categorical_accuracy: 0.4209","output_type":"stream"}]}]}