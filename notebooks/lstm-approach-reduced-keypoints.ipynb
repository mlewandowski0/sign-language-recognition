{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.11/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.14.0)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.9.0.80)\n",
      "Requirement already satisfied: mediapipe in /usr/local/lib/python3.11/dist-packages (0.10.11)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.1)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (15.0.2)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.16.6)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.20.0)\n",
      "Collecting tqdm\n",
      "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/2a/14/e75e52d521442e2fcc9f1df3c5e456aead034203d4797867980de558ab34/tqdm-4.66.2-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.8.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.58.0)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.14.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.14.0)\n",
      "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.14.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (23.1.0)\n",
      "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.26)\n",
      "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.26)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from mediapipe) (2.2.2)\n",
      "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.9.0.80)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.6)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (1.44.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.11/dist-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (8.2.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
      "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.15.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.23.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.3.7)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe) (3.13.4)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->mediapipe) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->mediapipe) (12.4.127)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch->mediapipe) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.0)\n",
      "Downloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.66.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow opencv-python mediapipe scikit-learn matplotlib pandas pyarrow wandb plotly tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#Load parquet data into dataset_parquet for training.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.regularizers import l2\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import random\n",
    "import time\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    # root = os.path.join(\"/\", \"kaggle\", \"input\", \"asl-signs\") \n",
    "    root = os.path.join(\".\")\n",
    "    DATA_LIMIT = 100\n",
    "    BATCH_SIZE = 8\n",
    "    VIDEO_LENGTH = 60\n",
    "    TRAIN_VAL_SPLIT = 0.9\n",
    "    WANDB_RUN = \"mediapipe-asl-dataset\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPS :  40\n",
      "EYE_LEFT :  20\n",
      "EYE_RIGHT :  20\n",
      "LEFT_HAND :  21\n",
      "RIGHT_HAND :  21\n",
      "LEFT_POSE :  5\n",
      "RIGHT_POSE :  5\n",
      "132\n"
     ]
    }
   ],
   "source": [
    "LIPS_IDXS0 = np.array([\n",
    "        61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "        291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "        78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "        95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "    ])\n",
    "\n",
    "EYE_LEFT = np.array([33, 7, 246, 161, 160, 159, 158, 157, 173, 133, 155, 154, 153, 145, 144, 163, 471, 470, 469, 472])\n",
    "EYE_RIGHT = np.array([362, 398, 384, 385, 386, 387, 388, 466, 263, 249, 390, 373, 374, 380, 381, 382, 476, 475, 474, 477])\n",
    "# Landmark indices in original data\n",
    "LEFT_HAND_IDXS0 = np.arange(468,489)\n",
    "RIGHT_HAND_IDXS0 = np.arange(522,543)\n",
    "LEFT_POSE_IDXS0 = np.array([502, 504, 506, 508, 510])\n",
    "RIGHT_POSE_IDXS0 = np.array([503, 505, 507, 509, 511])\n",
    "\n",
    "print(\"LIPS : \",len(LIPS_IDXS0))\n",
    "print(\"EYE_LEFT : \",len(EYE_LEFT))\n",
    "print(\"EYE_RIGHT : \",len(EYE_RIGHT))\n",
    "print(\"LEFT_HAND : \",len(LEFT_HAND_IDXS0))\n",
    "print(\"RIGHT_HAND : \",len(RIGHT_HAND_IDXS0))\n",
    "print(\"LEFT_POSE : \",len(LEFT_POSE_IDXS0))\n",
    "print(\"RIGHT_POSE : \",len(RIGHT_POSE_IDXS0))\n",
    "\n",
    "all_selection = np.concatenate([LIPS_IDXS0, EYE_LEFT, EYE_RIGHT, LEFT_HAND_IDXS0, RIGHT_HAND_IDXS0, LEFT_POSE_IDXS0, RIGHT_POSE_IDXS0])\n",
    "print(len(all_selection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code sorts out a parquet files and rearrange the order to pose,face, left-hand, right-hand\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "\n",
    "ids = None\n",
    "\n",
    "order_global = {\"pose\" : 10000, \"face\" : 1000, \"left_hand\" : 100, \"right_hand\" : 10}\n",
    "\n",
    "def visualize_keypoints(frames : np.ndarray, point_size : int):\n",
    "    if len(frames.shape) == 1:\n",
    "        frames = np.array([frames])\n",
    "    \n",
    "    for frame in frames:\n",
    "        frame = frame.reshape(-1, 3)\n",
    "        sizes = point_size * np.ones(frame.shape[0])\n",
    "\n",
    "        fig = go.Figure(data=go.Scatter(x=frame[:,0], y=2.5 - frame[:,1], mode='markers',\n",
    "                                        marker=dict(\n",
    "                                            size=sizes\n",
    "                                            )))\n",
    "\n",
    "    # Customize the layout\n",
    "    fig.update_layout(title='visualization of human keypoints',\n",
    "                        xaxis_title='',\n",
    "                        yaxis_title='',\n",
    "                        width=1000,\n",
    "                        height=1600)\n",
    "\n",
    "    fig.update_xaxes(range=[-0.2, 1.4])  # Set x-axis range from 0 to 6\n",
    "    fig.update_yaxes(range=[0, 2.5])  # Set y-axis range from 10 to 20\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def process_parquet(ds, idxes = None):\n",
    "    ret = []\n",
    "    frames_unique = sorted(np.unique(ds[\"frame\"]))\n",
    "    for i,frame in enumerate(frames_unique):\n",
    "        frame_ds = ds[ds['frame'] == frame]\n",
    "        \n",
    "        order = []\n",
    "        for el in frame_ds[\"row_id\"]:\n",
    "            _frame, part, keypoint = el.split(\"-\")\n",
    "            order.append(order_global[part] - int(keypoint))\n",
    "\n",
    "        order = np.array(order)\n",
    "        frame_ds.iloc[:, 1] = order\n",
    "        frame_ds = frame_ds.sort_values(by=\"row_id\", ascending=False)\n",
    "    \n",
    "        vals = np.array(frame_ds[[\"x\", \"y\", \"z\"]])\n",
    "        if idxes is not None:\n",
    "            vals = vals[idxes]\n",
    "    \n",
    "        vals = vals.flatten()\n",
    "\n",
    "        ret.append(vals)\n",
    "        \n",
    "    return np.array(ret)\n",
    "\n",
    "\n",
    "def process_parquet2(ds, idxes = None):\n",
    "    ret = []    \n",
    "    frame_size = 543\n",
    "    it = len(ds) // frame_size\n",
    "    assert it == len(ds) / frame_size\n",
    "    \n",
    "    for i in range(it):\n",
    "        vals = ds.iloc[ i * frame_size : (i + 1 ) * frame_size ]        \n",
    "        \n",
    "        if idxes is not None:          \n",
    "            vals = ds.iloc[idxes]\n",
    "                        \n",
    "        ret.append(np.array(vals[[\"x\",\"y\", \"z\"]]).flatten())\n",
    "        \n",
    "    return np.array(np.array(ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '79631423.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m f \u001b[38;5;241m=\u001b[39m process_parquet(\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m79631423.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      2\u001b[0m visualize_keypoints(f[\u001b[38;5;241m0\u001b[39m], point_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parquet.py:267\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    265\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    275\u001b[0m         path_or_handle,\n\u001b[1;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    280\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parquet.py:140\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    130\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '79631423.parquet'"
     ]
    }
   ],
   "source": [
    "f = process_parquet(pd.read_parquet(\"79631423.parquet\"))\n",
    "visualize_keypoints(f[0], point_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = process_parquet2(pd.read_parquet(\"79631423.parquet\"), idxes=all_selection)\n",
    "visualize_keypoints(f2[0], point_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/ASL-ds/train_landmark_files/16069/695046.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m..\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mASL-ds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_landmark_files\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m16069\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m695046.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parquet.py:267\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    265\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    275\u001b[0m         path_or_handle,\n\u001b[1;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    280\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parquet.py:140\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    130\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/ASL-ds/train_landmark_files/16069/695046.parquet'"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet( os.path.join(\"..\", \"data\",\"ASL-ds\", \"train_landmark_files\", \"16069\", \"695046.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94477/94477 [00:00<00:00, 315108.95it/s]\n",
      "100%|██████████| 94477/94477 [00:00<00:00, 315050.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cardinality of train : 113, cardinality of validation : 13\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#custom class to load data from Parquet files for training ML models.\n",
    "class ParquetDataset(keras.utils.Sequence):\n",
    "    def __init__(self, dataset_folder, csv_file : str, batch_size=CONFIG.BATCH_SIZE, \n",
    "                 data_limit :int= CONFIG.DATA_LIMIT, check_if_file_exists = True, \n",
    "                 preprocessing_func=None, frame_length :int = CONFIG.VIDEO_LENGTH,\n",
    "                 split : str = \"train\", train_val_split : float = CONFIG.TRAIN_VAL_SPLIT,\n",
    "                 sort_by_counts : bool = True, **kwargs\n",
    "                ):\n",
    "        super().__init__(**kwargs)\n",
    "        #taking keras sequence for .fit(), .evaluate(), .predict() methods\n",
    "        #load csv - it has the path to parquet file, and another to store label\n",
    "        self.csv_path = csv_file\n",
    "        self.root_folder = dataset_folder\n",
    "        self.batch_size = batch_size\n",
    "        #optional pre-processing function to the parquet files.\n",
    "        self.preprocessing_func = preprocessing_func\n",
    "        \n",
    "        self.csv_data = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        self.all_files = []\n",
    "        self.not_exists = []\n",
    "        self.frame_length = frame_length\n",
    "\n",
    "        \n",
    "        for path, label in tqdm(list(zip(self.csv_data[\"path\"], self.csv_data[\"sign\"]))):\n",
    "            prop_path = os.path.join(self.root_folder, path)\n",
    "            \n",
    "            if check_if_file_exists:\n",
    "                if os.path.exists(prop_path):\n",
    "                    self.all_files.append((prop_path, label))\n",
    "                else:\n",
    "                    self.not_exists.append(prop_path)\n",
    "            else:\n",
    "                self.all_files.append((prop_path, label))\n",
    "                \n",
    "                    \n",
    "        self.all_files = np.array(self.all_files)\n",
    "        self.unique_labels = np.unique(self.all_files[:, 1])\n",
    "        self.label_2_id = { key : i for i, key in enumerate(self.unique_labels)}\n",
    "    \n",
    "        # sort the values by popularity\n",
    "        if sort_by_counts:\n",
    "            cnt = Counter(self.all_files[:, 1])\n",
    "            vals = []\n",
    "            \n",
    "            for i,row in enumerate(self.all_files):\n",
    "                vals.append((int(1e6 * cnt[row[1]] + self.label_2_id [row[1]]),i))\n",
    "            \n",
    "            vals = np.array(sorted(vals)[::-1])\n",
    "            self.all_files = self.all_files[vals[:,1]]\n",
    "\n",
    "        \n",
    "        if data_limit < 0:\n",
    "            train_ds, val_ds = train_test_split(self.all_files, train_size=train_val_split, random_state=42)\n",
    "        else:\n",
    "            train_ds, val_ds = train_test_split(self.all_files[:data_limit], train_size=train_val_split, random_state=42)\n",
    "            self.unique_labels = np.unique(self.all_files[:data_limit, 1])\n",
    "            self.label_2_id = { key : i for i, key in enumerate(self.unique_labels)}\n",
    "            \n",
    "        if split.lower() == \"train\":\n",
    "            self.dataset = train_ds\n",
    "            \n",
    "        elif split.lower() == \"val\":\n",
    "            self.dataset = val_ds \n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"please specify split to be either train or val\")\n",
    "            \n",
    "        np.random.shuffle(self.dataset)\n",
    "                   \n",
    "\n",
    "    def __len__(self):\n",
    "        # Assuming each Parquet file should be one batch; adjust if necessary\n",
    "        return math.ceil(len(self.dataset) / self.batch_size)\n",
    "    \n",
    "    def get_single(self, idx):\n",
    "        # Load one file per batch\n",
    "        #take the idx value, 1st label, \n",
    "        path, label = self.dataset[idx]\n",
    "        \n",
    "        df = pd.read_parquet( path)\n",
    "        \n",
    "        # Apply preprocessing if specified\n",
    "        if self.preprocessing_func:\n",
    "            df = self.preprocessing_func(df, self.frame_length)\n",
    "        \n",
    "        one_hot_encoded_label = np.zeros(len(self.unique_labels))\n",
    "        one_hot_encoded_label[self.label_2_id[label]] = 1  \n",
    "        \n",
    "        return df, one_hot_encoded_label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X, Y = [], []\n",
    "        \n",
    "        low = idx * self.batch_size\n",
    "        high = min(low + self.batch_size, len(self.dataset))\n",
    "        \n",
    "        for i in range(low, high):\n",
    "            x, y = self.get_single(i)\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "        \n",
    "        return np.array(X), np.array(Y)\n",
    "                \n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        # Shuffle files for the next epoch\n",
    "        np.random.shuffle(self.dataset)\n",
    "\n",
    "def my_preprocessing_func(df, frame_length):\n",
    "    \n",
    "    # Define your preprocessing steps here\n",
    "    # Example: normalize numerical features\n",
    "    frames_mediapipe = process_parquet2(df, all_selection)\n",
    "    \n",
    "    current_length, num_features = frames_mediapipe.shape\n",
    "\n",
    "    if current_length >= frame_length:\n",
    "            # TODO: a better than uniform value ? Could place gaussian in the middle\n",
    "            random_start = random.randint(0, current_length - frame_length)\n",
    "            return np.nan_to_num(frames_mediapipe[random_start : (random_start + frame_length)])\n",
    "        \n",
    "    # padd the video to contain zeros \n",
    "    return np.concatenate([np.nan_to_num(frames_mediapipe), np.zeros((frame_length - current_length, num_features))], axis=0)\n",
    "    \n",
    "# Usage example\n",
    "parquet_folder_path = CONFIG.root\n",
    "train_dataset_parquet = ParquetDataset(parquet_folder_path, csv_file = os.path.join(CONFIG.root, \"train.csv\"), \n",
    "                                 batch_size=CONFIG.BATCH_SIZE, data_limit=1000,\n",
    "                                 preprocessing_func=my_preprocessing_func,\n",
    "                                check_if_file_exists = True,\n",
    "                                split=\"train\")\n",
    "\n",
    "val_dataset_parquet = ParquetDataset(parquet_folder_path, csv_file = os.path.join(CONFIG.root, \"train.csv\"), \n",
    "                                 batch_size=CONFIG.BATCH_SIZE, data_limit=1000,\n",
    "                                 preprocessing_func=my_preprocessing_func,\n",
    "                                 check_if_file_exists= True,\n",
    "                                 split=\"val\")\n",
    "\n",
    "print(f\"cardinality of train : {len(train_dataset_parquet)}, cardinality of validation : {len(val_dataset_parquet)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FEATURES = train_dataset_parquet[0][0].shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(train_dataset_parquet.dataset[:, 1], bins=len(train_dataset_parquet.unique_labels))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(val_dataset_parquet.dataset[:, 1], bins=len(val_dataset_parquet.unique_labels))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/113 [00:00<00:21,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 60, 396) (8, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:20<00:00,  5.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating through dataset took : 20.9304s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "isnans =False\n",
    "\n",
    "f = True\n",
    "labels_batches = []\n",
    "for el in tqdm(train_dataset_parquet):\n",
    "    if f:\n",
    "        print(el[0].shape, el[1].shape)\n",
    "        f = False\n",
    "    labels_batches.append(el[1])\n",
    "        \n",
    "    isnans |= np.any(np.isnan(el[0]))\n",
    "    if isnans:\n",
    "        print(\"FOUND NAN!\")\n",
    "        break\n",
    "\n",
    "\n",
    "print(f\"Iterating through dataset took : {round( time.time() - start , 4)}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, LeakyReLU\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras import backend as K\n",
    "from wandb.keras import WandbMetricsLogger\n",
    "from keras.callbacks import Callback\n",
    "import tensorflow as tf\n",
    "import wandb\n",
    "\n",
    "\n",
    "class CosineAnnealingLearningRateScheduler(Callback):\n",
    "    def __init__(self, max_lr, min_lr, T_max):\n",
    "        super(CosineAnnealingLearningRateScheduler, self).__init__()\n",
    "        self.max_lr = max_lr  # Maximum learning rate (i.e., start learning rate)\n",
    "        self.min_lr = min_lr  # Minimum learning rate\n",
    "        self.T_max = T_max    # Specifies the number of epochs per cycle\n",
    "        self.t = 0            # Current epoch\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.t += 1\n",
    "        cos = np.cos(np.pi * (self.t % self.T_max) / self.T_max)\n",
    "        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + cos)\n",
    "\n",
    "        keras.backend.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "def keras_train(model, filepath : str, max_lr = 1e-4, min_lr = 5e-5, T_max=50, epochs=100, run_name=\"\",\n",
    "                mediapipe_features = \"all\", USE_WANDB=True): \n",
    "    \n",
    "    \n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
    "                                                 monitor=\"val_categorical_accuracy\",\n",
    "                                                 verbose=0,\n",
    "                                                 save_best_only=True,\n",
    "                                                 mode=\"max\",\n",
    "                                                 save_freq=\"epoch\")\n",
    "    \n",
    "    cosine_annealer = CosineAnnealingLearningRateScheduler(max_lr=max_lr,\n",
    "                                                           min_lr=min_lr,\n",
    "                                                           T_max=T_max)\n",
    "    \n",
    "    #Adam Optimizer - fixed learning rate.\n",
    "    adam_optimizer = tf.keras.optimizers.Adam(learning_rate=max_lr, clipnorm=1.)\n",
    "\n",
    "    model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    \n",
    "    \n",
    "    callbacks  = [checkpoint, cosine_annealer]\n",
    "    \n",
    "    if USE_WANDB:\n",
    "        wandb.init(project=CONFIG.WANDB_RUN,\n",
    "                        name=run_name,\n",
    "                        notes=\"Model summary : \\n\" + str(model),\n",
    "                        config={\"max_lr\" : max_lr, \n",
    "                                \"min_lr\" : 5e-5, \n",
    "                                \"scheduler\" : \"cosineAnnealer\", \n",
    "                                \"epochs\" : epochs, \n",
    "                                \"T_max\" : T_max, \n",
    "                                \"train_size\" : len(train_dataset_parquet.dataset),\n",
    "                                \"val_size\" : len(val_dataset_parquet.dataset),\n",
    "                                \"unique_classes\" : len(train_dataset_parquet.unique_labels), \n",
    "                                \"video_length\" : CONFIG.VIDEO_LENGTH,\n",
    "                                \"features\" : mediapipe_features\n",
    "                                })\n",
    "        callbacks.append(WandbMetricsLogger())\n",
    "\n",
    "\n",
    "    history = model.fit(train_dataset_parquet, epochs=epochs, validation_data = val_dataset_parquet, \n",
    "                        batch_size = 8, callbacks=callbacks)\n",
    "    \n",
    "    if USE_WANDB:      \n",
    "        wandb.finish()\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:90nabvv3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM128-Dense128-Dense256-LipsEyesHandsPose</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/90nabvv3' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/90nabvv3</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240409_231739-90nabvv3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:90nabvv3). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc8d888372b24163812f09021887aea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112790865202745, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240409_231818-ob5aeisg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/ob5aeisg' target=\"_blank\">LSTM128-Dense128-Dense256-LipsEyesHandsPose</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/ob5aeisg' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/ob5aeisg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.1253 - categorical_accuracy: 0.4100INFO:tensorflow:Assets written to: models/LSTM1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 33s 263ms/step - loss: 1.1253 - categorical_accuracy: 0.4100 - val_loss: 1.0921 - val_categorical_accuracy: 0.4000\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0779 - categorical_accuracy: 0.4244INFO:tensorflow:Assets written to: models/LSTM1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 29s 258ms/step - loss: 1.0779 - categorical_accuracy: 0.4244 - val_loss: 1.0197 - val_categorical_accuracy: 0.4700\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - 23s 203ms/step - loss: 0.9808 - categorical_accuracy: 0.4444 - val_loss: 0.8970 - val_categorical_accuracy: 0.4700\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.9248 - categorical_accuracy: 0.5056INFO:tensorflow:Assets written to: models/LSTM1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 263ms/step - loss: 0.9248 - categorical_accuracy: 0.5056 - val_loss: 0.8610 - val_categorical_accuracy: 0.5500\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.9150 - categorical_accuracy: 0.5144INFO:tensorflow:Assets written to: models/LSTM1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 265ms/step - loss: 0.9150 - categorical_accuracy: 0.5144 - val_loss: 0.8333 - val_categorical_accuracy: 0.5600\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8939 - categorical_accuracy: 0.5267INFO:tensorflow:Assets written to: models/LSTM1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 29s 260ms/step - loss: 0.8939 - categorical_accuracy: 0.5267 - val_loss: 0.8182 - val_categorical_accuracy: 0.6200\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - 23s 199ms/step - loss: 0.8660 - categorical_accuracy: 0.5378 - val_loss: 0.7977 - val_categorical_accuracy: 0.5500\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - 23s 204ms/step - loss: 0.8511 - categorical_accuracy: 0.5489 - val_loss: 0.7955 - val_categorical_accuracy: 0.5900\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - 23s 201ms/step - loss: 0.8241 - categorical_accuracy: 0.5767 - val_loss: 0.7956 - val_categorical_accuracy: 0.6200\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8189 - categorical_accuracy: 0.5844INFO:tensorflow:Assets written to: models/LSTM1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 263ms/step - loss: 0.8189 - categorical_accuracy: 0.5844 - val_loss: 0.7627 - val_categorical_accuracy: 0.6400\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7835 - categorical_accuracy: 0.6244INFO:tensorflow:Assets written to: models/LSTM1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 29s 257ms/step - loss: 0.7835 - categorical_accuracy: 0.6244 - val_loss: 0.7299 - val_categorical_accuracy: 0.7000\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - 22s 197ms/step - loss: 0.7454 - categorical_accuracy: 0.6822 - val_loss: 0.7676 - val_categorical_accuracy: 0.6400\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - 23s 202ms/step - loss: 0.7114 - categorical_accuracy: 0.6978 - val_loss: 0.7432 - val_categorical_accuracy: 0.6800\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7309 - categorical_accuracy: 0.6856INFO:tensorflow:Assets written to: models/LSTM1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 263ms/step - loss: 0.7309 - categorical_accuracy: 0.6856 - val_loss: 0.7056 - val_categorical_accuracy: 0.7100\n",
      "Epoch 15/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6905 - categorical_accuracy: 0.7189INFO:tensorflow:Assets written to: models/LSTM1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 29s 258ms/step - loss: 0.6905 - categorical_accuracy: 0.7189 - val_loss: 0.6531 - val_categorical_accuracy: 0.7300\n",
      "Epoch 16/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6553 - categorical_accuracy: 0.7256INFO:tensorflow:Assets written to: models/LSTM1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 31s 277ms/step - loss: 0.6553 - categorical_accuracy: 0.7256 - val_loss: 0.6715 - val_categorical_accuracy: 0.7400\n",
      "Epoch 17/100\n",
      "113/113 [==============================] - 23s 202ms/step - loss: 0.6307 - categorical_accuracy: 0.7500 - val_loss: 0.6876 - val_categorical_accuracy: 0.6900\n",
      "Epoch 18/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6334 - categorical_accuracy: 0.7467INFO:tensorflow:Assets written to: models/LSTM1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 29s 255ms/step - loss: 0.6334 - categorical_accuracy: 0.7467 - val_loss: 0.6008 - val_categorical_accuracy: 0.8000\n",
      "Epoch 19/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6744 - categorical_accuracy: 0.7278INFO:tensorflow:Assets written to: models/LSTM1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 262ms/step - loss: 0.6744 - categorical_accuracy: 0.7278 - val_loss: 0.5728 - val_categorical_accuracy: 0.8200\n",
      "Epoch 20/100\n",
      "113/113 [==============================] - 23s 202ms/step - loss: 0.6020 - categorical_accuracy: 0.7689 - val_loss: 0.5708 - val_categorical_accuracy: 0.7600\n",
      "Epoch 21/100\n",
      "113/113 [==============================] - 23s 202ms/step - loss: 0.6196 - categorical_accuracy: 0.7600 - val_loss: 0.5965 - val_categorical_accuracy: 0.7700\n",
      "Epoch 22/100\n",
      "113/113 [==============================] - 23s 203ms/step - loss: 0.5918 - categorical_accuracy: 0.7800 - val_loss: 0.6273 - val_categorical_accuracy: 0.7700\n",
      "Epoch 23/100\n",
      "113/113 [==============================] - 23s 201ms/step - loss: 0.6016 - categorical_accuracy: 0.7733 - val_loss: 0.6371 - val_categorical_accuracy: 0.7800\n",
      "Epoch 24/100\n",
      "113/113 [==============================] - 23s 203ms/step - loss: 0.5681 - categorical_accuracy: 0.7922 - val_loss: 0.5793 - val_categorical_accuracy: 0.8000\n",
      "Epoch 25/100\n",
      "113/113 [==============================] - 23s 202ms/step - loss: 0.5668 - categorical_accuracy: 0.7922 - val_loss: 0.6808 - val_categorical_accuracy: 0.7300\n",
      "Epoch 26/100\n",
      "113/113 [==============================] - 23s 202ms/step - loss: 0.5521 - categorical_accuracy: 0.7922 - val_loss: 0.5637 - val_categorical_accuracy: 0.7900\n",
      "Epoch 27/100\n",
      "113/113 [==============================] - 23s 202ms/step - loss: 0.5843 - categorical_accuracy: 0.7856 - val_loss: 0.6052 - val_categorical_accuracy: 0.7900\n",
      "Epoch 28/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.5798 - categorical_accuracy: 0.7900 - val_loss: 0.6348 - val_categorical_accuracy: 0.7500\n",
      "Epoch 29/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.5412 - categorical_accuracy: 0.7978 - val_loss: 0.7006 - val_categorical_accuracy: 0.7600\n",
      "Epoch 30/100\n",
      "113/113 [==============================] - 24s 211ms/step - loss: 0.5696 - categorical_accuracy: 0.7978 - val_loss: 0.7559 - val_categorical_accuracy: 0.7200\n",
      "Epoch 31/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5542 - categorical_accuracy: 0.8011 - val_loss: 0.5662 - val_categorical_accuracy: 0.8200\n",
      "Epoch 32/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5526 - categorical_accuracy: 0.7967 - val_loss: 0.6024 - val_categorical_accuracy: 0.7900\n",
      "Epoch 33/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.5363 - categorical_accuracy: 0.8022 - val_loss: 0.5884 - val_categorical_accuracy: 0.7900\n",
      "Epoch 34/100\n",
      "113/113 [==============================] - 23s 204ms/step - loss: 0.5301 - categorical_accuracy: 0.7978 - val_loss: 0.5816 - val_categorical_accuracy: 0.8100\n",
      "Epoch 35/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5020 - categorical_accuracy: 0.8189 - val_loss: 0.5556 - val_categorical_accuracy: 0.8100\n",
      "Epoch 36/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5052 - categorical_accuracy: 0.8167 - val_loss: 0.5664 - val_categorical_accuracy: 0.8000\n",
      "Epoch 37/100\n",
      "113/113 [==============================] - 23s 203ms/step - loss: 0.5249 - categorical_accuracy: 0.8033 - val_loss: 0.6174 - val_categorical_accuracy: 0.7800\n",
      "Epoch 38/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5137 - categorical_accuracy: 0.8178 - val_loss: 0.5369 - val_categorical_accuracy: 0.8200\n",
      "Epoch 39/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.5411 - categorical_accuracy: 0.8067 - val_loss: 0.7681 - val_categorical_accuracy: 0.7300\n",
      "Epoch 40/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5121 - categorical_accuracy: 0.8211 - val_loss: 0.6216 - val_categorical_accuracy: 0.7700\n",
      "Epoch 41/100\n",
      "113/113 [==============================] - 23s 209ms/step - loss: 0.4993 - categorical_accuracy: 0.8256 - val_loss: 0.5943 - val_categorical_accuracy: 0.7900\n",
      "Epoch 42/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.4894 - categorical_accuracy: 0.8300 - val_loss: 0.5564 - val_categorical_accuracy: 0.8200\n",
      "Epoch 43/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.4845 - categorical_accuracy: 0.8311 - val_loss: 0.6612 - val_categorical_accuracy: 0.7300\n",
      "Epoch 44/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.4738 - categorical_accuracy: 0.8322 - val_loss: 0.5452 - val_categorical_accuracy: 0.8000\n",
      "Epoch 45/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.4799 - categorical_accuracy: 0.8244 - val_loss: 0.6381 - val_categorical_accuracy: 0.8000\n",
      "Epoch 46/100\n",
      "113/113 [==============================] - 24s 212ms/step - loss: 0.4527 - categorical_accuracy: 0.8433 - val_loss: 0.5959 - val_categorical_accuracy: 0.8100\n",
      "Epoch 47/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.4599 - categorical_accuracy: 0.8422 - val_loss: 0.6165 - val_categorical_accuracy: 0.8000\n",
      "Epoch 48/100\n",
      "113/113 [==============================] - 24s 211ms/step - loss: 0.4961 - categorical_accuracy: 0.8278 - val_loss: 0.5273 - val_categorical_accuracy: 0.8100\n",
      "Epoch 49/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.4984 - categorical_accuracy: 0.8233 - val_loss: 0.5579 - val_categorical_accuracy: 0.8000\n",
      "Epoch 50/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5061 - categorical_accuracy: 0.8111INFO:tensorflow:Assets written to: models/LSTM1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 268ms/step - loss: 0.5061 - categorical_accuracy: 0.8111 - val_loss: 0.5410 - val_categorical_accuracy: 0.8400\n",
      "Epoch 51/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.5446 - categorical_accuracy: 0.8022 - val_loss: 0.6589 - val_categorical_accuracy: 0.7500\n",
      "Epoch 52/100\n",
      "113/113 [==============================] - 23s 204ms/step - loss: 0.6166 - categorical_accuracy: 0.7833 - val_loss: 0.5433 - val_categorical_accuracy: 0.8000\n",
      "Epoch 53/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.5318 - categorical_accuracy: 0.8122 - val_loss: 0.5758 - val_categorical_accuracy: 0.8000\n",
      "Epoch 54/100\n",
      "113/113 [==============================] - 23s 204ms/step - loss: 0.5203 - categorical_accuracy: 0.8211 - val_loss: 0.7170 - val_categorical_accuracy: 0.7600\n",
      "Epoch 55/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5346 - categorical_accuracy: 0.8178 - val_loss: 0.5050 - val_categorical_accuracy: 0.8100\n",
      "Epoch 56/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5447 - categorical_accuracy: 0.8156 - val_loss: 0.5893 - val_categorical_accuracy: 0.8100\n",
      "Epoch 57/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.5169 - categorical_accuracy: 0.8211 - val_loss: 0.5757 - val_categorical_accuracy: 0.8100\n",
      "Epoch 58/100\n",
      "113/113 [==============================] - 23s 204ms/step - loss: 0.5323 - categorical_accuracy: 0.8200 - val_loss: 0.4929 - val_categorical_accuracy: 0.8400\n",
      "Epoch 59/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5058 - categorical_accuracy: 0.8300 - val_loss: 0.5498 - val_categorical_accuracy: 0.7800\n",
      "Epoch 60/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.5182 - categorical_accuracy: 0.8311 - val_loss: 0.5245 - val_categorical_accuracy: 0.8300\n",
      "Epoch 61/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5377 - categorical_accuracy: 0.8167 - val_loss: 0.5349 - val_categorical_accuracy: 0.8000\n",
      "Epoch 62/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.4924 - categorical_accuracy: 0.8256 - val_loss: 0.5328 - val_categorical_accuracy: 0.8100\n",
      "Epoch 63/100\n",
      "113/113 [==============================] - 23s 204ms/step - loss: 0.4725 - categorical_accuracy: 0.8278 - val_loss: 0.4930 - val_categorical_accuracy: 0.8300\n",
      "Epoch 64/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.4903 - categorical_accuracy: 0.8322 - val_loss: 0.6144 - val_categorical_accuracy: 0.7700\n",
      "Epoch 65/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.5641 - categorical_accuracy: 0.7944 - val_loss: 0.5834 - val_categorical_accuracy: 0.7800\n",
      "Epoch 66/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.5322 - categorical_accuracy: 0.8144 - val_loss: 0.5813 - val_categorical_accuracy: 0.7900\n",
      "Epoch 67/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.4552 - categorical_accuracy: 0.8478 - val_loss: 0.5303 - val_categorical_accuracy: 0.8200\n",
      "Epoch 68/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.4763 - categorical_accuracy: 0.8411 - val_loss: 0.6461 - val_categorical_accuracy: 0.8200\n",
      "Epoch 69/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4993 - categorical_accuracy: 0.8389INFO:tensorflow:Assets written to: models/LSTM1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 29s 260ms/step - loss: 0.4993 - categorical_accuracy: 0.8389 - val_loss: 0.5406 - val_categorical_accuracy: 0.8500\n",
      "Epoch 70/100\n",
      "113/113 [==============================] - 23s 204ms/step - loss: 0.4850 - categorical_accuracy: 0.8322 - val_loss: 0.5145 - val_categorical_accuracy: 0.8300\n",
      "Epoch 71/100\n",
      "113/113 [==============================] - 23s 203ms/step - loss: 0.4589 - categorical_accuracy: 0.8478 - val_loss: 0.5249 - val_categorical_accuracy: 0.8300\n",
      "Epoch 72/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.4838 - categorical_accuracy: 0.8289 - val_loss: 0.4846 - val_categorical_accuracy: 0.8500\n",
      "Epoch 73/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.4735 - categorical_accuracy: 0.8411 - val_loss: 0.6151 - val_categorical_accuracy: 0.7600\n",
      "Epoch 74/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.4451 - categorical_accuracy: 0.8533 - val_loss: 0.6607 - val_categorical_accuracy: 0.8200\n",
      "Epoch 75/100\n",
      "113/113 [==============================] - 23s 202ms/step - loss: 0.4700 - categorical_accuracy: 0.8456 - val_loss: 0.5045 - val_categorical_accuracy: 0.8500\n",
      "Epoch 76/100\n",
      "113/113 [==============================] - 23s 202ms/step - loss: 0.4738 - categorical_accuracy: 0.8544 - val_loss: 0.5483 - val_categorical_accuracy: 0.8000\n",
      "Epoch 77/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4259 - categorical_accuracy: 0.8611INFO:tensorflow:Assets written to: models/LSTM1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 264ms/step - loss: 0.4259 - categorical_accuracy: 0.8611 - val_loss: 0.4284 - val_categorical_accuracy: 0.8700\n",
      "Epoch 78/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.4545 - categorical_accuracy: 0.8422 - val_loss: 0.4475 - val_categorical_accuracy: 0.8300\n",
      "Epoch 79/100\n",
      "113/113 [==============================] - 23s 204ms/step - loss: 0.4674 - categorical_accuracy: 0.8400 - val_loss: 0.5831 - val_categorical_accuracy: 0.8200\n",
      "Epoch 80/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.4286 - categorical_accuracy: 0.8667 - val_loss: 0.5402 - val_categorical_accuracy: 0.8200\n",
      "Epoch 81/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4529 - categorical_accuracy: 0.8533 - val_loss: 0.4574 - val_categorical_accuracy: 0.8500\n",
      "Epoch 82/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.4160 - categorical_accuracy: 0.8667 - val_loss: 0.4793 - val_categorical_accuracy: 0.8500\n",
      "Epoch 83/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.4260 - categorical_accuracy: 0.8544 - val_loss: 0.5427 - val_categorical_accuracy: 0.8000\n",
      "Epoch 84/100\n",
      "113/113 [==============================] - 23s 209ms/step - loss: 0.4458 - categorical_accuracy: 0.8456 - val_loss: 0.5779 - val_categorical_accuracy: 0.8100\n",
      "Epoch 85/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.3868 - categorical_accuracy: 0.8722 - val_loss: 0.4888 - val_categorical_accuracy: 0.8000\n",
      "Epoch 86/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.4050 - categorical_accuracy: 0.8811 - val_loss: 0.4770 - val_categorical_accuracy: 0.8300\n",
      "Epoch 87/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.3840 - categorical_accuracy: 0.8756 - val_loss: 0.6059 - val_categorical_accuracy: 0.8100\n",
      "Epoch 88/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.3744 - categorical_accuracy: 0.8878 - val_loss: 0.4606 - val_categorical_accuracy: 0.8400\n",
      "Epoch 89/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.3753 - categorical_accuracy: 0.8867 - val_loss: 0.4211 - val_categorical_accuracy: 0.8500\n",
      "Epoch 90/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.3721 - categorical_accuracy: 0.8822 - val_loss: 0.5459 - val_categorical_accuracy: 0.8100\n",
      "Epoch 91/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.3614 - categorical_accuracy: 0.8900 - val_loss: 0.4641 - val_categorical_accuracy: 0.8300\n",
      "Epoch 92/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.3487 - categorical_accuracy: 0.8933 - val_loss: 0.5868 - val_categorical_accuracy: 0.8200\n",
      "Epoch 93/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.3799 - categorical_accuracy: 0.8822 - val_loss: 0.4154 - val_categorical_accuracy: 0.8600\n",
      "Epoch 94/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.3605 - categorical_accuracy: 0.8878 - val_loss: 0.4431 - val_categorical_accuracy: 0.8700\n",
      "Epoch 95/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.4000 - categorical_accuracy: 0.8789 - val_loss: 0.4929 - val_categorical_accuracy: 0.8500\n",
      "Epoch 96/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.3421 - categorical_accuracy: 0.8967 - val_loss: 0.4587 - val_categorical_accuracy: 0.8500\n",
      "Epoch 97/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.3674 - categorical_accuracy: 0.8856 - val_loss: 0.4777 - val_categorical_accuracy: 0.8600\n",
      "Epoch 98/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.3372 - categorical_accuracy: 0.8989 - val_loss: 0.4450 - val_categorical_accuracy: 0.8600\n",
      "Epoch 99/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.3802 - categorical_accuracy: 0.8867 - val_loss: 0.5679 - val_categorical_accuracy: 0.8200\n",
      "Epoch 100/100\n",
      "113/113 [==============================] - 23s 204ms/step - loss: 0.4509 - categorical_accuracy: 0.8544 - val_loss: 0.4861 - val_categorical_accuracy: 0.8100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁▁▃▃▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇█████▇</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁███▇▇▇▆▆▅▅▄▄▃▃▂▂▁▁▁█</td></tr><tr><td>epoch/loss</td><td>█▇▆▆▅▄▄▄▃▃▃▃▃▃▂▃▂▂▂▂▃▃▃▂▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁▂▄▄▅▅▆▇▇▇▇▆▇▇▇▆▇▇▇▇▆▆▇▇▇▇▇██▇█▇█▇▇█▇██▇</td></tr><tr><td>epoch/val_loss</td><td>█▆▅▅▄▄▄▃▃▃▂▃▃▃▃▅▃▂▃▂▃▄▃▂▂▃▂▂▂▃▁▃▂▃▃▁▃▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.85444</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>0.45092</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.81</td></tr><tr><td>epoch/val_loss</td><td>0.48608</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM128-Dense128-Dense256-LipsEyesHandsPose</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/ob5aeisg' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/ob5aeisg</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240409_231818-ob5aeisg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f7feb5ecad0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(128, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, 396),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"LSTM1.tf\"),\n",
    "            run_name=\"LSTM128-Dense128-Dense256-LipsEyesHandsPose\",\n",
    "           mediapipe_features=\"LipsEyesHandsPose\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 10:39:35.667665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22460 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:02:00.0, compute capability: 8.6\n",
      "2024-04-10 10:39:36.061004: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240410_103954-72ou0obc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/72ou0obc' target=\"_blank\">LSTM64-Dense128-Dense256-LipsEyesHandsPose</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/72ou0obc' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/72ou0obc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 10:39:58.529531: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8905\n",
      "2024-04-10 10:39:59.384809: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f8976b368c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-04-10 10:39:59.384836: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-04-10 10:39:59.392731: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-04-10 10:39:59.498867: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - ETA: 0s - loss: 1.1170 - categorical_accuracy: 0.3956INFO:tensorflow:Assets written to: models/LSTM64-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 34s 261ms/step - loss: 1.1170 - categorical_accuracy: 0.3956 - val_loss: 1.0925 - val_categorical_accuracy: 0.4500\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0836 - categorical_accuracy: 0.4178INFO:tensorflow:Assets written to: models/LSTM64-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 266ms/step - loss: 1.0836 - categorical_accuracy: 0.4178 - val_loss: 1.0480 - val_categorical_accuracy: 0.4600\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - 23s 202ms/step - loss: 1.0436 - categorical_accuracy: 0.4322 - val_loss: 0.9868 - val_categorical_accuracy: 0.4500\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.9560 - categorical_accuracy: 0.4767INFO:tensorflow:Assets written to: models/LSTM64-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 29s 261ms/step - loss: 0.9560 - categorical_accuracy: 0.4767 - val_loss: 0.8603 - val_categorical_accuracy: 0.5500\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - 23s 203ms/step - loss: 0.8873 - categorical_accuracy: 0.5356 - val_loss: 0.8334 - val_categorical_accuracy: 0.4800\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8501 - categorical_accuracy: 0.5644INFO:tensorflow:Assets written to: models/LSTM64-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 264ms/step - loss: 0.8501 - categorical_accuracy: 0.5644 - val_loss: 0.8121 - val_categorical_accuracy: 0.5900\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8423 - categorical_accuracy: 0.5811INFO:tensorflow:Assets written to: models/LSTM64-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 29s 261ms/step - loss: 0.8423 - categorical_accuracy: 0.5811 - val_loss: 0.7861 - val_categorical_accuracy: 0.6200\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8020 - categorical_accuracy: 0.6244INFO:tensorflow:Assets written to: models/LSTM64-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 263ms/step - loss: 0.8020 - categorical_accuracy: 0.6244 - val_loss: 0.7773 - val_categorical_accuracy: 0.6700\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - 23s 203ms/step - loss: 0.7593 - categorical_accuracy: 0.6578 - val_loss: 0.7573 - val_categorical_accuracy: 0.6600\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7487 - categorical_accuracy: 0.6756INFO:tensorflow:Assets written to: models/LSTM64-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 263ms/step - loss: 0.7487 - categorical_accuracy: 0.6756 - val_loss: 0.7168 - val_categorical_accuracy: 0.6900\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7130 - categorical_accuracy: 0.6944INFO:tensorflow:Assets written to: models/LSTM64-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 29s 260ms/step - loss: 0.7130 - categorical_accuracy: 0.6944 - val_loss: 0.6398 - val_categorical_accuracy: 0.7400\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - 23s 204ms/step - loss: 0.6564 - categorical_accuracy: 0.7289 - val_loss: 0.6527 - val_categorical_accuracy: 0.7300\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - 23s 203ms/step - loss: 0.6423 - categorical_accuracy: 0.7411 - val_loss: 0.6729 - val_categorical_accuracy: 0.6900\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6250 - categorical_accuracy: 0.7367INFO:tensorflow:Assets written to: models/LSTM64-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 263ms/step - loss: 0.6250 - categorical_accuracy: 0.7367 - val_loss: 0.5871 - val_categorical_accuracy: 0.7800\n",
      "Epoch 15/100\n",
      "113/113 [==============================] - 23s 203ms/step - loss: 0.6093 - categorical_accuracy: 0.7622 - val_loss: 0.6789 - val_categorical_accuracy: 0.7400\n",
      "Epoch 16/100\n",
      "113/113 [==============================] - 23s 202ms/step - loss: 0.5889 - categorical_accuracy: 0.7656 - val_loss: 0.7220 - val_categorical_accuracy: 0.7100\n",
      "Epoch 17/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.5722 - categorical_accuracy: 0.7678 - val_loss: 0.5783 - val_categorical_accuracy: 0.7800\n",
      "Epoch 18/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6130 - categorical_accuracy: 0.7589INFO:tensorflow:Assets written to: models/LSTM64-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 270ms/step - loss: 0.6130 - categorical_accuracy: 0.7589 - val_loss: 0.5475 - val_categorical_accuracy: 0.8100\n",
      "Epoch 19/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.5721 - categorical_accuracy: 0.7811 - val_loss: 0.6722 - val_categorical_accuracy: 0.7000\n",
      "Epoch 20/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5763 - categorical_accuracy: 0.7867 - val_loss: 0.5638 - val_categorical_accuracy: 0.7800\n",
      "Epoch 21/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5498 - categorical_accuracy: 0.7900INFO:tensorflow:Assets written to: models/LSTM64-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 264ms/step - loss: 0.5498 - categorical_accuracy: 0.7900 - val_loss: 0.5427 - val_categorical_accuracy: 0.8200\n",
      "Epoch 22/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.5453 - categorical_accuracy: 0.7878 - val_loss: 0.5573 - val_categorical_accuracy: 0.7900\n",
      "Epoch 23/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5563 - categorical_accuracy: 0.7867 - val_loss: 0.5702 - val_categorical_accuracy: 0.7800\n",
      "Epoch 24/100\n",
      "113/113 [==============================] - 24s 211ms/step - loss: 0.5426 - categorical_accuracy: 0.7844 - val_loss: 0.5465 - val_categorical_accuracy: 0.8000\n",
      "Epoch 25/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5319 - categorical_accuracy: 0.8044 - val_loss: 0.5539 - val_categorical_accuracy: 0.7900\n",
      "Epoch 26/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.5468 - categorical_accuracy: 0.8000 - val_loss: 0.7497 - val_categorical_accuracy: 0.7200\n",
      "Epoch 27/100\n",
      "101/113 [=========================>....] - ETA: 2s - loss: 0.5377 - categorical_accuracy: 0.8035"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5054 - categorical_accuracy: 0.8211 - val_loss: 0.4898 - val_categorical_accuracy: 0.8100\n",
      "Epoch 68/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.4384 - categorical_accuracy: 0.8633 - val_loss: 0.5279 - val_categorical_accuracy: 0.8400\n",
      "Epoch 69/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.4324 - categorical_accuracy: 0.8600 - val_loss: 0.5838 - val_categorical_accuracy: 0.8000\n",
      "Epoch 70/100\n",
      "113/113 [==============================] - 23s 204ms/step - loss: 0.4844 - categorical_accuracy: 0.8411 - val_loss: 0.4868 - val_categorical_accuracy: 0.8500\n",
      "Epoch 71/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.4487 - categorical_accuracy: 0.8444 - val_loss: 0.5062 - val_categorical_accuracy: 0.8300\n",
      "Epoch 72/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.4267 - categorical_accuracy: 0.8500 - val_loss: 0.4849 - val_categorical_accuracy: 0.8400\n",
      "Epoch 73/100\n",
      "113/113 [==============================] - 24s 212ms/step - loss: 0.4216 - categorical_accuracy: 0.8600 - val_loss: 0.5590 - val_categorical_accuracy: 0.8300\n",
      "Epoch 74/100\n",
      "113/113 [==============================] - 25s 218ms/step - loss: 0.4224 - categorical_accuracy: 0.8611 - val_loss: 0.5921 - val_categorical_accuracy: 0.8100\n",
      "Epoch 75/100\n",
      "113/113 [==============================] - 23s 204ms/step - loss: 0.4029 - categorical_accuracy: 0.8656 - val_loss: 0.4884 - val_categorical_accuracy: 0.8500\n",
      "Epoch 76/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.4016 - categorical_accuracy: 0.8656 - val_loss: 0.4420 - val_categorical_accuracy: 0.8600\n",
      "Epoch 77/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.4556 - categorical_accuracy: 0.8589 - val_loss: 0.6549 - val_categorical_accuracy: 0.7500\n",
      "Epoch 78/100\n",
      "113/113 [==============================] - 24s 207ms/step - loss: 0.3823 - categorical_accuracy: 0.8778 - val_loss: 0.5300 - val_categorical_accuracy: 0.8500\n",
      "Epoch 79/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4168 - categorical_accuracy: 0.8689 - val_loss: 0.5258 - val_categorical_accuracy: 0.8400\n",
      "Epoch 80/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.3844 - categorical_accuracy: 0.8767 - val_loss: 0.4207 - val_categorical_accuracy: 0.8400\n",
      "Epoch 81/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.3865 - categorical_accuracy: 0.8733 - val_loss: 0.5077 - val_categorical_accuracy: 0.8600\n",
      "Epoch 82/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.3660 - categorical_accuracy: 0.8811 - val_loss: 0.4721 - val_categorical_accuracy: 0.8400\n",
      "Epoch 83/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.4138 - categorical_accuracy: 0.8667 - val_loss: 0.5201 - val_categorical_accuracy: 0.8300\n",
      "Epoch 84/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.3788 - categorical_accuracy: 0.8844 - val_loss: 0.4979 - val_categorical_accuracy: 0.8500\n",
      "Epoch 85/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.3734 - categorical_accuracy: 0.8833 - val_loss: 0.5339 - val_categorical_accuracy: 0.8200\n",
      "Epoch 86/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.3652 - categorical_accuracy: 0.8856 - val_loss: 0.5249 - val_categorical_accuracy: 0.8400\n",
      "Epoch 87/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.3387 - categorical_accuracy: 0.9033 - val_loss: 0.5063 - val_categorical_accuracy: 0.8300\n",
      "Epoch 88/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.3483 - categorical_accuracy: 0.8933 - val_loss: 0.6249 - val_categorical_accuracy: 0.7900\n",
      "Epoch 89/100\n",
      "113/113 [==============================] - 23s 202ms/step - loss: 0.3634 - categorical_accuracy: 0.8822 - val_loss: 0.5238 - val_categorical_accuracy: 0.8300\n",
      "Epoch 90/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.3587 - categorical_accuracy: 0.8844 - val_loss: 0.4499 - val_categorical_accuracy: 0.8600\n",
      "Epoch 91/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.3674 - categorical_accuracy: 0.8889 - val_loss: 0.6509 - val_categorical_accuracy: 0.8200\n",
      "Epoch 92/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.3296 - categorical_accuracy: 0.8978INFO:tensorflow:Assets written to: models/LSTM64-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 267ms/step - loss: 0.3296 - categorical_accuracy: 0.8978 - val_loss: 0.4360 - val_categorical_accuracy: 0.8700\n",
      "Epoch 93/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.3600 - categorical_accuracy: 0.8856 - val_loss: 0.4780 - val_categorical_accuracy: 0.8400\n",
      "Epoch 94/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.3618 - categorical_accuracy: 0.8889 - val_loss: 0.4979 - val_categorical_accuracy: 0.8200\n",
      "Epoch 95/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.3112 - categorical_accuracy: 0.9044 - val_loss: 0.4479 - val_categorical_accuracy: 0.8500\n",
      "Epoch 96/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.3377 - categorical_accuracy: 0.8933 - val_loss: 0.4466 - val_categorical_accuracy: 0.8500\n",
      "Epoch 97/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.3372 - categorical_accuracy: 0.9011 - val_loss: 0.4639 - val_categorical_accuracy: 0.8500\n",
      "Epoch 98/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.3105 - categorical_accuracy: 0.9078 - val_loss: 0.6050 - val_categorical_accuracy: 0.7900\n",
      "Epoch 99/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.3356 - categorical_accuracy: 0.9011 - val_loss: 0.6840 - val_categorical_accuracy: 0.8200\n",
      "Epoch 100/100\n",
      "113/113 [==============================] - 23s 204ms/step - loss: 0.4616 - categorical_accuracy: 0.8578 - val_loss: 0.6631 - val_categorical_accuracy: 0.7800\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁▂▃▄▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇████████▇</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁███▇▇▇▆▆▅▅▄▄▃▃▂▂▁▁▁█</td></tr><tr><td>epoch/loss</td><td>█▇▆▅▄▄▃▄▃▃▃▃▃▂▂▃▂▂▂▂▃▃▂▂▃▂▃▂▂▂▂▂▁▁▁▁▁▁▁▂</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁▁▃▅▆▅▅▇▇▆▆▇▇▇▆▇▇▇▇▇▆█▆▇▇▅▇▇▇▇▆▇▇█▇▇█▇█▆</td></tr><tr><td>epoch/val_loss</td><td>█▇▅▅▃▄▄▂▂▂▄▂▂▂▄▂▁▂▂▂▃▁▃▃▂▂▂▃▂▃▃▂▁▂▂▂▁▂▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.85778</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>0.46162</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.78</td></tr><tr><td>epoch/val_loss</td><td>0.66313</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM64-Dense128-Dense256-LipsEyesHandsPose</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/72ou0obc' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/72ou0obc</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240410_103954-72ou0obc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f8d61148ad0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(64, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, NUM_FEATURES),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", f\"LSTM64-Dense128-Dense256-LipsEyesHandsPose_NF={NUM_FEATURES}.tf\"),\n",
    "            run_name=\"LSTM64-Dense128-Dense256-LipsEyesHandsPose\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmlewand\u001b[0m (\u001b[33mmlewand7\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240410_115000-j3791ixx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/j3791ixx' target=\"_blank\">LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/j3791ixx' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/j3791ixx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.1869 - categorical_accuracy: 0.3956INFO:tensorflow:Assets written to: models/LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 34s 272ms/step - loss: 1.1869 - categorical_accuracy: 0.3956 - val_loss: 1.1402 - val_categorical_accuracy: 0.4300\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.1127 - categorical_accuracy: 0.4100INFO:tensorflow:Assets written to: models/LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 265ms/step - loss: 1.1127 - categorical_accuracy: 0.4100 - val_loss: 1.0688 - val_categorical_accuracy: 0.4400\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - 24s 207ms/step - loss: 1.0709 - categorical_accuracy: 0.4333 - val_loss: 1.0520 - val_categorical_accuracy: 0.4300\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0512 - categorical_accuracy: 0.4322INFO:tensorflow:Assets written to: models/LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 270ms/step - loss: 1.0512 - categorical_accuracy: 0.4322 - val_loss: 1.0154 - val_categorical_accuracy: 0.4600\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 1.0434 - categorical_accuracy: 0.4478 - val_loss: 1.0200 - val_categorical_accuracy: 0.4500\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0325 - categorical_accuracy: 0.4333INFO:tensorflow:Assets written to: models/LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 267ms/step - loss: 1.0325 - categorical_accuracy: 0.4333 - val_loss: 0.9863 - val_categorical_accuracy: 0.4900\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 1.0400 - categorical_accuracy: 0.4222 - val_loss: 1.0153 - val_categorical_accuracy: 0.4400\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 1.0142 - categorical_accuracy: 0.4344 - val_loss: 1.0020 - val_categorical_accuracy: 0.4500\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.9932 - categorical_accuracy: 0.4856INFO:tensorflow:Assets written to: models/LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 268ms/step - loss: 0.9932 - categorical_accuracy: 0.4856 - val_loss: 0.9025 - val_categorical_accuracy: 0.5100\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.9267 - categorical_accuracy: 0.5600INFO:tensorflow:Assets written to: models/LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 269ms/step - loss: 0.9267 - categorical_accuracy: 0.5600 - val_loss: 0.8970 - val_categorical_accuracy: 0.5500\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.9548 - categorical_accuracy: 0.5056 - val_loss: 0.9071 - val_categorical_accuracy: 0.4500\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.9099 - categorical_accuracy: 0.4933 - val_loss: 0.8647 - val_categorical_accuracy: 0.4900\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.8789 - categorical_accuracy: 0.5611 - val_loss: 0.8528 - val_categorical_accuracy: 0.5200\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8550 - categorical_accuracy: 0.5867INFO:tensorflow:Assets written to: models/LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 265ms/step - loss: 0.8550 - categorical_accuracy: 0.5867 - val_loss: 0.8574 - val_categorical_accuracy: 0.5800\n",
      "Epoch 15/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.8528 - categorical_accuracy: 0.5867 - val_loss: 0.9220 - val_categorical_accuracy: 0.4800\n",
      "Epoch 16/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.8304 - categorical_accuracy: 0.5522 - val_loss: 0.8337 - val_categorical_accuracy: 0.4900\n",
      "Epoch 17/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7658 - categorical_accuracy: 0.6233INFO:tensorflow:Assets written to: models/LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 31s 271ms/step - loss: 0.7658 - categorical_accuracy: 0.6233 - val_loss: 0.7962 - val_categorical_accuracy: 0.6000\n",
      "Epoch 18/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.7452 - categorical_accuracy: 0.6533 - val_loss: 0.7421 - val_categorical_accuracy: 0.6000\n",
      "Epoch 19/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7018 - categorical_accuracy: 0.6978INFO:tensorflow:Assets written to: models/LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 264ms/step - loss: 0.7018 - categorical_accuracy: 0.6978 - val_loss: 0.7030 - val_categorical_accuracy: 0.6900\n",
      "Epoch 20/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.7045 - categorical_accuracy: 0.7100 - val_loss: 0.8114 - val_categorical_accuracy: 0.6400\n",
      "Epoch 21/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6841 - categorical_accuracy: 0.7089INFO:tensorflow:Assets written to: models/LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 267ms/step - loss: 0.6841 - categorical_accuracy: 0.7089 - val_loss: 0.6667 - val_categorical_accuracy: 0.7000\n",
      "Epoch 22/100\n",
      "113/113 [==============================] - 23s 204ms/step - loss: 0.6844 - categorical_accuracy: 0.7189 - val_loss: 0.7273 - val_categorical_accuracy: 0.6700\n",
      "Epoch 23/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.6322 - categorical_accuracy: 0.7233 - val_loss: 0.6674 - val_categorical_accuracy: 0.6900\n",
      "Epoch 24/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6281 - categorical_accuracy: 0.7444INFO:tensorflow:Assets written to: models/LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 270ms/step - loss: 0.6281 - categorical_accuracy: 0.7444 - val_loss: 0.6955 - val_categorical_accuracy: 0.7100\n",
      "Epoch 25/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.7088 - categorical_accuracy: 0.7189 - val_loss: 0.6787 - val_categorical_accuracy: 0.7100\n",
      "Epoch 26/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.6413 - categorical_accuracy: 0.7489 - val_loss: 0.6883 - val_categorical_accuracy: 0.7100\n",
      "Epoch 27/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.6204 - categorical_accuracy: 0.7544 - val_loss: 0.6821 - val_categorical_accuracy: 0.7000\n",
      "Epoch 28/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5702 - categorical_accuracy: 0.7756INFO:tensorflow:Assets written to: models/LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 29s 261ms/step - loss: 0.5702 - categorical_accuracy: 0.7756 - val_loss: 0.7232 - val_categorical_accuracy: 0.7200\n",
      "Epoch 29/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5969 - categorical_accuracy: 0.7678 - val_loss: 0.7307 - val_categorical_accuracy: 0.6500\n",
      "Epoch 30/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6020 - categorical_accuracy: 0.7700INFO:tensorflow:Assets written to: models/LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 269ms/step - loss: 0.6020 - categorical_accuracy: 0.7700 - val_loss: 0.5875 - val_categorical_accuracy: 0.7500\n",
      "Epoch 31/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5854 - categorical_accuracy: 0.7656INFO:tensorflow:Assets written to: models/LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 266ms/step - loss: 0.5854 - categorical_accuracy: 0.7656 - val_loss: 0.5601 - val_categorical_accuracy: 0.8000\n",
      "Epoch 32/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.5783 - categorical_accuracy: 0.7700 - val_loss: 0.6957 - val_categorical_accuracy: 0.7100\n",
      "Epoch 33/100\n",
      "113/113 [==============================] - 24s 206ms/step - loss: 0.5497 - categorical_accuracy: 0.7878 - val_loss: 0.5967 - val_categorical_accuracy: 0.7800\n",
      "Epoch 34/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5800 - categorical_accuracy: 0.7856 - val_loss: 0.5945 - val_categorical_accuracy: 0.7700\n",
      "Epoch 35/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.5670 - categorical_accuracy: 0.7844 - val_loss: 0.5650 - val_categorical_accuracy: 0.8000\n",
      "Epoch 36/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.5591 - categorical_accuracy: 0.7822 - val_loss: 0.6445 - val_categorical_accuracy: 0.7600\n",
      "Epoch 37/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5750 - categorical_accuracy: 0.7833 - val_loss: 0.5972 - val_categorical_accuracy: 0.7800\n",
      "Epoch 38/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5435 - categorical_accuracy: 0.8000 - val_loss: 0.5861 - val_categorical_accuracy: 0.7700\n",
      "Epoch 39/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5367 - categorical_accuracy: 0.8011INFO:tensorflow:Assets written to: models/LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 269ms/step - loss: 0.5367 - categorical_accuracy: 0.8011 - val_loss: 0.5551 - val_categorical_accuracy: 0.8200\n",
      "Epoch 40/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.5486 - categorical_accuracy: 0.7944 - val_loss: 0.6865 - val_categorical_accuracy: 0.7400\n",
      "Epoch 41/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5672 - categorical_accuracy: 0.7889 - val_loss: 0.5556 - val_categorical_accuracy: 0.7800\n",
      "Epoch 42/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.5196 - categorical_accuracy: 0.8167 - val_loss: 0.5586 - val_categorical_accuracy: 0.7900\n",
      "Epoch 43/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5195 - categorical_accuracy: 0.8078 - val_loss: 0.6287 - val_categorical_accuracy: 0.8000\n",
      "Epoch 44/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.5111 - categorical_accuracy: 0.8200 - val_loss: 0.5676 - val_categorical_accuracy: 0.8000\n",
      "Epoch 45/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5315 - categorical_accuracy: 0.8056 - val_loss: 0.5899 - val_categorical_accuracy: 0.7900\n",
      "Epoch 46/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.5081 - categorical_accuracy: 0.8189 - val_loss: 0.5582 - val_categorical_accuracy: 0.7800\n",
      "Epoch 47/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4989 - categorical_accuracy: 0.8156 - val_loss: 0.5948 - val_categorical_accuracy: 0.8000\n",
      "Epoch 48/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.5099 - categorical_accuracy: 0.8089 - val_loss: 0.7389 - val_categorical_accuracy: 0.7400\n",
      "Epoch 49/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.5011 - categorical_accuracy: 0.8156 - val_loss: 0.6194 - val_categorical_accuracy: 0.7900\n",
      "Epoch 50/100\n",
      "113/113 [==============================] - 24s 211ms/step - loss: 0.5864 - categorical_accuracy: 0.7889 - val_loss: 0.5976 - val_categorical_accuracy: 0.7900\n",
      "Epoch 51/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.6224 - categorical_accuracy: 0.7644 - val_loss: 0.5543 - val_categorical_accuracy: 0.8100\n",
      "Epoch 52/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.6587 - categorical_accuracy: 0.7622 - val_loss: 0.6844 - val_categorical_accuracy: 0.7400\n",
      "Epoch 53/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.6300 - categorical_accuracy: 0.7656 - val_loss: 0.6717 - val_categorical_accuracy: 0.7600\n",
      "Epoch 54/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.6994 - categorical_accuracy: 0.7133 - val_loss: 0.6124 - val_categorical_accuracy: 0.7500\n",
      "Epoch 55/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.6172 - categorical_accuracy: 0.7656 - val_loss: 0.5869 - val_categorical_accuracy: 0.7900\n",
      "Epoch 56/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.5830 - categorical_accuracy: 0.7878 - val_loss: 0.5292 - val_categorical_accuracy: 0.7800\n",
      "Epoch 57/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.5481 - categorical_accuracy: 0.8000 - val_loss: 0.5957 - val_categorical_accuracy: 0.8000\n",
      "Epoch 58/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5872 - categorical_accuracy: 0.7811INFO:tensorflow:Assets written to: models/LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 269ms/step - loss: 0.5872 - categorical_accuracy: 0.7811 - val_loss: 0.5442 - val_categorical_accuracy: 0.8400\n",
      "Epoch 59/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5922 - categorical_accuracy: 0.7844 - val_loss: 0.5134 - val_categorical_accuracy: 0.8300\n",
      "Epoch 60/100\n",
      "113/113 [==============================] - 23s 204ms/step - loss: 0.6008 - categorical_accuracy: 0.7756 - val_loss: 0.6457 - val_categorical_accuracy: 0.7400\n",
      "Epoch 61/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.5453 - categorical_accuracy: 0.7989 - val_loss: 0.6396 - val_categorical_accuracy: 0.7800\n",
      "Epoch 62/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.5864 - categorical_accuracy: 0.7822 - val_loss: 0.5257 - val_categorical_accuracy: 0.8000\n",
      "Epoch 63/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.5468 - categorical_accuracy: 0.8000 - val_loss: 0.5419 - val_categorical_accuracy: 0.8200\n",
      "Epoch 64/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5919 - categorical_accuracy: 0.7911 - val_loss: 0.5536 - val_categorical_accuracy: 0.8000\n",
      "Epoch 65/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5559 - categorical_accuracy: 0.7844 - val_loss: 0.6010 - val_categorical_accuracy: 0.7600\n",
      "Epoch 66/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.5431 - categorical_accuracy: 0.8000 - val_loss: 0.5578 - val_categorical_accuracy: 0.7900\n",
      "Epoch 67/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.5582 - categorical_accuracy: 0.7933 - val_loss: 0.5931 - val_categorical_accuracy: 0.8200\n",
      "Epoch 68/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5622 - categorical_accuracy: 0.7867 - val_loss: 0.5555 - val_categorical_accuracy: 0.8200\n",
      "Epoch 69/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.5491 - categorical_accuracy: 0.8011 - val_loss: 0.5069 - val_categorical_accuracy: 0.8100\n",
      "Epoch 70/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5223 - categorical_accuracy: 0.8089 - val_loss: 0.6064 - val_categorical_accuracy: 0.7300\n",
      "Epoch 71/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5258 - categorical_accuracy: 0.8033INFO:tensorflow:Assets written to: models/LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 266ms/step - loss: 0.5258 - categorical_accuracy: 0.8033 - val_loss: 0.4595 - val_categorical_accuracy: 0.8600\n",
      "Epoch 72/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5219 - categorical_accuracy: 0.8111 - val_loss: 0.5354 - val_categorical_accuracy: 0.7900\n",
      "Epoch 73/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.5051 - categorical_accuracy: 0.8178 - val_loss: 0.5213 - val_categorical_accuracy: 0.8000\n",
      "Epoch 74/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5070 - categorical_accuracy: 0.8144 - val_loss: 0.6066 - val_categorical_accuracy: 0.8000\n",
      "Epoch 75/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.4866 - categorical_accuracy: 0.8267 - val_loss: 0.5087 - val_categorical_accuracy: 0.8200\n",
      "Epoch 76/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5084 - categorical_accuracy: 0.8178 - val_loss: 0.6326 - val_categorical_accuracy: 0.7700\n",
      "Epoch 77/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.4878 - categorical_accuracy: 0.8311 - val_loss: 0.5336 - val_categorical_accuracy: 0.8300\n",
      "Epoch 78/100\n",
      "113/113 [==============================] - 24s 211ms/step - loss: 0.5130 - categorical_accuracy: 0.8200 - val_loss: 0.6121 - val_categorical_accuracy: 0.7600\n",
      "Epoch 79/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.5447 - categorical_accuracy: 0.8100 - val_loss: 0.5062 - val_categorical_accuracy: 0.8500\n",
      "Epoch 80/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.5070 - categorical_accuracy: 0.8311 - val_loss: 0.5824 - val_categorical_accuracy: 0.7800\n",
      "Epoch 81/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.5277 - categorical_accuracy: 0.8133 - val_loss: 0.4594 - val_categorical_accuracy: 0.8400\n",
      "Epoch 82/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4872 - categorical_accuracy: 0.8311 - val_loss: 0.4699 - val_categorical_accuracy: 0.8600\n",
      "Epoch 83/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.4762 - categorical_accuracy: 0.8400 - val_loss: 0.4690 - val_categorical_accuracy: 0.8100\n",
      "Epoch 84/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.4278 - categorical_accuracy: 0.8522 - val_loss: 0.4876 - val_categorical_accuracy: 0.8200\n",
      "Epoch 85/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.4266 - categorical_accuracy: 0.8478 - val_loss: 0.4516 - val_categorical_accuracy: 0.8300\n",
      "Epoch 86/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.4342 - categorical_accuracy: 0.8522 - val_loss: 0.5606 - val_categorical_accuracy: 0.7800\n",
      "Epoch 87/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.4513 - categorical_accuracy: 0.8478 - val_loss: 0.4595 - val_categorical_accuracy: 0.8300\n",
      "Epoch 88/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4095 - categorical_accuracy: 0.8667 - val_loss: 0.4522 - val_categorical_accuracy: 0.8500\n",
      "Epoch 89/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.4336 - categorical_accuracy: 0.8456 - val_loss: 0.4929 - val_categorical_accuracy: 0.8300\n",
      "Epoch 90/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.4338 - categorical_accuracy: 0.8544 - val_loss: 0.5659 - val_categorical_accuracy: 0.8200\n",
      "Epoch 91/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.4252 - categorical_accuracy: 0.8556 - val_loss: 0.5654 - val_categorical_accuracy: 0.8100\n",
      "Epoch 92/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.4319 - categorical_accuracy: 0.8533 - val_loss: 0.4963 - val_categorical_accuracy: 0.8300\n",
      "Epoch 93/100\n",
      "113/113 [==============================] - 24s 207ms/step - loss: 0.4531 - categorical_accuracy: 0.8489 - val_loss: 0.5238 - val_categorical_accuracy: 0.8000\n",
      "Epoch 94/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.4103 - categorical_accuracy: 0.8667 - val_loss: 0.5393 - val_categorical_accuracy: 0.8400\n",
      "Epoch 95/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.4344 - categorical_accuracy: 0.8644 - val_loss: 0.5527 - val_categorical_accuracy: 0.8100\n",
      "Epoch 96/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.4172 - categorical_accuracy: 0.8600 - val_loss: 0.4627 - val_categorical_accuracy: 0.8400\n",
      "Epoch 97/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.4293 - categorical_accuracy: 0.8567 - val_loss: 0.5124 - val_categorical_accuracy: 0.8300\n",
      "Epoch 98/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.4176 - categorical_accuracy: 0.8667 - val_loss: 0.3979 - val_categorical_accuracy: 0.8600\n",
      "Epoch 99/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.4197 - categorical_accuracy: 0.8600 - val_loss: 0.5038 - val_categorical_accuracy: 0.8500\n",
      "Epoch 100/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.5415 - categorical_accuracy: 0.8111 - val_loss: 0.5603 - val_categorical_accuracy: 0.8100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁▂▂▂▃▃▃▅▆▆▆▇▆▇▇▇▇▇▇▇▆▆▇▇▇▇▇▇▇▇▇▇▇██████▇</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁███▇▇▇▆▆▅▅▄▄▃▃▂▂▁▁▁█</td></tr><tr><td>epoch/loss</td><td>█▇▇▆▆▅▅▄▃▃▃▂▃▃▂▂▂▂▂▂▃▄▃▃▂▃▂▂▂▂▂▂▂▁▁▁▁▁▁▂</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁▁▂▁▁▂▂▄▅▅▆▆▇▇▆▇▇▇▇▇▇▆▇█▇▇▇▇▇▇███▇█████▇</td></tr><tr><td>epoch/val_loss</td><td>█▇▆▇▆▅▅▄▃▃▃▄▂▂▃▂▂▂▂▃▂▃▂▂▃▂▂▁▂▃▂▁▁▁▁▁▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.81111</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>0.54151</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.81</td></tr><tr><td>epoch/val_loss</td><td>0.5603</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF=396</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/j3791ixx' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/j3791ixx</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240410_115000-j3791ixx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f8d811df7d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(256, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, 1629),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", f\"LSTM256-Dense128-Dense256-LipsEyesHandsPose_NF={NUM_FEATURES}.tf\"),\n",
    "            run_name=f\"LSTM256-Dense128-Dense256-allfeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240410_123701-1f21pbbk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/1f21pbbk' target=\"_blank\">LSTM128-Dense256-LipsEyesHandsPose</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/1f21pbbk' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/1f21pbbk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.1278 - categorical_accuracy: 0.4178INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 33s 269ms/step - loss: 1.1278 - categorical_accuracy: 0.4178 - val_loss: 1.0969 - val_categorical_accuracy: 0.5100\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 1.0807 - categorical_accuracy: 0.4133 - val_loss: 1.0268 - val_categorical_accuracy: 0.4500\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.9822 - categorical_accuracy: 0.4578INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 268ms/step - loss: 0.9822 - categorical_accuracy: 0.4578 - val_loss: 0.8553 - val_categorical_accuracy: 0.5500\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.9092 - categorical_accuracy: 0.5056 - val_loss: 0.8547 - val_categorical_accuracy: 0.5400\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8810 - categorical_accuracy: 0.5244INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 265ms/step - loss: 0.8810 - categorical_accuracy: 0.5244 - val_loss: 0.8266 - val_categorical_accuracy: 0.5800\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8634 - categorical_accuracy: 0.5956INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 266ms/step - loss: 0.8634 - categorical_accuracy: 0.5956 - val_loss: 0.8122 - val_categorical_accuracy: 0.6100\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8561 - categorical_accuracy: 0.6256INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 261ms/step - loss: 0.8561 - categorical_accuracy: 0.6256 - val_loss: 0.8166 - val_categorical_accuracy: 0.6300\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.8361 - categorical_accuracy: 0.6211 - val_loss: 0.8184 - val_categorical_accuracy: 0.6000\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.8691 - categorical_accuracy: 0.6089 - val_loss: 0.9457 - val_categorical_accuracy: 0.5400\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.8121 - categorical_accuracy: 0.6422 - val_loss: 0.9020 - val_categorical_accuracy: 0.5600\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.8315 - categorical_accuracy: 0.6233 - val_loss: 0.7937 - val_categorical_accuracy: 0.5800\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - 24s 211ms/step - loss: 0.7917 - categorical_accuracy: 0.6411 - val_loss: 0.7710 - val_categorical_accuracy: 0.6100\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.7882 - categorical_accuracy: 0.6511 - val_loss: 0.7956 - val_categorical_accuracy: 0.6000\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7695 - categorical_accuracy: 0.6689INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 269ms/step - loss: 0.7695 - categorical_accuracy: 0.6689 - val_loss: 0.8096 - val_categorical_accuracy: 0.6500\n",
      "Epoch 15/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.7618 - categorical_accuracy: 0.6611 - val_loss: 0.8058 - val_categorical_accuracy: 0.6400\n",
      "Epoch 16/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.7758 - categorical_accuracy: 0.6544 - val_loss: 0.7668 - val_categorical_accuracy: 0.6100\n",
      "Epoch 17/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.7710 - categorical_accuracy: 0.6522 - val_loss: 0.8140 - val_categorical_accuracy: 0.6300\n",
      "Epoch 18/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7409 - categorical_accuracy: 0.6856INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 263ms/step - loss: 0.7409 - categorical_accuracy: 0.6856 - val_loss: 0.7513 - val_categorical_accuracy: 0.7100\n",
      "Epoch 19/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.7045 - categorical_accuracy: 0.7222 - val_loss: 0.8445 - val_categorical_accuracy: 0.6100\n",
      "Epoch 20/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6717 - categorical_accuracy: 0.7411INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 264ms/step - loss: 0.6717 - categorical_accuracy: 0.7411 - val_loss: 0.6603 - val_categorical_accuracy: 0.7500\n",
      "Epoch 21/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.6517 - categorical_accuracy: 0.7522 - val_loss: 0.7114 - val_categorical_accuracy: 0.7100\n",
      "Epoch 22/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.6440 - categorical_accuracy: 0.7600 - val_loss: 0.7203 - val_categorical_accuracy: 0.7200\n",
      "Epoch 23/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.6539 - categorical_accuracy: 0.7467 - val_loss: 0.7692 - val_categorical_accuracy: 0.6800\n",
      "Epoch 24/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.6461 - categorical_accuracy: 0.7589 - val_loss: 0.7264 - val_categorical_accuracy: 0.7100\n",
      "Epoch 25/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.6302 - categorical_accuracy: 0.7633 - val_loss: 0.7154 - val_categorical_accuracy: 0.7100\n",
      "Epoch 26/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.6586 - categorical_accuracy: 0.7422 - val_loss: 0.7366 - val_categorical_accuracy: 0.7200\n",
      "Epoch 27/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.6242 - categorical_accuracy: 0.7711 - val_loss: 0.7137 - val_categorical_accuracy: 0.7100\n",
      "Epoch 28/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.6027 - categorical_accuracy: 0.7800 - val_loss: 0.6956 - val_categorical_accuracy: 0.7500\n",
      "Epoch 29/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.6150 - categorical_accuracy: 0.7711 - val_loss: 0.7395 - val_categorical_accuracy: 0.7000\n",
      "Epoch 30/100\n",
      "113/113 [==============================] - 23s 204ms/step - loss: 0.5903 - categorical_accuracy: 0.7911 - val_loss: 0.7933 - val_categorical_accuracy: 0.6800\n",
      "Epoch 31/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.6010 - categorical_accuracy: 0.7811 - val_loss: 0.6749 - val_categorical_accuracy: 0.7200\n",
      "Epoch 32/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5821 - categorical_accuracy: 0.7800INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 266ms/step - loss: 0.5821 - categorical_accuracy: 0.7800 - val_loss: 0.6384 - val_categorical_accuracy: 0.7700\n",
      "Epoch 33/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.5741 - categorical_accuracy: 0.7956 - val_loss: 0.7332 - val_categorical_accuracy: 0.7300\n",
      "Epoch 34/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.5702 - categorical_accuracy: 0.7911 - val_loss: 0.7505 - val_categorical_accuracy: 0.7200\n",
      "Epoch 35/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.5598 - categorical_accuracy: 0.7933 - val_loss: 0.6531 - val_categorical_accuracy: 0.7400\n",
      "Epoch 36/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.5525 - categorical_accuracy: 0.8033 - val_loss: 0.6794 - val_categorical_accuracy: 0.7500\n",
      "Epoch 37/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5717 - categorical_accuracy: 0.7889 - val_loss: 0.6610 - val_categorical_accuracy: 0.7200\n",
      "Epoch 38/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.5479 - categorical_accuracy: 0.8111 - val_loss: 0.6500 - val_categorical_accuracy: 0.7500\n",
      "Epoch 39/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5564 - categorical_accuracy: 0.8000 - val_loss: 0.6438 - val_categorical_accuracy: 0.7400\n",
      "Epoch 40/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.5433 - categorical_accuracy: 0.8111 - val_loss: 0.6707 - val_categorical_accuracy: 0.7300\n",
      "Epoch 41/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.5321 - categorical_accuracy: 0.8244 - val_loss: 0.6708 - val_categorical_accuracy: 0.7500\n",
      "Epoch 42/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.5355 - categorical_accuracy: 0.8156 - val_loss: 0.7732 - val_categorical_accuracy: 0.7100\n",
      "Epoch 43/100\n",
      "113/113 [==============================] - 24s 212ms/step - loss: 0.5602 - categorical_accuracy: 0.8022 - val_loss: 0.7010 - val_categorical_accuracy: 0.7100\n",
      "Epoch 44/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.5380 - categorical_accuracy: 0.8111 - val_loss: 0.7011 - val_categorical_accuracy: 0.7400\n",
      "Epoch 45/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.5428 - categorical_accuracy: 0.8111 - val_loss: 0.8358 - val_categorical_accuracy: 0.6700\n",
      "Epoch 46/100\n",
      "113/113 [==============================] - 24s 207ms/step - loss: 0.5150 - categorical_accuracy: 0.8200 - val_loss: 0.6640 - val_categorical_accuracy: 0.7400\n",
      "Epoch 47/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.5071 - categorical_accuracy: 0.8267 - val_loss: 0.6850 - val_categorical_accuracy: 0.7300\n",
      "Epoch 48/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.5380 - categorical_accuracy: 0.8078 - val_loss: 0.6667 - val_categorical_accuracy: 0.7500\n",
      "Epoch 49/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5364 - categorical_accuracy: 0.8067 - val_loss: 0.6771 - val_categorical_accuracy: 0.7400\n",
      "Epoch 50/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5727 - categorical_accuracy: 0.7889 - val_loss: 0.6797 - val_categorical_accuracy: 0.7300\n",
      "Epoch 51/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5586 - categorical_accuracy: 0.8067 - val_loss: 0.7203 - val_categorical_accuracy: 0.6800\n",
      "Epoch 52/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.6005 - categorical_accuracy: 0.7789 - val_loss: 0.6556 - val_categorical_accuracy: 0.7600\n",
      "Epoch 53/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.5910 - categorical_accuracy: 0.7900 - val_loss: 0.7063 - val_categorical_accuracy: 0.7000\n",
      "Epoch 54/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.6004 - categorical_accuracy: 0.7700 - val_loss: 0.7516 - val_categorical_accuracy: 0.7100\n",
      "Epoch 55/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5570 - categorical_accuracy: 0.8022INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 264ms/step - loss: 0.5570 - categorical_accuracy: 0.8022 - val_loss: 0.6231 - val_categorical_accuracy: 0.7900\n",
      "Epoch 56/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5489 - categorical_accuracy: 0.7967 - val_loss: 0.8648 - val_categorical_accuracy: 0.6500\n",
      "Epoch 57/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.6694 - categorical_accuracy: 0.7478 - val_loss: 0.6612 - val_categorical_accuracy: 0.7700\n",
      "Epoch 58/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.5712 - categorical_accuracy: 0.7833 - val_loss: 0.6868 - val_categorical_accuracy: 0.7400\n",
      "Epoch 59/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5377 - categorical_accuracy: 0.8033 - val_loss: 0.6810 - val_categorical_accuracy: 0.7000\n",
      "Epoch 60/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.5724 - categorical_accuracy: 0.7922 - val_loss: 0.7386 - val_categorical_accuracy: 0.7200\n",
      "Epoch 61/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.5789 - categorical_accuracy: 0.7933 - val_loss: 0.6927 - val_categorical_accuracy: 0.7500\n",
      "Epoch 62/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.5703 - categorical_accuracy: 0.7956 - val_loss: 0.6624 - val_categorical_accuracy: 0.7500\n",
      "Epoch 63/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5127 - categorical_accuracy: 0.8144 - val_loss: 0.6031 - val_categorical_accuracy: 0.7700\n",
      "Epoch 64/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.4926 - categorical_accuracy: 0.8256 - val_loss: 0.6194 - val_categorical_accuracy: 0.7700\n",
      "Epoch 65/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4869 - categorical_accuracy: 0.8167 - val_loss: 0.7040 - val_categorical_accuracy: 0.7000\n",
      "Epoch 66/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4625 - categorical_accuracy: 0.8256INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 266ms/step - loss: 0.4625 - categorical_accuracy: 0.8256 - val_loss: 0.5162 - val_categorical_accuracy: 0.8200\n",
      "Epoch 67/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4424 - categorical_accuracy: 0.8367INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 268ms/step - loss: 0.4424 - categorical_accuracy: 0.8367 - val_loss: 0.5564 - val_categorical_accuracy: 0.8300\n",
      "Epoch 68/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.4861 - categorical_accuracy: 0.8378 - val_loss: 0.5770 - val_categorical_accuracy: 0.8200\n",
      "Epoch 69/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.4566 - categorical_accuracy: 0.8400 - val_loss: 0.5963 - val_categorical_accuracy: 0.8100\n",
      "Epoch 70/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.4515 - categorical_accuracy: 0.8444 - val_loss: 0.5909 - val_categorical_accuracy: 0.8200\n",
      "Epoch 71/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4824 - categorical_accuracy: 0.8456 - val_loss: 0.5653 - val_categorical_accuracy: 0.7900\n",
      "Epoch 72/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.4370 - categorical_accuracy: 0.8589 - val_loss: 0.5207 - val_categorical_accuracy: 0.7900\n",
      "Epoch 73/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.4473 - categorical_accuracy: 0.8433 - val_loss: 0.7179 - val_categorical_accuracy: 0.7600\n",
      "Epoch 74/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.4603 - categorical_accuracy: 0.8400 - val_loss: 0.5274 - val_categorical_accuracy: 0.8200\n",
      "Epoch 75/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.3941 - categorical_accuracy: 0.8689 - val_loss: 0.5005 - val_categorical_accuracy: 0.8200\n",
      "Epoch 76/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.3999 - categorical_accuracy: 0.8733 - val_loss: 0.6948 - val_categorical_accuracy: 0.7900\n",
      "Epoch 77/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4284 - categorical_accuracy: 0.8578 - val_loss: 0.6430 - val_categorical_accuracy: 0.7700\n",
      "Epoch 78/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.3821 - categorical_accuracy: 0.8822 - val_loss: 0.4765 - val_categorical_accuracy: 0.8100\n",
      "Epoch 79/100\n",
      "113/113 [==============================] - 24s 207ms/step - loss: 0.3963 - categorical_accuracy: 0.8789 - val_loss: 0.6140 - val_categorical_accuracy: 0.7800\n",
      "Epoch 80/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.3791 - categorical_accuracy: 0.8856 - val_loss: 0.5654 - val_categorical_accuracy: 0.8300\n",
      "Epoch 81/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.4059 - categorical_accuracy: 0.8678 - val_loss: 0.6284 - val_categorical_accuracy: 0.8100\n",
      "Epoch 82/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.4231 - categorical_accuracy: 0.8656 - val_loss: 0.6050 - val_categorical_accuracy: 0.8000\n",
      "Epoch 83/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.3983 - categorical_accuracy: 0.8767INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 266ms/step - loss: 0.3983 - categorical_accuracy: 0.8767 - val_loss: 0.5015 - val_categorical_accuracy: 0.8400\n",
      "Epoch 84/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.3923 - categorical_accuracy: 0.8789 - val_loss: 0.5693 - val_categorical_accuracy: 0.7900\n",
      "Epoch 85/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4089 - categorical_accuracy: 0.8744 - val_loss: 0.5546 - val_categorical_accuracy: 0.8300\n",
      "Epoch 86/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.3886 - categorical_accuracy: 0.8800 - val_loss: 0.5865 - val_categorical_accuracy: 0.8000\n",
      "Epoch 87/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.3806 - categorical_accuracy: 0.8833 - val_loss: 0.6073 - val_categorical_accuracy: 0.8300\n",
      "Epoch 88/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.3585 - categorical_accuracy: 0.8922 - val_loss: 0.5232 - val_categorical_accuracy: 0.8300\n",
      "Epoch 89/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.3255 - categorical_accuracy: 0.9056 - val_loss: 0.5635 - val_categorical_accuracy: 0.8200\n",
      "Epoch 90/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.3562 - categorical_accuracy: 0.8911 - val_loss: 0.5394 - val_categorical_accuracy: 0.8400\n",
      "Epoch 91/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.3596 - categorical_accuracy: 0.8933 - val_loss: 0.6465 - val_categorical_accuracy: 0.8200\n",
      "Epoch 92/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.3304 - categorical_accuracy: 0.8978 - val_loss: 0.5601 - val_categorical_accuracy: 0.8000\n",
      "Epoch 93/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.3362 - categorical_accuracy: 0.8967 - val_loss: 0.5903 - val_categorical_accuracy: 0.8300\n",
      "Epoch 94/100\n",
      "113/113 [==============================] - 24s 211ms/step - loss: 0.3340 - categorical_accuracy: 0.9011 - val_loss: 0.5900 - val_categorical_accuracy: 0.8400\n",
      "Epoch 95/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.3341 - categorical_accuracy: 0.9044INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 267ms/step - loss: 0.3341 - categorical_accuracy: 0.9044 - val_loss: 0.4749 - val_categorical_accuracy: 0.8500\n",
      "Epoch 96/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.3291 - categorical_accuracy: 0.8978INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 266ms/step - loss: 0.3291 - categorical_accuracy: 0.8978 - val_loss: 0.4835 - val_categorical_accuracy: 0.8600\n",
      "Epoch 97/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.3441 - categorical_accuracy: 0.9011 - val_loss: 0.6744 - val_categorical_accuracy: 0.8100\n",
      "Epoch 98/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.2945 - categorical_accuracy: 0.9122 - val_loss: 0.5999 - val_categorical_accuracy: 0.8300\n",
      "Epoch 99/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.3170 - categorical_accuracy: 0.9067 - val_loss: 0.5721 - val_categorical_accuracy: 0.8400\n",
      "Epoch 100/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.5162 - categorical_accuracy: 0.8411 - val_loss: 0.6316 - val_categorical_accuracy: 0.8200\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁▂▄▄▄▄▄▅▆▆▆▆▆▆▇▆▇▇▇▇▇▆▆▇▆▇▇▇▇▇▇█▇██████▇</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁███▇▇▇▆▆▅▅▄▄▃▃▂▂▁▁▁█</td></tr><tr><td>epoch/loss</td><td>█▇▆▅▅▅▅▅▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▃</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁▂▃▃▂▃▃▅▅▅▅▆▅▅▆▆▆▆▆▆▅▅▄▅▆▇█▇▇█▇▇▇▇██▇█▇█</td></tr><tr><td>epoch/val_loss</td><td>█▅▅▅▄▄▄▄▃▄▄▃▃▄▃▂▃▃▃▃▃▄▅▃▃▂▁▂▁▁▂▂▂▂▂▂▁▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.84111</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>0.51617</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.82</td></tr><tr><td>epoch/val_loss</td><td>0.63159</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM128-Dense256-LipsEyesHandsPose</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/1f21pbbk' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/1f21pbbk</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240410_123701-1f21pbbk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f8d5c303550>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(128, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, NUM_FEATURES),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", f\"LSTM128-Dense256-LipsEyesHandsPose_NF={NUM_FEATURES}.tf\"),\n",
    "            run_name=\"LSTM128-Dense256-LipsEyesHandsPose\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(128, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, 1629),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", f\"LSTM128-Dense256-LipsEyesHandsPose_NF={NUM_FEATURES}.tf\"),\n",
    "            run_name=\"LSTM128-Dense128-LipsEyesHandsPose\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240410_143215-09hhzkno</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/09hhzkno' target=\"_blank\">LSTM128-Dense256-LipsEyesHandsPose</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/09hhzkno' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/09hhzkno</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.1311 - categorical_accuracy: 0.3789INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 34s 272ms/step - loss: 1.1311 - categorical_accuracy: 0.3789 - val_loss: 1.1027 - val_categorical_accuracy: 0.5000\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 1.0955 - categorical_accuracy: 0.4122 - val_loss: 1.0472 - val_categorical_accuracy: 0.4700\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.9997 - categorical_accuracy: 0.4600 - val_loss: 0.8613 - val_categorical_accuracy: 0.5000\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.9474 - categorical_accuracy: 0.5022INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 264ms/step - loss: 0.9474 - categorical_accuracy: 0.5022 - val_loss: 0.8488 - val_categorical_accuracy: 0.5700\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8828 - categorical_accuracy: 0.5289INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 265ms/step - loss: 0.8828 - categorical_accuracy: 0.5289 - val_loss: 0.8102 - val_categorical_accuracy: 0.5800\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.8716 - categorical_accuracy: 0.5478 - val_loss: 0.8289 - val_categorical_accuracy: 0.5600\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.8605 - categorical_accuracy: 0.5456 - val_loss: 0.8446 - val_categorical_accuracy: 0.5800\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8792 - categorical_accuracy: 0.5589INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 268ms/step - loss: 0.8792 - categorical_accuracy: 0.5589 - val_loss: 0.8476 - val_categorical_accuracy: 0.6000\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.8610 - categorical_accuracy: 0.6200 - val_loss: 0.8277 - val_categorical_accuracy: 0.6000\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8515 - categorical_accuracy: 0.5900INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 29s 261ms/step - loss: 0.8515 - categorical_accuracy: 0.5900 - val_loss: 0.7934 - val_categorical_accuracy: 0.6700\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.8333 - categorical_accuracy: 0.6022 - val_loss: 0.8267 - val_categorical_accuracy: 0.5700\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.8162 - categorical_accuracy: 0.5978 - val_loss: 0.7937 - val_categorical_accuracy: 0.6200\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - 24s 207ms/step - loss: 0.8129 - categorical_accuracy: 0.6178 - val_loss: 0.7740 - val_categorical_accuracy: 0.6200\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8034 - categorical_accuracy: 0.6244INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 268ms/step - loss: 0.8034 - categorical_accuracy: 0.6244 - val_loss: 0.7691 - val_categorical_accuracy: 0.6800\n",
      "Epoch 15/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.7827 - categorical_accuracy: 0.6311 - val_loss: 0.7708 - val_categorical_accuracy: 0.6000\n",
      "Epoch 16/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.7550 - categorical_accuracy: 0.6656 - val_loss: 0.8181 - val_categorical_accuracy: 0.5800\n",
      "Epoch 17/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.7593 - categorical_accuracy: 0.6744 - val_loss: 0.7455 - val_categorical_accuracy: 0.6600\n",
      "Epoch 18/100\n",
      "113/113 [==============================] - 24s 211ms/step - loss: 0.7286 - categorical_accuracy: 0.6889 - val_loss: 0.7946 - val_categorical_accuracy: 0.6400\n",
      "Epoch 19/100\n",
      "112/113 [============================>.] - ETA: 0s - loss: 0.7510 - categorical_accuracy: 0.6775INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 268ms/step - loss: 0.7492 - categorical_accuracy: 0.6789 - val_loss: 0.7397 - val_categorical_accuracy: 0.6900\n",
      "Epoch 20/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6793 - categorical_accuracy: 0.7289INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 270ms/step - loss: 0.6793 - categorical_accuracy: 0.7289 - val_loss: 0.6869 - val_categorical_accuracy: 0.7300\n",
      "Epoch 21/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.6884 - categorical_accuracy: 0.7311 - val_loss: 0.6701 - val_categorical_accuracy: 0.7300\n",
      "Epoch 22/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6616 - categorical_accuracy: 0.7511INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 263ms/step - loss: 0.6616 - categorical_accuracy: 0.7511 - val_loss: 0.6486 - val_categorical_accuracy: 0.7500\n",
      "Epoch 23/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6289 - categorical_accuracy: 0.7556INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 269ms/step - loss: 0.6289 - categorical_accuracy: 0.7556 - val_loss: 0.6536 - val_categorical_accuracy: 0.7600\n",
      "Epoch 24/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.6518 - categorical_accuracy: 0.7389 - val_loss: 0.6748 - val_categorical_accuracy: 0.7200\n",
      "Epoch 25/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.6451 - categorical_accuracy: 0.7500 - val_loss: 0.6781 - val_categorical_accuracy: 0.7100\n",
      "Epoch 26/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.6273 - categorical_accuracy: 0.7667 - val_loss: 0.6722 - val_categorical_accuracy: 0.7400\n",
      "Epoch 27/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.6504 - categorical_accuracy: 0.7522 - val_loss: 0.7024 - val_categorical_accuracy: 0.7200\n",
      "Epoch 28/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.6487 - categorical_accuracy: 0.7522 - val_loss: 0.7037 - val_categorical_accuracy: 0.7300\n",
      "Epoch 29/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.6159 - categorical_accuracy: 0.7722 - val_loss: 0.7157 - val_categorical_accuracy: 0.6800\n",
      "Epoch 30/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.5984 - categorical_accuracy: 0.7778 - val_loss: 0.6741 - val_categorical_accuracy: 0.7300\n",
      "Epoch 31/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.6130 - categorical_accuracy: 0.7678 - val_loss: 0.6788 - val_categorical_accuracy: 0.7400\n",
      "Epoch 32/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5971 - categorical_accuracy: 0.7822INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 267ms/step - loss: 0.5971 - categorical_accuracy: 0.7822 - val_loss: 0.5995 - val_categorical_accuracy: 0.7800\n",
      "Epoch 33/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5753 - categorical_accuracy: 0.7922 - val_loss: 0.5993 - val_categorical_accuracy: 0.7800\n",
      "Epoch 34/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5705 - categorical_accuracy: 0.7967INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 268ms/step - loss: 0.5705 - categorical_accuracy: 0.7967 - val_loss: 0.5728 - val_categorical_accuracy: 0.8100\n",
      "Epoch 35/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.5595 - categorical_accuracy: 0.7978 - val_loss: 0.5899 - val_categorical_accuracy: 0.7600\n",
      "Epoch 36/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.5430 - categorical_accuracy: 0.8089 - val_loss: 0.6292 - val_categorical_accuracy: 0.7900\n",
      "Epoch 37/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5658 - categorical_accuracy: 0.8000INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 269ms/step - loss: 0.5658 - categorical_accuracy: 0.8000 - val_loss: 0.4716 - val_categorical_accuracy: 0.8700\n",
      "Epoch 38/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.5527 - categorical_accuracy: 0.8078 - val_loss: 0.5368 - val_categorical_accuracy: 0.8300\n",
      "Epoch 39/100\n",
      "113/113 [==============================] - 24s 211ms/step - loss: 0.5886 - categorical_accuracy: 0.7844 - val_loss: 0.5655 - val_categorical_accuracy: 0.7700\n",
      "Epoch 40/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.5535 - categorical_accuracy: 0.8056 - val_loss: 0.5687 - val_categorical_accuracy: 0.7900\n",
      "Epoch 41/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.5450 - categorical_accuracy: 0.8067 - val_loss: 0.5632 - val_categorical_accuracy: 0.8100\n",
      "Epoch 42/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.5360 - categorical_accuracy: 0.8144 - val_loss: 0.5319 - val_categorical_accuracy: 0.8300\n",
      "Epoch 43/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.5210 - categorical_accuracy: 0.8233 - val_loss: 0.4811 - val_categorical_accuracy: 0.8500\n",
      "Epoch 44/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5037 - categorical_accuracy: 0.8222 - val_loss: 0.4920 - val_categorical_accuracy: 0.8600\n",
      "Epoch 45/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.4989 - categorical_accuracy: 0.8311 - val_loss: 0.5315 - val_categorical_accuracy: 0.8300\n",
      "Epoch 46/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.4746 - categorical_accuracy: 0.8422 - val_loss: 0.4776 - val_categorical_accuracy: 0.8600\n",
      "Epoch 47/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.5116 - categorical_accuracy: 0.8178 - val_loss: 0.5891 - val_categorical_accuracy: 0.8000\n",
      "Epoch 48/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.4922 - categorical_accuracy: 0.8344 - val_loss: 0.4957 - val_categorical_accuracy: 0.8600\n",
      "Epoch 49/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.5018 - categorical_accuracy: 0.8300 - val_loss: 0.6352 - val_categorical_accuracy: 0.8000\n",
      "Epoch 50/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.5659 - categorical_accuracy: 0.7989 - val_loss: 0.6671 - val_categorical_accuracy: 0.7500\n",
      "Epoch 51/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.5446 - categorical_accuracy: 0.8144 - val_loss: 0.5651 - val_categorical_accuracy: 0.8300\n",
      "Epoch 52/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.5213 - categorical_accuracy: 0.8156 - val_loss: 0.6547 - val_categorical_accuracy: 0.8000\n",
      "Epoch 53/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.5706 - categorical_accuracy: 0.8022 - val_loss: 0.5563 - val_categorical_accuracy: 0.7900\n",
      "Epoch 54/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.5549 - categorical_accuracy: 0.7967 - val_loss: 0.6038 - val_categorical_accuracy: 0.7700\n",
      "Epoch 55/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.5076 - categorical_accuracy: 0.8211 - val_loss: 0.7854 - val_categorical_accuracy: 0.7300\n",
      "Epoch 56/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5758 - categorical_accuracy: 0.8056 - val_loss: 0.5099 - val_categorical_accuracy: 0.8000\n",
      "Epoch 57/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5359 - categorical_accuracy: 0.8078 - val_loss: 0.8828 - val_categorical_accuracy: 0.7400\n",
      "Epoch 58/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5095 - categorical_accuracy: 0.8333 - val_loss: 0.5874 - val_categorical_accuracy: 0.8000\n",
      "Epoch 59/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5658 - categorical_accuracy: 0.7989 - val_loss: 0.6052 - val_categorical_accuracy: 0.7900\n",
      "Epoch 60/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.5456 - categorical_accuracy: 0.8178 - val_loss: 0.5665 - val_categorical_accuracy: 0.8000\n",
      "Epoch 61/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.5405 - categorical_accuracy: 0.8033 - val_loss: 0.5633 - val_categorical_accuracy: 0.7900\n",
      "Epoch 62/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.4668 - categorical_accuracy: 0.8378 - val_loss: 0.5200 - val_categorical_accuracy: 0.8300\n",
      "Epoch 63/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.4674 - categorical_accuracy: 0.8478 - val_loss: 0.4709 - val_categorical_accuracy: 0.8400\n",
      "Epoch 64/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.4872 - categorical_accuracy: 0.8311 - val_loss: 0.5813 - val_categorical_accuracy: 0.8100\n",
      "Epoch 65/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.5543 - categorical_accuracy: 0.8044 - val_loss: 0.5232 - val_categorical_accuracy: 0.8300\n",
      "Epoch 66/100\n",
      "113/113 [==============================] - 24s 211ms/step - loss: 0.4920 - categorical_accuracy: 0.8156 - val_loss: 0.6383 - val_categorical_accuracy: 0.7900\n",
      "Epoch 67/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.4953 - categorical_accuracy: 0.8222 - val_loss: 0.6147 - val_categorical_accuracy: 0.7800\n",
      "Epoch 68/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.4817 - categorical_accuracy: 0.8333 - val_loss: 0.5746 - val_categorical_accuracy: 0.7800\n",
      "Epoch 69/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5257 - categorical_accuracy: 0.8189 - val_loss: 0.4639 - val_categorical_accuracy: 0.8400\n",
      "Epoch 70/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4715 - categorical_accuracy: 0.8356 - val_loss: 0.5354 - val_categorical_accuracy: 0.8300\n",
      "Epoch 71/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.4501 - categorical_accuracy: 0.8522 - val_loss: 0.4747 - val_categorical_accuracy: 0.8600\n",
      "Epoch 72/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.4656 - categorical_accuracy: 0.8322 - val_loss: 0.5851 - val_categorical_accuracy: 0.8000\n",
      "Epoch 73/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.4477 - categorical_accuracy: 0.8467 - val_loss: 0.5477 - val_categorical_accuracy: 0.8200\n",
      "Epoch 74/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.4823 - categorical_accuracy: 0.8400 - val_loss: 0.4843 - val_categorical_accuracy: 0.8500\n",
      "Epoch 75/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.4710 - categorical_accuracy: 0.8389 - val_loss: 0.4713 - val_categorical_accuracy: 0.8400\n",
      "Epoch 76/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4542 - categorical_accuracy: 0.8467 - val_loss: 0.6059 - val_categorical_accuracy: 0.7700\n",
      "Epoch 77/100\n",
      "113/113 [==============================] - 24s 207ms/step - loss: 0.4727 - categorical_accuracy: 0.8367 - val_loss: 0.5349 - val_categorical_accuracy: 0.8200\n",
      "Epoch 78/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.4223 - categorical_accuracy: 0.8556 - val_loss: 0.4870 - val_categorical_accuracy: 0.8400\n",
      "Epoch 79/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.4599 - categorical_accuracy: 0.8367 - val_loss: 0.5201 - val_categorical_accuracy: 0.7900\n",
      "Epoch 80/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.4847 - categorical_accuracy: 0.8344 - val_loss: 0.5412 - val_categorical_accuracy: 0.8100\n",
      "Epoch 81/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.4500 - categorical_accuracy: 0.8511 - val_loss: 0.4787 - val_categorical_accuracy: 0.8500\n",
      "Epoch 82/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.4211 - categorical_accuracy: 0.8633 - val_loss: 0.5205 - val_categorical_accuracy: 0.8300\n",
      "Epoch 83/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.4089 - categorical_accuracy: 0.8633 - val_loss: 0.4392 - val_categorical_accuracy: 0.8400\n",
      "Epoch 84/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.3971 - categorical_accuracy: 0.8711 - val_loss: 0.6131 - val_categorical_accuracy: 0.7800\n",
      "Epoch 85/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.4153 - categorical_accuracy: 0.8744 - val_loss: 0.4844 - val_categorical_accuracy: 0.8500\n",
      "Epoch 86/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.3842 - categorical_accuracy: 0.8722 - val_loss: 0.5154 - val_categorical_accuracy: 0.8200\n",
      "Epoch 87/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.3982 - categorical_accuracy: 0.8767 - val_loss: 0.5329 - val_categorical_accuracy: 0.8100\n",
      "Epoch 88/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.4146 - categorical_accuracy: 0.8678 - val_loss: 0.4606 - val_categorical_accuracy: 0.8300\n",
      "Epoch 89/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.4060 - categorical_accuracy: 0.8644 - val_loss: 0.5736 - val_categorical_accuracy: 0.8000\n",
      "Epoch 90/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.4169 - categorical_accuracy: 0.8644 - val_loss: 0.5779 - val_categorical_accuracy: 0.8100\n",
      "Epoch 91/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.4065 - categorical_accuracy: 0.8689 - val_loss: 0.5546 - val_categorical_accuracy: 0.8100\n",
      "Epoch 92/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.3638 - categorical_accuracy: 0.8856 - val_loss: 0.5141 - val_categorical_accuracy: 0.7800\n",
      "Epoch 93/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.3682 - categorical_accuracy: 0.8811 - val_loss: 0.5459 - val_categorical_accuracy: 0.8200\n",
      "Epoch 94/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.3923 - categorical_accuracy: 0.8700 - val_loss: 0.5069 - val_categorical_accuracy: 0.8400\n",
      "Epoch 95/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.4050 - categorical_accuracy: 0.8689 - val_loss: 0.5045 - val_categorical_accuracy: 0.8100\n",
      "Epoch 96/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.3809 - categorical_accuracy: 0.8833 - val_loss: 0.6020 - val_categorical_accuracy: 0.8100\n",
      "Epoch 97/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.3809 - categorical_accuracy: 0.8844 - val_loss: 0.5300 - val_categorical_accuracy: 0.8300\n",
      "Epoch 98/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.3675 - categorical_accuracy: 0.8800 - val_loss: 0.5133 - val_categorical_accuracy: 0.8200\n",
      "Epoch 99/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.3597 - categorical_accuracy: 0.8789 - val_loss: 0.6474 - val_categorical_accuracy: 0.7800\n",
      "Epoch 100/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.4643 - categorical_accuracy: 0.8456 - val_loss: 0.6158 - val_categorical_accuracy: 0.8000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁▂▃▃▄▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████▇</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁███▇▇▇▆▆▅▅▄▄▃▃▂▂▁▁▁█</td></tr><tr><td>epoch/loss</td><td>█▇▆▆▅▅▅▄▄▃▃▄▃▃▃▃▃▂▂▂▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁▁▂▃▂▃▃▄▅▆▆▅▆▇▇▆▇██▇▇▆▇▇▇▇▆█▇█▇▇▇▆▇▇▆█▇▇</td></tr><tr><td>epoch/val_loss</td><td>█▅▅▅▅▄▅▅▃▃▃▄▃▂▃▂▂▁▁▃▂▃▂▃▂▂▃▁▂▁▂▂▂▃▂▂▂▁▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.84556</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>0.46433</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.8</td></tr><tr><td>epoch/val_loss</td><td>0.61581</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM128-Dense256-LipsEyesHandsPose</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/09hhzkno' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/09hhzkno</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240410_143215-09hhzkno/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f830c2c7f90>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(128, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, NUM_FEATURES),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", f\"LSTM128-Dense256-LipsEyesHandsPose_NF={NUM_FEATURES}.tf\"),\n",
    "            run_name=\"LSTM128-Dense256-LipsEyesHandsPose\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240410_151331-aox13z1l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/aox13z1l' target=\"_blank\">LSTM128_l2-Dense128-Dense256-LipsEyesHandsPose</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/aox13z1l' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/aox13z1l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0851 - categorical_accuracy: 0.4322INFO:tensorflow:Assets written to: models/LSTM128_l2-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128_l2-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 235ms/step - loss: 1.0851 - categorical_accuracy: 0.4322 - val_loss: 1.0499 - val_categorical_accuracy: 0.4400\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0508 - categorical_accuracy: 0.4322INFO:tensorflow:Assets written to: models/LSTM128_l2-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128_l2-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 26s 232ms/step - loss: 1.0508 - categorical_accuracy: 0.4322 - val_loss: 1.0254 - val_categorical_accuracy: 0.4700\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 2.0936 - categorical_accuracy: 0.4444 - val_loss: 1.4172 - val_categorical_accuracy: 0.4600\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.6131 - categorical_accuracy: 0.5222INFO:tensorflow:Assets written to: models/LSTM128_l2-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128_l2-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 27s 237ms/step - loss: 1.6131 - categorical_accuracy: 0.5222 - val_loss: 0.8351 - val_categorical_accuracy: 0.6100\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 1.2382 - categorical_accuracy: 0.5744 - val_loss: 0.8471 - val_categorical_accuracy: 0.6000\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8741 - categorical_accuracy: 0.6133INFO:tensorflow:Assets written to: models/LSTM128_l2-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128_l2-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 27s 237ms/step - loss: 0.8741 - categorical_accuracy: 0.6133 - val_loss: 0.7797 - val_categorical_accuracy: 0.6300\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.9141 - categorical_accuracy: 0.6344INFO:tensorflow:Assets written to: models/LSTM128_l2-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128_l2-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 27s 236ms/step - loss: 0.9141 - categorical_accuracy: 0.6344 - val_loss: 0.7303 - val_categorical_accuracy: 0.6700\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8547 - categorical_accuracy: 0.6522INFO:tensorflow:Assets written to: models/LSTM128_l2-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128_l2-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 27s 238ms/step - loss: 0.8547 - categorical_accuracy: 0.6522 - val_loss: 0.6717 - val_categorical_accuracy: 0.6800\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7246 - categorical_accuracy: 0.6511INFO:tensorflow:Assets written to: models/LSTM128_l2-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128_l2-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 27s 237ms/step - loss: 0.7246 - categorical_accuracy: 0.6511 - val_loss: 0.9137 - val_categorical_accuracy: 0.6900\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.8245 - categorical_accuracy: 0.6200 - val_loss: 0.7579 - val_categorical_accuracy: 0.6600\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7430 - categorical_accuracy: 0.6478INFO:tensorflow:Assets written to: models/LSTM128_l2-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128_l2-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 27s 237ms/step - loss: 0.7430 - categorical_accuracy: 0.6478 - val_loss: 0.7376 - val_categorical_accuracy: 0.7200\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.7039 - categorical_accuracy: 0.6989 - val_loss: 0.7419 - val_categorical_accuracy: 0.6500\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6577 - categorical_accuracy: 0.7333INFO:tensorflow:Assets written to: models/LSTM128_l2-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128_l2-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 27s 240ms/step - loss: 0.6577 - categorical_accuracy: 0.7333 - val_loss: 0.6279 - val_categorical_accuracy: 0.7600\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.6809 - categorical_accuracy: 0.7389 - val_loss: 1.2419 - val_categorical_accuracy: 0.7000\n",
      "Epoch 15/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.7462 - categorical_accuracy: 0.7444 - val_loss: 0.5694 - val_categorical_accuracy: 0.7400\n",
      "Epoch 16/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6660 - categorical_accuracy: 0.7300INFO:tensorflow:Assets written to: models/LSTM128_l2-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128_l2-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 27s 237ms/step - loss: 0.6660 - categorical_accuracy: 0.7300 - val_loss: 0.5790 - val_categorical_accuracy: 0.7900\n",
      "Epoch 17/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.5792 - categorical_accuracy: 0.7567 - val_loss: 0.5935 - val_categorical_accuracy: 0.7200\n",
      "Epoch 18/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5760 - categorical_accuracy: 0.7689 - val_loss: 0.6286 - val_categorical_accuracy: 0.7500\n",
      "Epoch 19/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.6299 - categorical_accuracy: 0.7578 - val_loss: 0.5560 - val_categorical_accuracy: 0.7500\n",
      "Epoch 20/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5443 - categorical_accuracy: 0.7767INFO:tensorflow:Assets written to: models/LSTM128_l2-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128_l2-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 27s 238ms/step - loss: 0.5443 - categorical_accuracy: 0.7767 - val_loss: 0.5433 - val_categorical_accuracy: 0.8100\n",
      "Epoch 21/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.5616 - categorical_accuracy: 0.7756 - val_loss: 0.5452 - val_categorical_accuracy: 0.7600\n",
      "Epoch 22/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5716 - categorical_accuracy: 0.7756 - val_loss: 0.5873 - val_categorical_accuracy: 0.7000\n",
      "Epoch 23/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5640 - categorical_accuracy: 0.7722 - val_loss: 0.4932 - val_categorical_accuracy: 0.7600\n",
      "Epoch 24/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.5430 - categorical_accuracy: 0.7956 - val_loss: 0.5337 - val_categorical_accuracy: 0.7700\n",
      "Epoch 25/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5274 - categorical_accuracy: 0.7800 - val_loss: 0.6318 - val_categorical_accuracy: 0.7800\n",
      "Epoch 26/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.5236 - categorical_accuracy: 0.7922 - val_loss: 0.4524 - val_categorical_accuracy: 0.7600\n",
      "Epoch 27/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.5108 - categorical_accuracy: 0.7933 - val_loss: 0.6491 - val_categorical_accuracy: 0.8100\n",
      "Epoch 28/100\n",
      "113/113 [==============================] - 24s 212ms/step - loss: 0.5211 - categorical_accuracy: 0.7989 - val_loss: 0.5408 - val_categorical_accuracy: 0.8000\n",
      "Epoch 29/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.4809 - categorical_accuracy: 0.8067 - val_loss: 0.4918 - val_categorical_accuracy: 0.8100\n",
      "Epoch 30/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.4882 - categorical_accuracy: 0.8033 - val_loss: 0.5184 - val_categorical_accuracy: 0.7500\n",
      "Epoch 31/100\n",
      "113/113 [==============================] - 23s 204ms/step - loss: 0.5263 - categorical_accuracy: 0.8000 - val_loss: 0.7472 - val_categorical_accuracy: 0.7900\n",
      "Epoch 32/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.4828 - categorical_accuracy: 0.8100 - val_loss: 0.8347 - val_categorical_accuracy: 0.7800\n",
      "Epoch 33/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.9653 - categorical_accuracy: 0.7878 - val_loss: 5.8879 - val_categorical_accuracy: 0.7800\n",
      "Epoch 34/100\n",
      "113/113 [==============================] - 24s 206ms/step - loss: 1.8343 - categorical_accuracy: 0.7978 - val_loss: 0.9542 - val_categorical_accuracy: 0.7700\n",
      "Epoch 35/100\n",
      "113/113 [==============================] - 24s 207ms/step - loss: 0.5985 - categorical_accuracy: 0.7967 - val_loss: 0.8792 - val_categorical_accuracy: 0.7400\n",
      "Epoch 36/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.6222 - categorical_accuracy: 0.8089 - val_loss: 0.6095 - val_categorical_accuracy: 0.8000\n",
      "Epoch 37/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.4738 - categorical_accuracy: 0.8200 - val_loss: 0.7538 - val_categorical_accuracy: 0.7300\n",
      "Epoch 38/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.5010 - categorical_accuracy: 0.8156 - val_loss: 0.7653 - val_categorical_accuracy: 0.7800\n",
      "Epoch 39/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.4443 - categorical_accuracy: 0.8311 - val_loss: 1.0396 - val_categorical_accuracy: 0.7800\n",
      "Epoch 40/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.5503 - categorical_accuracy: 0.8000 - val_loss: 0.9695 - val_categorical_accuracy: 0.7700\n",
      "Epoch 41/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5074 - categorical_accuracy: 0.8200 - val_loss: 0.4924 - val_categorical_accuracy: 0.8100\n",
      "Epoch 42/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.4673 - categorical_accuracy: 0.8156 - val_loss: 0.8852 - val_categorical_accuracy: 0.7800\n",
      "Epoch 43/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.5500 - categorical_accuracy: 0.8222 - val_loss: 0.6690 - val_categorical_accuracy: 0.7500\n",
      "Epoch 44/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5197 - categorical_accuracy: 0.8000 - val_loss: 0.9234 - val_categorical_accuracy: 0.7700\n",
      "Epoch 45/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4189 - categorical_accuracy: 0.8533INFO:tensorflow:Assets written to: models/LSTM128_l2-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128_l2-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 27s 236ms/step - loss: 0.4189 - categorical_accuracy: 0.8533 - val_loss: 0.4349 - val_categorical_accuracy: 0.8600\n",
      "Epoch 46/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5718 - categorical_accuracy: 0.8078 - val_loss: 0.6133 - val_categorical_accuracy: 0.8000\n",
      "Epoch 47/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.9645 - categorical_accuracy: 0.8022 - val_loss: 0.8878 - val_categorical_accuracy: 0.7400\n",
      "Epoch 48/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4271 - categorical_accuracy: 0.8489 - val_loss: 0.7049 - val_categorical_accuracy: 0.7600\n",
      "Epoch 49/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4216 - categorical_accuracy: 0.8556 - val_loss: 0.5903 - val_categorical_accuracy: 0.7700\n",
      "Epoch 50/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.6679 - categorical_accuracy: 0.7989 - val_loss: 1.1187 - val_categorical_accuracy: 0.7300\n",
      "Epoch 51/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5183 - categorical_accuracy: 0.8089 - val_loss: 0.6420 - val_categorical_accuracy: 0.7400\n",
      "Epoch 52/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5139 - categorical_accuracy: 0.8044 - val_loss: 0.4279 - val_categorical_accuracy: 0.8200\n",
      "Epoch 53/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5028 - categorical_accuracy: 0.8178 - val_loss: 0.6031 - val_categorical_accuracy: 0.7600\n",
      "Epoch 54/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.6012 - categorical_accuracy: 0.8000 - val_loss: 0.4761 - val_categorical_accuracy: 0.7800\n",
      "Epoch 55/100\n",
      "113/113 [==============================] - 24s 207ms/step - loss: 0.8188 - categorical_accuracy: 0.7756 - val_loss: 0.7929 - val_categorical_accuracy: 0.7500\n",
      "Epoch 56/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.6120 - categorical_accuracy: 0.7722 - val_loss: 0.9496 - val_categorical_accuracy: 0.7600\n",
      "Epoch 57/100\n",
      "113/113 [==============================] - 24s 211ms/step - loss: 0.5258 - categorical_accuracy: 0.7911 - val_loss: 0.7096 - val_categorical_accuracy: 0.8000\n",
      "Epoch 58/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.4972 - categorical_accuracy: 0.7956 - val_loss: 0.7829 - val_categorical_accuracy: 0.7200\n",
      "Epoch 59/100\n",
      "113/113 [==============================] - 24s 211ms/step - loss: 0.8863 - categorical_accuracy: 0.7856 - val_loss: 0.5502 - val_categorical_accuracy: 0.7600\n",
      "Epoch 60/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 1.3513 - categorical_accuracy: 0.7111 - val_loss: 0.6883 - val_categorical_accuracy: 0.7400\n",
      "Epoch 61/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5293 - categorical_accuracy: 0.7922 - val_loss: 0.7661 - val_categorical_accuracy: 0.7700\n",
      "Epoch 62/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.4743 - categorical_accuracy: 0.8044 - val_loss: 0.5373 - val_categorical_accuracy: 0.7800\n",
      "Epoch 63/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4920 - categorical_accuracy: 0.8033 - val_loss: 0.7779 - val_categorical_accuracy: 0.7500\n",
      "Epoch 64/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.4643 - categorical_accuracy: 0.8222 - val_loss: 0.6647 - val_categorical_accuracy: 0.7200\n",
      "Epoch 65/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.4401 - categorical_accuracy: 0.8211 - val_loss: 0.6089 - val_categorical_accuracy: 0.7800\n",
      "Epoch 66/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4830 - categorical_accuracy: 0.8189 - val_loss: 0.6264 - val_categorical_accuracy: 0.7000\n",
      "Epoch 67/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.4827 - categorical_accuracy: 0.8189 - val_loss: 0.4986 - val_categorical_accuracy: 0.8000\n",
      "Epoch 68/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4076 - categorical_accuracy: 0.8367 - val_loss: 0.7821 - val_categorical_accuracy: 0.7700\n",
      "Epoch 69/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4447 - categorical_accuracy: 0.8456 - val_loss: 0.7377 - val_categorical_accuracy: 0.7600\n",
      "Epoch 70/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4162 - categorical_accuracy: 0.8400 - val_loss: 0.4204 - val_categorical_accuracy: 0.8300\n",
      "Epoch 71/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.4225 - categorical_accuracy: 0.8489 - val_loss: 0.4463 - val_categorical_accuracy: 0.8200\n",
      "Epoch 72/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.3837 - categorical_accuracy: 0.8522 - val_loss: 0.4578 - val_categorical_accuracy: 0.8100\n",
      "Epoch 73/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.3973 - categorical_accuracy: 0.8456 - val_loss: 0.7897 - val_categorical_accuracy: 0.7900\n",
      "Epoch 74/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4274 - categorical_accuracy: 0.8333 - val_loss: 0.6381 - val_categorical_accuracy: 0.8400\n",
      "Epoch 75/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.4440 - categorical_accuracy: 0.8367 - val_loss: 0.4478 - val_categorical_accuracy: 0.8000\n",
      "Epoch 76/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.4029 - categorical_accuracy: 0.8478 - val_loss: 1.0218 - val_categorical_accuracy: 0.7800\n",
      "Epoch 77/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.4061 - categorical_accuracy: 0.8467 - val_loss: 0.4556 - val_categorical_accuracy: 0.8400\n",
      "Epoch 78/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.3990 - categorical_accuracy: 0.8667 - val_loss: 0.6682 - val_categorical_accuracy: 0.7800\n",
      "Epoch 79/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4762 - categorical_accuracy: 0.8311 - val_loss: 0.5302 - val_categorical_accuracy: 0.8200\n",
      "Epoch 80/100\n",
      "113/113 [==============================] - 24s 207ms/step - loss: 0.4235 - categorical_accuracy: 0.8533 - val_loss: 0.5581 - val_categorical_accuracy: 0.8300\n",
      "Epoch 81/100\n",
      "113/113 [==============================] - 24s 211ms/step - loss: 0.3490 - categorical_accuracy: 0.8656 - val_loss: 0.5929 - val_categorical_accuracy: 0.8500\n",
      "Epoch 82/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.3607 - categorical_accuracy: 0.8756 - val_loss: 0.8175 - val_categorical_accuracy: 0.8300\n",
      "Epoch 83/100\n",
      "113/113 [==============================] - 23s 204ms/step - loss: 0.3658 - categorical_accuracy: 0.8622 - val_loss: 0.6833 - val_categorical_accuracy: 0.7600\n",
      "Epoch 84/100\n",
      "113/113 [==============================] - 24s 207ms/step - loss: 0.4419 - categorical_accuracy: 0.8578 - val_loss: 0.5716 - val_categorical_accuracy: 0.8000\n",
      "Epoch 85/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4057 - categorical_accuracy: 0.8633 - val_loss: 0.6414 - val_categorical_accuracy: 0.8300\n",
      "Epoch 86/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.3805 - categorical_accuracy: 0.8667 - val_loss: 0.6642 - val_categorical_accuracy: 0.8200\n",
      "Epoch 87/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.3581 - categorical_accuracy: 0.8767 - val_loss: 0.6225 - val_categorical_accuracy: 0.8300\n",
      "Epoch 88/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.3782 - categorical_accuracy: 0.8822 - val_loss: 0.8586 - val_categorical_accuracy: 0.8000\n",
      "Epoch 89/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.3743 - categorical_accuracy: 0.8700 - val_loss: 0.6506 - val_categorical_accuracy: 0.8100\n",
      "Epoch 90/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.3427 - categorical_accuracy: 0.8844 - val_loss: 0.8370 - val_categorical_accuracy: 0.8200\n",
      "Epoch 91/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.3707 - categorical_accuracy: 0.8789 - val_loss: 1.0540 - val_categorical_accuracy: 0.8000\n",
      "Epoch 92/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.3418 - categorical_accuracy: 0.8878 - val_loss: 0.7773 - val_categorical_accuracy: 0.8400\n",
      "Epoch 93/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 911802.6250 - categorical_accuracy: 0.6722 - val_loss: 54981.2969 - val_categorical_accuracy: 0.4700\n",
      "Epoch 94/100\n",
      "113/113 [==============================] - 23s 204ms/step - loss: 2617947.2500 - categorical_accuracy: 0.5189 - val_loss: 1.5923 - val_categorical_accuracy: 0.5800\n",
      "Epoch 95/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 1.1653 - categorical_accuracy: 0.6789 - val_loss: 0.7488 - val_categorical_accuracy: 0.6400\n",
      "Epoch 96/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.5936 - categorical_accuracy: 0.7156 - val_loss: 0.7720 - val_categorical_accuracy: 0.7000\n",
      "Epoch 97/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5373 - categorical_accuracy: 0.8111 - val_loss: 0.8809 - val_categorical_accuracy: 0.7700\n",
      "Epoch 98/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.4939 - categorical_accuracy: 0.8367 - val_loss: 0.8290 - val_categorical_accuracy: 0.8200\n",
      "Epoch 99/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4613 - categorical_accuracy: 0.8244 - val_loss: 0.6227 - val_categorical_accuracy: 0.7900\n",
      "Epoch 100/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4922 - categorical_accuracy: 0.8333 - val_loss: 0.9010 - val_categorical_accuracy: 0.7900\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁▁▄▄▄▆▆▆▆▆▇▇▇▇▇▇▇▇▇█▇▇▆▆▇▇▇▇▇▇▇▇█████▂▇▇</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁███▇▇▇▆▆▅▅▄▄▃▃▂▂▁▁▁█</td></tr><tr><td>epoch/loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁▁▄▅▆▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▆▇▇▇████▇█▇█▃▇▇</td></tr><tr><td>epoch/val_loss</td><td>▅▇▃▂▃▂▂▂▂▁▁▂▃▄▂▅▁▄▂▂▂▁▄▂▃▂▁▃▁▂▁▁▃▂▂▂▃█▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.83333</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>0.49223</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.79</td></tr><tr><td>epoch/val_loss</td><td>0.90103</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM128_l2-Dense128-Dense256-LipsEyesHandsPose</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/aox13z1l' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/aox13z1l</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240410_151331-aox13z1l/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f82c4415190>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(128, return_sequences=False, activation='relu', input_shape=(CONFIG.VIDEO_LENGTH, NUM_FEATURES),\n",
    "               kernel_regularizer=l2(0.0000001), \n",
    "               activity_regularizer=l2(0.0000001)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", f\"LSTM128_l2-Dense128-Dense256-LipsEyesHandsPose_NF={NUM_FEATURES}.tf\"),\n",
    "            run_name=\"LSTM128_l2-Dense128-Dense256-LipsEyesHandsPose\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:g1jztv1q) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁</td></tr><tr><td>epoch/epoch</td><td>▁</td></tr><tr><td>epoch/learning_rate</td><td>▁</td></tr><tr><td>epoch/loss</td><td>▁</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁</td></tr><tr><td>epoch/val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.36889</td></tr><tr><td>epoch/epoch</td><td>0</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>2.82214</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.45</td></tr><tr><td>epoch/val_loss</td><td>2.39018</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM64-Dense128-Dense256-allfeatures_bigger_reg</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/g1jztv1q' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/g1jztv1q</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240408_085721-g1jztv1q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:g1jztv1q). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae32d08edf64e04a8009a32b4a48b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113070862160788, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240408_085904-b2cvthn0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/b2cvthn0' target=\"_blank\">LSTM64-Dense128-Dense256-allfeatures_bigger_reg</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/b2cvthn0' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/b2cvthn0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 2.8411 - categorical_accuracy: 0.4000INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 60s 499ms/step - loss: 2.8411 - categorical_accuracy: 0.4000 - val_loss: 2.4088 - val_categorical_accuracy: 0.4200\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 2.1274 - categorical_accuracy: 0.4056INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 502ms/step - loss: 2.1274 - categorical_accuracy: 0.4056 - val_loss: 1.8624 - val_categorical_accuracy: 0.4300\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - 49s 439ms/step - loss: 1.7043 - categorical_accuracy: 0.4044 - val_loss: 1.5451 - val_categorical_accuracy: 0.4300\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 1.4585 - categorical_accuracy: 0.4056 - val_loss: 1.3563 - val_categorical_accuracy: 0.4400\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - 50s 441ms/step - loss: 1.3106 - categorical_accuracy: 0.4056 - val_loss: 1.2412 - val_categorical_accuracy: 0.4300\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.2187 - categorical_accuracy: 0.4167INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 503ms/step - loss: 1.2187 - categorical_accuracy: 0.4167 - val_loss: 1.1653 - val_categorical_accuracy: 0.4200\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.1668 - categorical_accuracy: 0.4211INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 505ms/step - loss: 1.1668 - categorical_accuracy: 0.4211 - val_loss: 1.0656 - val_categorical_accuracy: 0.5000\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - 50s 442ms/step - loss: 1.1402 - categorical_accuracy: 0.4167 - val_loss: 1.0608 - val_categorical_accuracy: 0.5100\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0638 - categorical_accuracy: 0.4833INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 505ms/step - loss: 1.0638 - categorical_accuracy: 0.4833 - val_loss: 0.9921 - val_categorical_accuracy: 0.5600\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0240 - categorical_accuracy: 0.4889INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 500ms/step - loss: 1.0240 - categorical_accuracy: 0.4889 - val_loss: 1.0163 - val_categorical_accuracy: 0.5200\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0002 - categorical_accuracy: 0.5100INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 512ms/step - loss: 1.0002 - categorical_accuracy: 0.5100 - val_loss: 0.9776 - val_categorical_accuracy: 0.5200\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.9623 - categorical_accuracy: 0.5144INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 512ms/step - loss: 0.9623 - categorical_accuracy: 0.5144 - val_loss: 0.9214 - val_categorical_accuracy: 0.5500\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.9209 - categorical_accuracy: 0.5544INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 505ms/step - loss: 0.9209 - categorical_accuracy: 0.5544 - val_loss: 0.8942 - val_categorical_accuracy: 0.5600\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.9315 - categorical_accuracy: 0.5333 - val_loss: 0.9100 - val_categorical_accuracy: 0.5400\n",
      "Epoch 15/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8799 - categorical_accuracy: 0.5733INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 512ms/step - loss: 0.8799 - categorical_accuracy: 0.5733 - val_loss: 0.8732 - val_categorical_accuracy: 0.5500\n",
      "Epoch 16/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.8731 - categorical_accuracy: 0.5589 - val_loss: 0.8565 - val_categorical_accuracy: 0.5900\n",
      "Epoch 17/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.8757 - categorical_accuracy: 0.5556 - val_loss: 0.8595 - val_categorical_accuracy: 0.5800\n",
      "Epoch 18/100\n",
      "113/113 [==============================] - 50s 444ms/step - loss: 0.8616 - categorical_accuracy: 0.5733 - val_loss: 0.8362 - val_categorical_accuracy: 0.6100\n",
      "Epoch 19/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8751 - categorical_accuracy: 0.5800INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 505ms/step - loss: 0.8751 - categorical_accuracy: 0.5800 - val_loss: 0.8408 - val_categorical_accuracy: 0.5700\n",
      "Epoch 20/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8239 - categorical_accuracy: 0.6022INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 56s 502ms/step - loss: 0.8239 - categorical_accuracy: 0.6022 - val_loss: 0.8111 - val_categorical_accuracy: 0.6000\n",
      "Epoch 21/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8207 - categorical_accuracy: 0.6067INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 511ms/step - loss: 0.8207 - categorical_accuracy: 0.6067 - val_loss: 0.8012 - val_categorical_accuracy: 0.6100\n",
      "Epoch 22/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7945 - categorical_accuracy: 0.6511INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 510ms/step - loss: 0.7945 - categorical_accuracy: 0.6511 - val_loss: 0.7536 - val_categorical_accuracy: 0.6500\n",
      "Epoch 23/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.7951 - categorical_accuracy: 0.6400 - val_loss: 0.9351 - val_categorical_accuracy: 0.6400\n",
      "Epoch 24/100\n",
      "113/113 [==============================] - 50s 449ms/step - loss: 0.8354 - categorical_accuracy: 0.6267 - val_loss: 1.3834 - val_categorical_accuracy: 0.3300\n",
      "Epoch 25/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7755 - categorical_accuracy: 0.6633INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 56s 499ms/step - loss: 0.7755 - categorical_accuracy: 0.6633 - val_loss: 0.7559 - val_categorical_accuracy: 0.6800\n",
      "Epoch 26/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7431 - categorical_accuracy: 0.7156INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 515ms/step - loss: 0.7431 - categorical_accuracy: 0.7156 - val_loss: 0.8453 - val_categorical_accuracy: 0.5700\n",
      "Epoch 27/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7037 - categorical_accuracy: 0.7278INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 56s 502ms/step - loss: 0.7037 - categorical_accuracy: 0.7278 - val_loss: 0.7106 - val_categorical_accuracy: 0.7100\n",
      "Epoch 28/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6386 - categorical_accuracy: 0.7778INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 502ms/step - loss: 0.6386 - categorical_accuracy: 0.7778 - val_loss: 0.7482 - val_categorical_accuracy: 0.6600\n",
      "Epoch 29/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.6493 - categorical_accuracy: 0.7622 - val_loss: 0.5469 - val_categorical_accuracy: 0.8400\n",
      "Epoch 30/100\n",
      "113/113 [==============================] - 50s 441ms/step - loss: 0.6398 - categorical_accuracy: 0.7711 - val_loss: 0.5749 - val_categorical_accuracy: 0.7700\n",
      "Epoch 31/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6062 - categorical_accuracy: 0.7833INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 508ms/step - loss: 0.6062 - categorical_accuracy: 0.7833 - val_loss: 0.7759 - val_categorical_accuracy: 0.6900\n",
      "Epoch 32/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5734 - categorical_accuracy: 0.8022INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 510ms/step - loss: 0.5734 - categorical_accuracy: 0.8022 - val_loss: 0.6057 - val_categorical_accuracy: 0.7800\n",
      "Epoch 33/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5675 - categorical_accuracy: 0.8044INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 506ms/step - loss: 0.5675 - categorical_accuracy: 0.8044 - val_loss: 0.7343 - val_categorical_accuracy: 0.7800\n",
      "Epoch 34/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.5910 - categorical_accuracy: 0.7911 - val_loss: 0.5328 - val_categorical_accuracy: 0.8200\n",
      "Epoch 35/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.6100 - categorical_accuracy: 0.7944 - val_loss: 0.9632 - val_categorical_accuracy: 0.7100\n",
      "Epoch 36/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.6669 - categorical_accuracy: 0.7811 - val_loss: 0.5713 - val_categorical_accuracy: 0.8100\n",
      "Epoch 37/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.5737 - categorical_accuracy: 0.8033 - val_loss: 0.8288 - val_categorical_accuracy: 0.6900\n",
      "Epoch 38/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5330 - categorical_accuracy: 0.8344INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 510ms/step - loss: 0.5330 - categorical_accuracy: 0.8344 - val_loss: 0.8726 - val_categorical_accuracy: 0.6300\n",
      "Epoch 39/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.5579 - categorical_accuracy: 0.8111 - val_loss: 0.7435 - val_categorical_accuracy: 0.7500\n",
      "Epoch 40/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.5458 - categorical_accuracy: 0.8233 - val_loss: 0.8340 - val_categorical_accuracy: 0.7000\n",
      "Epoch 41/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.6143 - categorical_accuracy: 0.7856 - val_loss: 0.7396 - val_categorical_accuracy: 0.7700\n",
      "Epoch 42/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.5929 - categorical_accuracy: 0.7967 - val_loss: 0.7475 - val_categorical_accuracy: 0.6900\n",
      "Epoch 43/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.5422 - categorical_accuracy: 0.8333 - val_loss: 0.6000 - val_categorical_accuracy: 0.8200\n",
      "Epoch 44/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4798 - categorical_accuracy: 0.8567INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 507ms/step - loss: 0.4798 - categorical_accuracy: 0.8567 - val_loss: 0.6378 - val_categorical_accuracy: 0.7800\n",
      "Epoch 45/100\n",
      "113/113 [==============================] - 49s 438ms/step - loss: 0.5517 - categorical_accuracy: 0.8100 - val_loss: 0.6610 - val_categorical_accuracy: 0.8000\n",
      "Epoch 46/100\n",
      "113/113 [==============================] - 50s 444ms/step - loss: 0.5207 - categorical_accuracy: 0.8422 - val_loss: 0.5422 - val_categorical_accuracy: 0.8600\n",
      "Epoch 47/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.5088 - categorical_accuracy: 0.8489 - val_loss: 0.5806 - val_categorical_accuracy: 0.7900\n",
      "Epoch 48/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.5605 - categorical_accuracy: 0.8167 - val_loss: 0.4904 - val_categorical_accuracy: 0.8400\n",
      "Epoch 49/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.5473 - categorical_accuracy: 0.8278 - val_loss: 0.6565 - val_categorical_accuracy: 0.7600\n",
      "Epoch 50/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.5422 - categorical_accuracy: 0.8378 - val_loss: 0.6482 - val_categorical_accuracy: 0.8100\n",
      "Epoch 51/100\n",
      "113/113 [==============================] - 50s 444ms/step - loss: 0.4920 - categorical_accuracy: 0.8533 - val_loss: 0.9195 - val_categorical_accuracy: 0.7200\n",
      "Epoch 52/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.5176 - categorical_accuracy: 0.8456 - val_loss: 0.4050 - val_categorical_accuracy: 0.8900\n",
      "Epoch 53/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.5224 - categorical_accuracy: 0.8367 - val_loss: 0.6238 - val_categorical_accuracy: 0.8300\n",
      "Epoch 54/100\n",
      "113/113 [==============================] - 51s 446ms/step - loss: 0.4998 - categorical_accuracy: 0.8511 - val_loss: 0.7593 - val_categorical_accuracy: 0.6900\n",
      "Epoch 55/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4259 - categorical_accuracy: 0.8800INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 505ms/step - loss: 0.4259 - categorical_accuracy: 0.8800 - val_loss: 0.6574 - val_categorical_accuracy: 0.7800\n",
      "Epoch 56/100\n",
      "113/113 [==============================] - 51s 446ms/step - loss: 0.5203 - categorical_accuracy: 0.8444 - val_loss: 0.5315 - val_categorical_accuracy: 0.8300\n",
      "Epoch 57/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.5094 - categorical_accuracy: 0.8478 - val_loss: 0.6249 - val_categorical_accuracy: 0.8100\n",
      "Epoch 58/100\n",
      "113/113 [==============================] - 51s 447ms/step - loss: 0.5697 - categorical_accuracy: 0.8200 - val_loss: 0.4367 - val_categorical_accuracy: 0.9000\n",
      "Epoch 59/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.4575 - categorical_accuracy: 0.8767 - val_loss: 0.6292 - val_categorical_accuracy: 0.8000\n",
      "Epoch 65/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.4987 - categorical_accuracy: 0.8511 - val_loss: 0.5974 - val_categorical_accuracy: 0.8300\n",
      "Epoch 66/100\n",
      "113/113 [==============================] - 50s 448ms/step - loss: 0.5852 - categorical_accuracy: 0.8289 - val_loss: 0.6959 - val_categorical_accuracy: 0.7500\n",
      "Epoch 67/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.5027 - categorical_accuracy: 0.8533 - val_loss: 0.5093 - val_categorical_accuracy: 0.8500\n",
      "Epoch 68/100\n",
      "113/113 [==============================] - 50s 441ms/step - loss: 0.4957 - categorical_accuracy: 0.8622 - val_loss: 0.4846 - val_categorical_accuracy: 0.8500\n",
      "Epoch 69/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.5055 - categorical_accuracy: 0.8533 - val_loss: 0.5980 - val_categorical_accuracy: 0.8600\n",
      "Epoch 70/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.5464 - categorical_accuracy: 0.8367 - val_loss: 0.5365 - val_categorical_accuracy: 0.8400\n",
      "Epoch 71/100\n",
      "113/113 [==============================] - 50s 448ms/step - loss: 0.4602 - categorical_accuracy: 0.8689 - val_loss: 0.4441 - val_categorical_accuracy: 0.8800\n",
      "Epoch 72/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.5463 - categorical_accuracy: 0.8189 - val_loss: 0.5223 - val_categorical_accuracy: 0.8000\n",
      "Epoch 73/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4312 - categorical_accuracy: 0.8878INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 513ms/step - loss: 0.4312 - categorical_accuracy: 0.8878 - val_loss: 0.4676 - val_categorical_accuracy: 0.8600\n",
      "Epoch 74/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.4691 - categorical_accuracy: 0.8589 - val_loss: 0.4604 - val_categorical_accuracy: 0.8700\n",
      "Epoch 75/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.4864 - categorical_accuracy: 0.8567 - val_loss: 0.4465 - val_categorical_accuracy: 0.9000\n",
      "Epoch 76/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.4778 - categorical_accuracy: 0.8678 - val_loss: 0.4316 - val_categorical_accuracy: 0.8800\n",
      "Epoch 77/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.4969 - categorical_accuracy: 0.8444 - val_loss: 0.4481 - val_categorical_accuracy: 0.8800\n",
      "Epoch 78/100\n",
      "113/113 [==============================] - 50s 448ms/step - loss: 0.4584 - categorical_accuracy: 0.8644 - val_loss: 0.4431 - val_categorical_accuracy: 0.9100\n",
      "Epoch 79/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4070 - categorical_accuracy: 0.8900INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 505ms/step - loss: 0.4070 - categorical_accuracy: 0.8900 - val_loss: 0.4321 - val_categorical_accuracy: 0.8800\n",
      "Epoch 80/100\n",
      "113/113 [==============================] - 50s 442ms/step - loss: 0.4880 - categorical_accuracy: 0.8633 - val_loss: 0.4699 - val_categorical_accuracy: 0.8800\n",
      "Epoch 81/100\n",
      "113/113 [==============================] - 50s 439ms/step - loss: 0.4324 - categorical_accuracy: 0.8822 - val_loss: 0.4674 - val_categorical_accuracy: 0.8600\n",
      "Epoch 82/100\n",
      "113/113 [==============================] - 50s 439ms/step - loss: 0.4607 - categorical_accuracy: 0.8722 - val_loss: 0.4195 - val_categorical_accuracy: 0.9000\n",
      "Epoch 83/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.5813 - categorical_accuracy: 0.8144 - val_loss: 0.7107 - val_categorical_accuracy: 0.8300\n",
      "Epoch 84/100\n",
      "113/113 [==============================] - 50s 440ms/step - loss: 0.5578 - categorical_accuracy: 0.8311 - val_loss: 0.4452 - val_categorical_accuracy: 0.8800\n",
      "Epoch 85/100\n",
      "113/113 [==============================] - 50s 439ms/step - loss: 0.4656 - categorical_accuracy: 0.8722 - val_loss: 0.4972 - val_categorical_accuracy: 0.8600\n",
      "Epoch 86/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.4155 - categorical_accuracy: 0.8800 - val_loss: 0.7409 - val_categorical_accuracy: 0.8000\n",
      "Epoch 87/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.5258 - categorical_accuracy: 0.8367 - val_loss: 0.4980 - val_categorical_accuracy: 0.8300\n",
      "Epoch 88/100\n",
      "113/113 [==============================] - 50s 442ms/step - loss: 0.4853 - categorical_accuracy: 0.8689 - val_loss: 0.5915 - val_categorical_accuracy: 0.8200\n",
      "Epoch 89/100\n",
      "113/113 [==============================] - 50s 439ms/step - loss: 0.4376 - categorical_accuracy: 0.8811 - val_loss: 0.4558 - val_categorical_accuracy: 0.8700\n",
      "Epoch 90/100\n",
      "113/113 [==============================] - 50s 441ms/step - loss: 0.4365 - categorical_accuracy: 0.8822 - val_loss: 0.5814 - val_categorical_accuracy: 0.8300\n",
      "Epoch 91/100\n",
      "113/113 [==============================] - 50s 440ms/step - loss: 0.5081 - categorical_accuracy: 0.8378 - val_loss: 0.7659 - val_categorical_accuracy: 0.7900\n",
      "Epoch 92/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.4477 - categorical_accuracy: 0.8767 - val_loss: 0.4377 - val_categorical_accuracy: 0.8800\n",
      "Epoch 93/100\n",
      "113/113 [==============================] - 50s 438ms/step - loss: 0.4153 - categorical_accuracy: 0.8900 - val_loss: 0.3688 - val_categorical_accuracy: 0.9100\n",
      "Epoch 94/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4063 - categorical_accuracy: 0.8922INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 56s 498ms/step - loss: 0.4063 - categorical_accuracy: 0.8922 - val_loss: 0.4536 - val_categorical_accuracy: 0.8800\n",
      "Epoch 95/100\n",
      "113/113 [==============================] - 51s 444ms/step - loss: 0.4751 - categorical_accuracy: 0.8656 - val_loss: 0.5127 - val_categorical_accuracy: 0.8400\n",
      "Epoch 96/100\n",
      "113/113 [==============================] - 50s 442ms/step - loss: 0.4397 - categorical_accuracy: 0.8767 - val_loss: 0.4737 - val_categorical_accuracy: 0.8700\n",
      "Epoch 97/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.4376 - categorical_accuracy: 0.8722 - val_loss: 0.4013 - val_categorical_accuracy: 0.8800\n",
      "Epoch 98/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.4417 - categorical_accuracy: 0.8722 - val_loss: 0.4582 - val_categorical_accuracy: 0.8600\n",
      "Epoch 99/100\n",
      "113/113 [==============================] - 50s 441ms/step - loss: 0.4192 - categorical_accuracy: 0.8867 - val_loss: 0.4077 - val_categorical_accuracy: 0.9000\n",
      "Epoch 100/100\n",
      "113/113 [==============================] - 50s 444ms/step - loss: 0.5048 - categorical_accuracy: 0.8467 - val_loss: 0.6696 - val_categorical_accuracy: 0.8000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁▁▁▁▃▃▃▃▄▄▅▆▆▇▆▇▆▇▇▇▇▇▇█▇█▇▇▇█▇██▇▇████▇</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▅▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁▁▁▂▂▃▃▄▄▄▃▅▅▇▇▆▆▆▇▆▅▅▇▇▇▇▇▇▇█████▇████▇</td></tr><tr><td>epoch/val_loss</td><td>█▅▄▃▃▃▃▃▂▃▃▂▂▁▂▂▂▂▁▂▃▂▁▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.84667</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>0.50475</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.8</td></tr><tr><td>epoch/val_loss</td><td>0.66957</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM64-Dense128-Dense256-allfeatures_bigger_reg</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/b2cvthn0' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/b2cvthn0</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240408_085904-b2cvthn0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fda6bc02690>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(64, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, NUM_FEATURES),\n",
    "               kernel_regularizer=l2(0.005), \n",
    "               activity_regularizer=l2(0.005)))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", f\"LSTM-L64-D128-D256-reg=0.005-LipsEyesHandsPose_NF={NUM_FEATURES}.tf\"),\n",
    "            run_name=\"LSTM64-Dense128-Dense256-LipsEyesHandsPose_bigger_reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240410_160411-6k7673ys</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/6k7673ys' target=\"_blank\">LSTM128-Dense128-Dense256-LipsEyesHandsPose</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/6k7673ys' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/6k7673ys</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.1307 - categorical_accuracy: 0.4056INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 33s 266ms/step - loss: 1.1307 - categorical_accuracy: 0.4056 - val_loss: 1.1032 - val_categorical_accuracy: 0.4300\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0914 - categorical_accuracy: 0.4178INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 262ms/step - loss: 1.0914 - categorical_accuracy: 0.4178 - val_loss: 1.0474 - val_categorical_accuracy: 0.4600\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0148 - categorical_accuracy: 0.4556INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 270ms/step - loss: 1.0148 - categorical_accuracy: 0.4556 - val_loss: 0.9097 - val_categorical_accuracy: 0.4900\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.9390 - categorical_accuracy: 0.5156INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 31s 272ms/step - loss: 0.9390 - categorical_accuracy: 0.5156 - val_loss: 0.8502 - val_categorical_accuracy: 0.5800\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.8808 - categorical_accuracy: 0.5533 - val_loss: 0.8318 - val_categorical_accuracy: 0.5500\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.8731 - categorical_accuracy: 0.5700 - val_loss: 0.8167 - val_categorical_accuracy: 0.5300\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.8465 - categorical_accuracy: 0.6033 - val_loss: 0.8999 - val_categorical_accuracy: 0.5600\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8403 - categorical_accuracy: 0.6333INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 268ms/step - loss: 0.8403 - categorical_accuracy: 0.6333 - val_loss: 0.8279 - val_categorical_accuracy: 0.6300\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.8400 - categorical_accuracy: 0.6144 - val_loss: 0.8482 - val_categorical_accuracy: 0.5000\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.8017 - categorical_accuracy: 0.6278 - val_loss: 0.8499 - val_categorical_accuracy: 0.5800\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.7902 - categorical_accuracy: 0.6422 - val_loss: 0.8072 - val_categorical_accuracy: 0.5900\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.7885 - categorical_accuracy: 0.6522 - val_loss: 0.8468 - val_categorical_accuracy: 0.6100\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.7751 - categorical_accuracy: 0.6400 - val_loss: 0.8329 - val_categorical_accuracy: 0.6100\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.7420 - categorical_accuracy: 0.6633 - val_loss: 0.8273 - val_categorical_accuracy: 0.6000\n",
      "Epoch 15/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7364 - categorical_accuracy: 0.6767INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 269ms/step - loss: 0.7364 - categorical_accuracy: 0.6767 - val_loss: 0.7561 - val_categorical_accuracy: 0.7000\n",
      "Epoch 16/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.7069 - categorical_accuracy: 0.7133 - val_loss: 0.7279 - val_categorical_accuracy: 0.6900\n",
      "Epoch 17/100\n",
      "113/113 [==============================] - 23s 209ms/step - loss: 0.6928 - categorical_accuracy: 0.7389 - val_loss: 0.7527 - val_categorical_accuracy: 0.6700\n",
      "Epoch 18/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.6610 - categorical_accuracy: 0.7456 - val_loss: 0.7435 - val_categorical_accuracy: 0.6600\n",
      "Epoch 19/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.6866 - categorical_accuracy: 0.7211 - val_loss: 0.7779 - val_categorical_accuracy: 0.6800\n",
      "Epoch 20/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6555 - categorical_accuracy: 0.7378INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 31s 272ms/step - loss: 0.6555 - categorical_accuracy: 0.7378 - val_loss: 0.6665 - val_categorical_accuracy: 0.7300\n",
      "Epoch 21/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.6601 - categorical_accuracy: 0.7333 - val_loss: 0.7521 - val_categorical_accuracy: 0.6800\n",
      "Epoch 22/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6090 - categorical_accuracy: 0.7600INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 266ms/step - loss: 0.6090 - categorical_accuracy: 0.7600 - val_loss: 0.5828 - val_categorical_accuracy: 0.7600\n",
      "Epoch 23/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.5865 - categorical_accuracy: 0.7789 - val_loss: 0.6936 - val_categorical_accuracy: 0.7200\n",
      "Epoch 24/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5992 - categorical_accuracy: 0.7744INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 268ms/step - loss: 0.5992 - categorical_accuracy: 0.7744 - val_loss: 0.6043 - val_categorical_accuracy: 0.8300\n",
      "Epoch 25/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5837 - categorical_accuracy: 0.7711 - val_loss: 0.6273 - val_categorical_accuracy: 0.7500\n",
      "Epoch 26/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5530 - categorical_accuracy: 0.7778 - val_loss: 0.5340 - val_categorical_accuracy: 0.8300\n",
      "Epoch 27/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5401 - categorical_accuracy: 0.7878 - val_loss: 0.5620 - val_categorical_accuracy: 0.8000\n",
      "Epoch 28/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.5401 - categorical_accuracy: 0.7956 - val_loss: 0.5923 - val_categorical_accuracy: 0.8100\n",
      "Epoch 29/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.5443 - categorical_accuracy: 0.7911 - val_loss: 0.6617 - val_categorical_accuracy: 0.7300\n",
      "Epoch 30/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.4918 - categorical_accuracy: 0.8322 - val_loss: 0.5425 - val_categorical_accuracy: 0.8100\n",
      "Epoch 31/100\n",
      "113/113 [==============================] - 24s 211ms/step - loss: 0.5158 - categorical_accuracy: 0.8067 - val_loss: 0.6740 - val_categorical_accuracy: 0.7800\n",
      "Epoch 32/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.5107 - categorical_accuracy: 0.8156 - val_loss: 0.5640 - val_categorical_accuracy: 0.7800\n",
      "Epoch 33/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5110 - categorical_accuracy: 0.8100 - val_loss: 0.6546 - val_categorical_accuracy: 0.7900\n",
      "Epoch 34/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5298 - categorical_accuracy: 0.8100 - val_loss: 0.5047 - val_categorical_accuracy: 0.8300\n",
      "Epoch 35/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4967 - categorical_accuracy: 0.8222 - val_loss: 0.5906 - val_categorical_accuracy: 0.7700\n",
      "Epoch 36/100\n",
      "113/113 [==============================] - 24s 211ms/step - loss: 0.4867 - categorical_accuracy: 0.8278 - val_loss: 0.4864 - val_categorical_accuracy: 0.8300\n",
      "Epoch 37/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.4702 - categorical_accuracy: 0.8389 - val_loss: 0.6237 - val_categorical_accuracy: 0.8000\n",
      "Epoch 38/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5253 - categorical_accuracy: 0.8211INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 268ms/step - loss: 0.5253 - categorical_accuracy: 0.8211 - val_loss: 0.5099 - val_categorical_accuracy: 0.8400\n",
      "Epoch 39/100\n",
      "113/113 [==============================] - 23s 204ms/step - loss: 0.4600 - categorical_accuracy: 0.8444 - val_loss: 0.5245 - val_categorical_accuracy: 0.8300\n",
      "Epoch 40/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.4927 - categorical_accuracy: 0.8300 - val_loss: 0.5457 - val_categorical_accuracy: 0.8200\n",
      "Epoch 41/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.5046 - categorical_accuracy: 0.8156 - val_loss: 0.5558 - val_categorical_accuracy: 0.8300\n",
      "Epoch 42/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.4672 - categorical_accuracy: 0.8444 - val_loss: 0.5653 - val_categorical_accuracy: 0.8100\n",
      "Epoch 43/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.4642 - categorical_accuracy: 0.8522 - val_loss: 0.5836 - val_categorical_accuracy: 0.7900\n",
      "Epoch 44/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4691 - categorical_accuracy: 0.8467INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 31s 273ms/step - loss: 0.4691 - categorical_accuracy: 0.8467 - val_loss: 0.5068 - val_categorical_accuracy: 0.8600\n",
      "Epoch 45/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.4806 - categorical_accuracy: 0.8356 - val_loss: 0.5092 - val_categorical_accuracy: 0.7900\n",
      "Epoch 46/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.4650 - categorical_accuracy: 0.8322 - val_loss: 0.5216 - val_categorical_accuracy: 0.8200\n",
      "Epoch 47/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4609 - categorical_accuracy: 0.8378 - val_loss: 0.5566 - val_categorical_accuracy: 0.8200\n",
      "Epoch 48/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.4333 - categorical_accuracy: 0.8489 - val_loss: 0.5629 - val_categorical_accuracy: 0.8200\n",
      "Epoch 49/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.4598 - categorical_accuracy: 0.8411 - val_loss: 0.4579 - val_categorical_accuracy: 0.8600\n",
      "Epoch 50/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5398 - categorical_accuracy: 0.8000 - val_loss: 0.5638 - val_categorical_accuracy: 0.8100\n",
      "Epoch 51/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5571 - categorical_accuracy: 0.7844 - val_loss: 0.5621 - val_categorical_accuracy: 0.7800\n",
      "Epoch 52/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5308 - categorical_accuracy: 0.8022 - val_loss: 0.6234 - val_categorical_accuracy: 0.7800\n",
      "Epoch 53/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.4871 - categorical_accuracy: 0.8289 - val_loss: 0.4680 - val_categorical_accuracy: 0.8500\n",
      "Epoch 54/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5054 - categorical_accuracy: 0.8311 - val_loss: 0.5801 - val_categorical_accuracy: 0.7600\n",
      "Epoch 55/100\n",
      "113/113 [==============================] - 24s 212ms/step - loss: 0.5223 - categorical_accuracy: 0.8133 - val_loss: 0.4519 - val_categorical_accuracy: 0.8400\n",
      "Epoch 56/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5003 - categorical_accuracy: 0.8400 - val_loss: 0.6782 - val_categorical_accuracy: 0.7700\n",
      "Epoch 57/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.5455 - categorical_accuracy: 0.8133 - val_loss: 0.6643 - val_categorical_accuracy: 0.7900\n",
      "Epoch 58/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5580 - categorical_accuracy: 0.8033 - val_loss: 0.4241 - val_categorical_accuracy: 0.8500\n",
      "Epoch 59/100\n",
      "113/113 [==============================] - 24s 211ms/step - loss: 0.5448 - categorical_accuracy: 0.8211 - val_loss: 0.5165 - val_categorical_accuracy: 0.8100\n",
      "Epoch 60/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.5126 - categorical_accuracy: 0.8189 - val_loss: 0.6354 - val_categorical_accuracy: 0.7700\n",
      "Epoch 61/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5154 - categorical_accuracy: 0.8256 - val_loss: 0.5706 - val_categorical_accuracy: 0.8100\n",
      "Epoch 62/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.4838 - categorical_accuracy: 0.8278 - val_loss: 0.7414 - val_categorical_accuracy: 0.8000\n",
      "Epoch 63/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.5194 - categorical_accuracy: 0.8044 - val_loss: 0.4854 - val_categorical_accuracy: 0.8000\n",
      "Epoch 64/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.5784 - categorical_accuracy: 0.7867 - val_loss: 0.7351 - val_categorical_accuracy: 0.6900\n",
      "Epoch 65/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.4953 - categorical_accuracy: 0.8278 - val_loss: 0.6401 - val_categorical_accuracy: 0.8100\n",
      "Epoch 66/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4988 - categorical_accuracy: 0.8278 - val_loss: 0.5036 - val_categorical_accuracy: 0.7900\n",
      "Epoch 67/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.4995 - categorical_accuracy: 0.8278 - val_loss: 0.5005 - val_categorical_accuracy: 0.8100\n",
      "Epoch 68/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4567 - categorical_accuracy: 0.8367 - val_loss: 0.5868 - val_categorical_accuracy: 0.7600\n",
      "Epoch 69/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.4675 - categorical_accuracy: 0.8344 - val_loss: 0.5526 - val_categorical_accuracy: 0.8200\n",
      "Epoch 70/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.4358 - categorical_accuracy: 0.8422 - val_loss: 0.5350 - val_categorical_accuracy: 0.8400\n",
      "Epoch 71/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4327 - categorical_accuracy: 0.8533 - val_loss: 0.4508 - val_categorical_accuracy: 0.8400\n",
      "Epoch 72/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5273 - categorical_accuracy: 0.8178INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 267ms/step - loss: 0.5273 - categorical_accuracy: 0.8178 - val_loss: 0.4283 - val_categorical_accuracy: 0.8700\n",
      "Epoch 73/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.4923 - categorical_accuracy: 0.8300 - val_loss: 0.5173 - val_categorical_accuracy: 0.8100\n",
      "Epoch 74/100\n",
      "113/113 [==============================] - 23s 202ms/step - loss: 0.4628 - categorical_accuracy: 0.8444 - val_loss: 0.5027 - val_categorical_accuracy: 0.8300\n",
      "Epoch 75/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.4506 - categorical_accuracy: 0.8478 - val_loss: 0.4754 - val_categorical_accuracy: 0.8400\n",
      "Epoch 76/100\n",
      "113/113 [==============================] - 23s 203ms/step - loss: 0.4279 - categorical_accuracy: 0.8633 - val_loss: 0.4837 - val_categorical_accuracy: 0.8200\n",
      "Epoch 77/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.4810 - categorical_accuracy: 0.8400 - val_loss: 0.4411 - val_categorical_accuracy: 0.8600\n",
      "Epoch 78/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4252 - categorical_accuracy: 0.8633 - val_loss: 0.5249 - val_categorical_accuracy: 0.8200\n",
      "Epoch 79/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.4137 - categorical_accuracy: 0.8678 - val_loss: 0.5127 - val_categorical_accuracy: 0.7900\n",
      "Epoch 80/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.4445 - categorical_accuracy: 0.8456 - val_loss: 0.4384 - val_categorical_accuracy: 0.8400\n",
      "Epoch 81/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.4014 - categorical_accuracy: 0.8622 - val_loss: 0.4576 - val_categorical_accuracy: 0.8500\n",
      "Epoch 82/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4254 - categorical_accuracy: 0.8511 - val_loss: 0.5440 - val_categorical_accuracy: 0.8000\n",
      "Epoch 83/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.3919 - categorical_accuracy: 0.8678 - val_loss: 0.4706 - val_categorical_accuracy: 0.8200\n",
      "Epoch 84/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.3592 - categorical_accuracy: 0.8800 - val_loss: 0.4698 - val_categorical_accuracy: 0.8200\n",
      "Epoch 85/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.3646 - categorical_accuracy: 0.8733 - val_loss: 0.6176 - val_categorical_accuracy: 0.8300\n",
      "Epoch 86/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.3935 - categorical_accuracy: 0.8733 - val_loss: 0.5271 - val_categorical_accuracy: 0.8200\n",
      "Epoch 87/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.3491 - categorical_accuracy: 0.8822 - val_loss: 0.5945 - val_categorical_accuracy: 0.7800\n",
      "Epoch 88/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.3527 - categorical_accuracy: 0.8789 - val_loss: 0.5093 - val_categorical_accuracy: 0.8400\n",
      "Epoch 89/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.3687 - categorical_accuracy: 0.8756 - val_loss: 0.5842 - val_categorical_accuracy: 0.8000\n",
      "Epoch 90/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.3878 - categorical_accuracy: 0.8722 - val_loss: 0.4388 - val_categorical_accuracy: 0.8400\n",
      "Epoch 91/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.3506 - categorical_accuracy: 0.8944 - val_loss: 0.4912 - val_categorical_accuracy: 0.8300\n",
      "Epoch 92/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.3416 - categorical_accuracy: 0.8900 - val_loss: 0.4668 - val_categorical_accuracy: 0.8500\n",
      "Epoch 93/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.3565 - categorical_accuracy: 0.8856 - val_loss: 0.4997 - val_categorical_accuracy: 0.8300\n",
      "Epoch 94/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.3583 - categorical_accuracy: 0.8867 - val_loss: 0.5086 - val_categorical_accuracy: 0.8400\n",
      "Epoch 95/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.3522 - categorical_accuracy: 0.8878 - val_loss: 0.4539 - val_categorical_accuracy: 0.8400\n",
      "Epoch 96/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.3437 - categorical_accuracy: 0.8989 - val_loss: 0.4863 - val_categorical_accuracy: 0.8300\n",
      "Epoch 97/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.3276 - categorical_accuracy: 0.8944 - val_loss: 0.4572 - val_categorical_accuracy: 0.8600\n",
      "Epoch 98/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.3703 - categorical_accuracy: 0.8767 - val_loss: 0.4492 - val_categorical_accuracy: 0.8400\n",
      "Epoch 99/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.3175 - categorical_accuracy: 0.9044 - val_loss: 0.4876 - val_categorical_accuracy: 0.8300\n",
      "Epoch 100/100\n",
      "113/113 [==============================] - 24s 212ms/step - loss: 0.3906 - categorical_accuracy: 0.8767 - val_loss: 0.5290 - val_categorical_accuracy: 0.8100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁▂▃▄▄▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇▆▇▇▇▇▆▇▇▇▇▇█▇███████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁███▇▇▇▆▆▅▅▄▄▃▃▂▂▁▁▁█</td></tr><tr><td>epoch/loss</td><td>█▇▆▅▅▅▄▄▄▃▃▃▃▃▂▂▃▂▂▂▃▃▃▃▃▃▂▂▃▂▂▂▂▁▁▁▁▁▁▂</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁▂▃▄▄▄▅▅▅▆▇▇▇▇▇▇▇█▇█▇▆▆▇▇▅▇▇█▇█▇▇▇▇▇███▇</td></tr><tr><td>epoch/val_loss</td><td>█▆▅▅▅▅▄▄▄▄▂▃▄▂▂▂▂▂▂▁▂▃▄▂▂▄▂▂▁▂▁▂▂▁▃▃▁▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.87667</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>0.39061</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.81</td></tr><tr><td>epoch/val_loss</td><td>0.52896</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM128-Dense128-Dense256-LipsEyesHandsPose</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/6k7673ys' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/6k7673ys</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240410_160411-6k7673ys/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f83303cff90>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(128, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, NUM_FEATURES),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", f\"LSTM128-Dense128-Dense256-LipsEyesHandsPose_NF={NUM_FEATURES}.tf\"),\n",
    "            run_name=\"LSTM128-Dense128-Dense256-LipsEyesHandsPose\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240410_164453-gw9abece</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/gw9abece' target=\"_blank\">LSTM64-Dense64-LipsEyesHandsPose_NF</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/gw9abece' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/gw9abece</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(64, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, NUM_FEATURES),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", f\"LSTM64-Dense64-LipsEyesHandsPose_NF={NUM_FEATURES}.tf\"),\n",
    "            run_name=\"LSTM64-Dense64-LipsEyesHandsPose_NF\", T_max=75, epochs=100, \n",
    "            max_lr = 1e-4, min_lr = 2.5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240410_195902-mx4vk35b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/mx4vk35b' target=\"_blank\">LSTM64-Dense128-LipsEyesHandsPose</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/mx4vk35b' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/mx4vk35b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.1223 - categorical_accuracy: 0.4111INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 32s 262ms/step - loss: 1.1223 - categorical_accuracy: 0.4111 - val_loss: 1.1090 - val_categorical_accuracy: 0.4400\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.1013 - categorical_accuracy: 0.4267INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 31s 271ms/step - loss: 1.1013 - categorical_accuracy: 0.4267 - val_loss: 1.0793 - val_categorical_accuracy: 0.4500\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 1.0671 - categorical_accuracy: 0.4344 - val_loss: 1.0352 - val_categorical_accuracy: 0.4500\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0137 - categorical_accuracy: 0.4567INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 266ms/step - loss: 1.0137 - categorical_accuracy: 0.4567 - val_loss: 0.9434 - val_categorical_accuracy: 0.5000\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.9316 - categorical_accuracy: 0.5078INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 31s 272ms/step - loss: 0.9316 - categorical_accuracy: 0.5078 - val_loss: 0.8580 - val_categorical_accuracy: 0.5300\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.9218 - categorical_accuracy: 0.5289 - val_loss: 0.8802 - val_categorical_accuracy: 0.5300\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8936 - categorical_accuracy: 0.5356INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 265ms/step - loss: 0.8936 - categorical_accuracy: 0.5356 - val_loss: 0.8179 - val_categorical_accuracy: 0.5500\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8809 - categorical_accuracy: 0.5511INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 268ms/step - loss: 0.8809 - categorical_accuracy: 0.5511 - val_loss: 0.8355 - val_categorical_accuracy: 0.5800\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - 23s 204ms/step - loss: 0.8811 - categorical_accuracy: 0.5444 - val_loss: 0.8496 - val_categorical_accuracy: 0.5600\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.8466 - categorical_accuracy: 0.5622 - val_loss: 0.7968 - val_categorical_accuracy: 0.5700\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8256 - categorical_accuracy: 0.5811INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 266ms/step - loss: 0.8256 - categorical_accuracy: 0.5811 - val_loss: 0.7767 - val_categorical_accuracy: 0.6300\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.8198 - categorical_accuracy: 0.5933 - val_loss: 0.7587 - val_categorical_accuracy: 0.6300\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7998 - categorical_accuracy: 0.5933INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 265ms/step - loss: 0.7998 - categorical_accuracy: 0.5933 - val_loss: 0.7838 - val_categorical_accuracy: 0.6600\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.7724 - categorical_accuracy: 0.6556 - val_loss: 0.7679 - val_categorical_accuracy: 0.6300\n",
      "Epoch 15/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7644 - categorical_accuracy: 0.6711INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 270ms/step - loss: 0.7644 - categorical_accuracy: 0.6711 - val_loss: 0.7227 - val_categorical_accuracy: 0.7200\n",
      "Epoch 16/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7193 - categorical_accuracy: 0.7000INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 268ms/step - loss: 0.7193 - categorical_accuracy: 0.7000 - val_loss: 0.6893 - val_categorical_accuracy: 0.7400\n",
      "Epoch 17/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.7082 - categorical_accuracy: 0.7100 - val_loss: 0.6794 - val_categorical_accuracy: 0.7400\n",
      "Epoch 18/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6853 - categorical_accuracy: 0.7244INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 266ms/step - loss: 0.6853 - categorical_accuracy: 0.7244 - val_loss: 0.6449 - val_categorical_accuracy: 0.7500\n",
      "Epoch 19/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6643 - categorical_accuracy: 0.7256INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 266ms/step - loss: 0.6643 - categorical_accuracy: 0.7256 - val_loss: 0.6305 - val_categorical_accuracy: 0.7600\n",
      "Epoch 20/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6318 - categorical_accuracy: 0.7533INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 268ms/step - loss: 0.6318 - categorical_accuracy: 0.7533 - val_loss: 0.6225 - val_categorical_accuracy: 0.7700\n",
      "Epoch 21/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.6158 - categorical_accuracy: 0.7622 - val_loss: 0.6281 - val_categorical_accuracy: 0.7300\n",
      "Epoch 22/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.6122 - categorical_accuracy: 0.7567 - val_loss: 0.6684 - val_categorical_accuracy: 0.7200\n",
      "Epoch 23/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.6182 - categorical_accuracy: 0.7544 - val_loss: 0.6411 - val_categorical_accuracy: 0.7300\n",
      "Epoch 24/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.6149 - categorical_accuracy: 0.7589 - val_loss: 0.6185 - val_categorical_accuracy: 0.7700\n",
      "Epoch 25/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5649 - categorical_accuracy: 0.7856INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 267ms/step - loss: 0.5649 - categorical_accuracy: 0.7856 - val_loss: 0.5848 - val_categorical_accuracy: 0.7800\n",
      "Epoch 26/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.6272 - categorical_accuracy: 0.7611 - val_loss: 0.6810 - val_categorical_accuracy: 0.7700\n",
      "Epoch 27/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5782 - categorical_accuracy: 0.7822INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 31s 273ms/step - loss: 0.5782 - categorical_accuracy: 0.7822 - val_loss: 0.5376 - val_categorical_accuracy: 0.8000\n",
      "Epoch 28/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5962 - categorical_accuracy: 0.7633INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 266ms/step - loss: 0.5962 - categorical_accuracy: 0.7633 - val_loss: 0.5797 - val_categorical_accuracy: 0.8200\n",
      "Epoch 29/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5510 - categorical_accuracy: 0.7911 - val_loss: 0.5165 - val_categorical_accuracy: 0.8200\n",
      "Epoch 30/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5740 - categorical_accuracy: 0.7933 - val_loss: 0.6077 - val_categorical_accuracy: 0.8000\n",
      "Epoch 31/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5493 - categorical_accuracy: 0.7978 - val_loss: 0.5995 - val_categorical_accuracy: 0.8200\n",
      "Epoch 32/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5448 - categorical_accuracy: 0.7967 - val_loss: 0.6650 - val_categorical_accuracy: 0.7600\n",
      "Epoch 33/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5650 - categorical_accuracy: 0.7944INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 31s 272ms/step - loss: 0.5650 - categorical_accuracy: 0.7944 - val_loss: 0.5397 - val_categorical_accuracy: 0.8400\n",
      "Epoch 34/100\n",
      "113/113 [==============================] - 24s 212ms/step - loss: 0.5350 - categorical_accuracy: 0.8000 - val_loss: 0.5725 - val_categorical_accuracy: 0.8200\n",
      "Epoch 35/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5489 - categorical_accuracy: 0.7956 - val_loss: 0.5777 - val_categorical_accuracy: 0.7900\n",
      "Epoch 36/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.5153 - categorical_accuracy: 0.8133 - val_loss: 0.5474 - val_categorical_accuracy: 0.8200\n",
      "Epoch 37/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5193 - categorical_accuracy: 0.8111 - val_loss: 0.5578 - val_categorical_accuracy: 0.8300\n",
      "Epoch 38/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.4973 - categorical_accuracy: 0.8267 - val_loss: 0.5591 - val_categorical_accuracy: 0.8300\n",
      "Epoch 39/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5264 - categorical_accuracy: 0.8100 - val_loss: 0.5386 - val_categorical_accuracy: 0.8300\n",
      "Epoch 40/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.5222 - categorical_accuracy: 0.8156 - val_loss: 0.5450 - val_categorical_accuracy: 0.7700\n",
      "Epoch 41/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5073 - categorical_accuracy: 0.8233 - val_loss: 0.5343 - val_categorical_accuracy: 0.8100\n",
      "Epoch 42/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5149 - categorical_accuracy: 0.8133INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 266ms/step - loss: 0.5149 - categorical_accuracy: 0.8133 - val_loss: 0.5125 - val_categorical_accuracy: 0.8500\n",
      "Epoch 43/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5010 - categorical_accuracy: 0.8200 - val_loss: 0.5377 - val_categorical_accuracy: 0.8200\n",
      "Epoch 44/100\n",
      "113/113 [==============================] - 23s 204ms/step - loss: 0.4909 - categorical_accuracy: 0.8167 - val_loss: 0.5252 - val_categorical_accuracy: 0.8400\n",
      "Epoch 45/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.4823 - categorical_accuracy: 0.8311 - val_loss: 0.5891 - val_categorical_accuracy: 0.8200\n",
      "Epoch 46/100\n",
      "113/113 [==============================] - 24s 207ms/step - loss: 0.4796 - categorical_accuracy: 0.8322 - val_loss: 0.5715 - val_categorical_accuracy: 0.7900\n",
      "Epoch 47/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.4664 - categorical_accuracy: 0.8400 - val_loss: 0.5603 - val_categorical_accuracy: 0.8200\n",
      "Epoch 48/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.4567 - categorical_accuracy: 0.8444 - val_loss: 0.5933 - val_categorical_accuracy: 0.8000\n",
      "Epoch 49/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.4780 - categorical_accuracy: 0.8244 - val_loss: 0.5706 - val_categorical_accuracy: 0.8000\n",
      "Epoch 50/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.4599 - categorical_accuracy: 0.8456 - val_loss: 0.6667 - val_categorical_accuracy: 0.7800\n",
      "Epoch 51/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.4435 - categorical_accuracy: 0.8467 - val_loss: 0.5632 - val_categorical_accuracy: 0.8200\n",
      "Epoch 52/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.4494 - categorical_accuracy: 0.8467 - val_loss: 0.5522 - val_categorical_accuracy: 0.8400\n",
      "Epoch 53/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.4502 - categorical_accuracy: 0.8511 - val_loss: 0.5924 - val_categorical_accuracy: 0.8200\n",
      "Epoch 54/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.4433 - categorical_accuracy: 0.8489 - val_loss: 0.6232 - val_categorical_accuracy: 0.8000\n",
      "Epoch 55/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4406 - categorical_accuracy: 0.8533 - val_loss: 0.5962 - val_categorical_accuracy: 0.8000\n",
      "Epoch 56/100\n",
      "113/113 [==============================] - 23s 204ms/step - loss: 0.4557 - categorical_accuracy: 0.8478 - val_loss: 0.5028 - val_categorical_accuracy: 0.8500\n",
      "Epoch 57/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.4589 - categorical_accuracy: 0.8511 - val_loss: 0.5349 - val_categorical_accuracy: 0.8300\n",
      "Epoch 58/100\n",
      "113/113 [==============================] - 24s 211ms/step - loss: 0.4373 - categorical_accuracy: 0.8533 - val_loss: 0.5369 - val_categorical_accuracy: 0.8300\n",
      "Epoch 59/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.4413 - categorical_accuracy: 0.8456 - val_loss: 0.5402 - val_categorical_accuracy: 0.8100\n",
      "Epoch 60/100\n",
      "113/113 [==============================] - 23s 203ms/step - loss: 0.4047 - categorical_accuracy: 0.8689 - val_loss: 0.5525 - val_categorical_accuracy: 0.8200\n",
      "Epoch 61/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.4144 - categorical_accuracy: 0.8622 - val_loss: 0.5244 - val_categorical_accuracy: 0.8400\n",
      "Epoch 62/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.4197 - categorical_accuracy: 0.8578 - val_loss: 0.5256 - val_categorical_accuracy: 0.8400\n",
      "Epoch 63/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4221 - categorical_accuracy: 0.8644 - val_loss: 0.5117 - val_categorical_accuracy: 0.8500\n",
      "Epoch 64/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.3902 - categorical_accuracy: 0.8644 - val_loss: 0.5599 - val_categorical_accuracy: 0.8400\n",
      "Epoch 65/100\n",
      "113/113 [==============================] - 24s 213ms/step - loss: 0.4177 - categorical_accuracy: 0.8578 - val_loss: 0.6312 - val_categorical_accuracy: 0.8000\n",
      "Epoch 66/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.4115 - categorical_accuracy: 0.8644 - val_loss: 0.5830 - val_categorical_accuracy: 0.8200\n",
      "Epoch 67/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.3976 - categorical_accuracy: 0.8689 - val_loss: 0.5305 - val_categorical_accuracy: 0.8300\n",
      "Epoch 68/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.3877 - categorical_accuracy: 0.8767 - val_loss: 0.5754 - val_categorical_accuracy: 0.8200\n",
      "Epoch 69/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.4042 - categorical_accuracy: 0.8667 - val_loss: 0.5759 - val_categorical_accuracy: 0.8100\n",
      "Epoch 70/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.4005 - categorical_accuracy: 0.8644 - val_loss: 0.5329 - val_categorical_accuracy: 0.8200\n",
      "Epoch 71/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.4021 - categorical_accuracy: 0.8667 - val_loss: 0.5389 - val_categorical_accuracy: 0.8300\n",
      "Epoch 72/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.3908 - categorical_accuracy: 0.8767 - val_loss: 0.5417 - val_categorical_accuracy: 0.8300\n",
      "Epoch 73/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4041 - categorical_accuracy: 0.8667 - val_loss: 0.5199 - val_categorical_accuracy: 0.8300\n",
      "Epoch 74/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4099 - categorical_accuracy: 0.8644 - val_loss: 0.5277 - val_categorical_accuracy: 0.8500\n",
      "Epoch 75/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.4993 - categorical_accuracy: 0.8256 - val_loss: 0.5100 - val_categorical_accuracy: 0.8400\n",
      "Epoch 76/100\n",
      "113/113 [==============================] - 24s 211ms/step - loss: 0.6520 - categorical_accuracy: 0.7722 - val_loss: 0.5926 - val_categorical_accuracy: 0.8000\n",
      "Epoch 77/100\n",
      "113/113 [==============================] - 24s 211ms/step - loss: 0.5311 - categorical_accuracy: 0.8178 - val_loss: 0.7061 - val_categorical_accuracy: 0.7400\n",
      "Epoch 78/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5335 - categorical_accuracy: 0.8156 - val_loss: 0.5903 - val_categorical_accuracy: 0.7800\n",
      "Epoch 79/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.5220 - categorical_accuracy: 0.8200 - val_loss: 0.5064 - val_categorical_accuracy: 0.8400\n",
      "Epoch 80/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.5115 - categorical_accuracy: 0.8122 - val_loss: 0.5452 - val_categorical_accuracy: 0.8200\n",
      "Epoch 81/100\n",
      "113/113 [==============================] - 24s 212ms/step - loss: 0.4882 - categorical_accuracy: 0.8311 - val_loss: 0.6013 - val_categorical_accuracy: 0.7800\n",
      "Epoch 82/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.5348 - categorical_accuracy: 0.8144 - val_loss: 0.5779 - val_categorical_accuracy: 0.8200\n",
      "Epoch 83/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5518 - categorical_accuracy: 0.8022 - val_loss: 0.5277 - val_categorical_accuracy: 0.8300\n",
      "Epoch 84/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4711 - categorical_accuracy: 0.8378 - val_loss: 0.6851 - val_categorical_accuracy: 0.7800\n",
      "Epoch 85/100\n",
      "113/113 [==============================] - 24s 211ms/step - loss: 0.4784 - categorical_accuracy: 0.8278 - val_loss: 0.6208 - val_categorical_accuracy: 0.8100\n",
      "Epoch 86/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.4910 - categorical_accuracy: 0.8333 - val_loss: 0.5067 - val_categorical_accuracy: 0.8500\n",
      "Epoch 87/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.4928 - categorical_accuracy: 0.8289 - val_loss: 0.6270 - val_categorical_accuracy: 0.7900\n",
      "Epoch 88/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.5345 - categorical_accuracy: 0.8122 - val_loss: 0.5809 - val_categorical_accuracy: 0.8100\n",
      "Epoch 89/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5350 - categorical_accuracy: 0.8133 - val_loss: 0.4909 - val_categorical_accuracy: 0.8400\n",
      "Epoch 90/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.5641 - categorical_accuracy: 0.7989 - val_loss: 0.5374 - val_categorical_accuracy: 0.8300\n",
      "Epoch 91/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.4821 - categorical_accuracy: 0.8356 - val_loss: 0.6507 - val_categorical_accuracy: 0.7900\n",
      "Epoch 92/100\n",
      "113/113 [==============================] - 24s 213ms/step - loss: 0.4813 - categorical_accuracy: 0.8322 - val_loss: 0.4872 - val_categorical_accuracy: 0.8400\n",
      "Epoch 93/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.4734 - categorical_accuracy: 0.8378 - val_loss: 0.5663 - val_categorical_accuracy: 0.8000\n",
      "Epoch 94/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4628 - categorical_accuracy: 0.8389INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 31s 274ms/step - loss: 0.4628 - categorical_accuracy: 0.8389 - val_loss: 0.4809 - val_categorical_accuracy: 0.8600\n",
      "Epoch 95/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.4777 - categorical_accuracy: 0.8333 - val_loss: 0.5665 - val_categorical_accuracy: 0.8100\n",
      "Epoch 96/100\n",
      "113/113 [==============================] - 24s 211ms/step - loss: 0.4785 - categorical_accuracy: 0.8344 - val_loss: 0.5035 - val_categorical_accuracy: 0.8400\n",
      "Epoch 97/100\n",
      "113/113 [==============================] - 24s 208ms/step - loss: 0.4671 - categorical_accuracy: 0.8422 - val_loss: 0.5060 - val_categorical_accuracy: 0.8300\n",
      "Epoch 98/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4610 - categorical_accuracy: 0.8367 - val_loss: 0.5027 - val_categorical_accuracy: 0.8600\n",
      "Epoch 99/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.4830 - categorical_accuracy: 0.8300 - val_loss: 0.5160 - val_categorical_accuracy: 0.8200\n",
      "Epoch 100/100\n",
      "113/113 [==============================] - 25s 218ms/step - loss: 0.4242 - categorical_accuracy: 0.8644 - val_loss: 0.5298 - val_categorical_accuracy: 0.8200\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁▁▃▃▄▄▅▆▆▆▆▆▇▇▇▇▇▇▇▇██████████▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>█████▇▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁█████▇▇▇▇▆</td></tr><tr><td>epoch/loss</td><td>█▇▆▆▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▁</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁▁▂▃▄▅▆▆▆▆▆▇▇▇▇▇▇█▇▇▇▇█▇██▇▇▇█▆█▇▇▇███▇▇</td></tr><tr><td>epoch/val_loss</td><td>█▇▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▁▂▂▂▃▁▂▁▂▂▂▂▂▄▁▂▃▃▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.86444</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>8e-05</td></tr><tr><td>epoch/loss</td><td>0.42417</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.82</td></tr><tr><td>epoch/val_loss</td><td>0.52981</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM64-Dense128-LipsEyesHandsPose</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/mx4vk35b' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/mx4vk35b</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240410_195902-mx4vk35b/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f82b8fc67d0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(64, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, NUM_FEATURES),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", f\"LSTM64-Dense128-LipsEyesHandsPose_NF={NUM_FEATURES}.tf\"),\n",
    "            run_name=\"LSTM64-Dense128-LipsEyesHandsPose\", T_max=75, epochs=100, \n",
    "            max_lr = 1e-4, min_lr = 2.5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240410_204035-gheuw0t5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/gheuw0t5' target=\"_blank\">LSTM64-Dense256-LipsEyesHandsPose</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/gheuw0t5' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/gheuw0t5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.1176 - categorical_accuracy: 0.3911INFO:tensorflow:Assets written to: models/LSTM64-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 34s 278ms/step - loss: 1.1176 - categorical_accuracy: 0.3911 - val_loss: 1.0950 - val_categorical_accuracy: 0.4400\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 1.0907 - categorical_accuracy: 0.3833 - val_loss: 1.0665 - val_categorical_accuracy: 0.4400\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0620 - categorical_accuracy: 0.4367INFO:tensorflow:Assets written to: models/LSTM64-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 31s 272ms/step - loss: 1.0620 - categorical_accuracy: 0.4367 - val_loss: 1.0362 - val_categorical_accuracy: 0.4600\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0239 - categorical_accuracy: 0.4578INFO:tensorflow:Assets written to: models/LSTM64-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 262ms/step - loss: 1.0239 - categorical_accuracy: 0.4578 - val_loss: 0.9731 - val_categorical_accuracy: 0.4700\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - 23s 205ms/step - loss: 0.9416 - categorical_accuracy: 0.4922 - val_loss: 0.8732 - val_categorical_accuracy: 0.4700\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8826 - categorical_accuracy: 0.5378INFO:tensorflow:Assets written to: models/LSTM64-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 267ms/step - loss: 0.8826 - categorical_accuracy: 0.5378 - val_loss: 0.8281 - val_categorical_accuracy: 0.5500\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8707 - categorical_accuracy: 0.5522INFO:tensorflow:Assets written to: models/LSTM64-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 267ms/step - loss: 0.8707 - categorical_accuracy: 0.5522 - val_loss: 0.8618 - val_categorical_accuracy: 0.5800\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.8742 - categorical_accuracy: 0.5444 - val_loss: 0.8617 - val_categorical_accuracy: 0.5200\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - 24s 211ms/step - loss: 0.8392 - categorical_accuracy: 0.5633 - val_loss: 0.8461 - val_categorical_accuracy: 0.5600\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.8246 - categorical_accuracy: 0.6022 - val_loss: 0.8382 - val_categorical_accuracy: 0.5500\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8003 - categorical_accuracy: 0.6022INFO:tensorflow:Assets written to: models/LSTM64-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 31s 271ms/step - loss: 0.8003 - categorical_accuracy: 0.6022 - val_loss: 0.7601 - val_categorical_accuracy: 0.7000\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.7501 - categorical_accuracy: 0.6656 - val_loss: 0.7399 - val_categorical_accuracy: 0.6200\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.7491 - categorical_accuracy: 0.6722 - val_loss: 0.7482 - val_categorical_accuracy: 0.6800\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - 24s 211ms/step - loss: 0.7184 - categorical_accuracy: 0.7022 - val_loss: 0.7496 - val_categorical_accuracy: 0.6900\n",
      "Epoch 15/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7022 - categorical_accuracy: 0.7111INFO:tensorflow:Assets written to: models/LSTM64-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 270ms/step - loss: 0.7022 - categorical_accuracy: 0.7111 - val_loss: 0.6878 - val_categorical_accuracy: 0.7100\n",
      "Epoch 16/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.6798 - categorical_accuracy: 0.7033 - val_loss: 0.6534 - val_categorical_accuracy: 0.7100\n",
      "Epoch 17/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6459 - categorical_accuracy: 0.7356INFO:tensorflow:Assets written to: models/LSTM64-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 265ms/step - loss: 0.6459 - categorical_accuracy: 0.7356 - val_loss: 0.6334 - val_categorical_accuracy: 0.7900\n",
      "Epoch 18/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.6339 - categorical_accuracy: 0.7344 - val_loss: 0.6532 - val_categorical_accuracy: 0.7000\n",
      "Epoch 19/100\n",
      "113/113 [==============================] - 24s 211ms/step - loss: 0.6299 - categorical_accuracy: 0.7511 - val_loss: 0.6284 - val_categorical_accuracy: 0.7200\n",
      "Epoch 20/100\n",
      "113/113 [==============================] - 24s 213ms/step - loss: 0.5869 - categorical_accuracy: 0.7756 - val_loss: 0.5971 - val_categorical_accuracy: 0.7500\n",
      "Epoch 21/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.5480 - categorical_accuracy: 0.7900 - val_loss: 0.6396 - val_categorical_accuracy: 0.7400\n",
      "Epoch 22/100\n",
      "113/113 [==============================] - 24s 212ms/step - loss: 0.5699 - categorical_accuracy: 0.7656 - val_loss: 0.6444 - val_categorical_accuracy: 0.7000\n",
      "Epoch 23/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5517 - categorical_accuracy: 0.7822INFO:tensorflow:Assets written to: models/LSTM64-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 31s 273ms/step - loss: 0.5517 - categorical_accuracy: 0.7822 - val_loss: 0.5486 - val_categorical_accuracy: 0.8000\n",
      "Epoch 24/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.5810 - categorical_accuracy: 0.7822 - val_loss: 0.6236 - val_categorical_accuracy: 0.7700\n",
      "Epoch 25/100\n",
      "113/113 [==============================] - 24s 213ms/step - loss: 0.5617 - categorical_accuracy: 0.7844 - val_loss: 0.5622 - val_categorical_accuracy: 0.8000\n",
      "Epoch 26/100\n",
      "113/113 [==============================] - 24s 207ms/step - loss: 0.5665 - categorical_accuracy: 0.7967 - val_loss: 0.6168 - val_categorical_accuracy: 0.7600\n",
      "Epoch 27/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5566 - categorical_accuracy: 0.7911INFO:tensorflow:Assets written to: models/LSTM64-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 266ms/step - loss: 0.5566 - categorical_accuracy: 0.7911 - val_loss: 0.5709 - val_categorical_accuracy: 0.8100\n",
      "Epoch 28/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5334 - categorical_accuracy: 0.8011 - val_loss: 0.6132 - val_categorical_accuracy: 0.7700\n",
      "Epoch 29/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5195 - categorical_accuracy: 0.8056INFO:tensorflow:Assets written to: models/LSTM64-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 267ms/step - loss: 0.5195 - categorical_accuracy: 0.8056 - val_loss: 0.5725 - val_categorical_accuracy: 0.8200\n",
      "Epoch 30/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.5162 - categorical_accuracy: 0.8089 - val_loss: 0.5767 - val_categorical_accuracy: 0.7900\n",
      "Epoch 31/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.5288 - categorical_accuracy: 0.8100 - val_loss: 0.6555 - val_categorical_accuracy: 0.7400\n",
      "Epoch 32/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.5279 - categorical_accuracy: 0.8011 - val_loss: 0.5979 - val_categorical_accuracy: 0.7800\n",
      "Epoch 33/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.5282 - categorical_accuracy: 0.8078 - val_loss: 0.5898 - val_categorical_accuracy: 0.7800\n",
      "Epoch 34/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5202 - categorical_accuracy: 0.8111INFO:tensorflow:Assets written to: models/LSTM64-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 31s 273ms/step - loss: 0.5202 - categorical_accuracy: 0.8111 - val_loss: 0.5698 - val_categorical_accuracy: 0.8300\n",
      "Epoch 35/100\n",
      "113/113 [==============================] - 23s 207ms/step - loss: 0.5039 - categorical_accuracy: 0.8022 - val_loss: 0.5626 - val_categorical_accuracy: 0.7900\n",
      "Epoch 36/100\n",
      "113/113 [==============================] - 23s 208ms/step - loss: 0.5050 - categorical_accuracy: 0.8100 - val_loss: 0.5331 - val_categorical_accuracy: 0.8300\n",
      "Epoch 37/100\n",
      "113/113 [==============================] - 24s 209ms/step - loss: 0.4723 - categorical_accuracy: 0.8267 - val_loss: 0.5398 - val_categorical_accuracy: 0.8000\n",
      "Epoch 38/100\n",
      "113/113 [==============================] - 24s 210ms/step - loss: 0.4859 - categorical_accuracy: 0.8222 - val_loss: 0.5563 - val_categorical_accuracy: 0.8200\n",
      "Epoch 39/100\n",
      "113/113 [==============================] - 23s 206ms/step - loss: 0.4847 - categorical_accuracy: 0.8256 - val_loss: 0.6348 - val_categorical_accuracy: 0.7800\n",
      "Epoch 40/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4797 - categorical_accuracy: 0.8289INFO:tensorflow:Assets written to: models/LSTM64-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256-LipsEyesHandsPose_NF=396.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 30s 267ms/step - loss: 0.4797 - categorical_accuracy: 0.8289 - val_loss: 0.5145 - val_categorical_accuracy: 0.8400\n",
      "Epoch 41/100\n",
      " 20/113 [====>.........................] - ETA: 16s - loss: 0.4587 - categorical_accuracy: 0.8526"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(64, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, NUM_FEATURES),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", f\"LSTM64-Dense256-LipsEyesHandsPose_NF={NUM_FEATURES}.tf\"),\n",
    "            run_name=\"LSTM64-Dense256-LipsEyesHandsPose\", T_max=75, epochs=100, \n",
    "            max_lr = 1e-4, min_lr = 2.5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 60, 1629)\n"
     ]
    }
   ],
   "source": [
    "x, y = train_dataset_parquet[0]\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "class PositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, embed_dim):\n",
    "        super(PositionEmbedding, self).__init__()\n",
    "        self.pos_emb = layers.Embedding(input_dim=embed_dim, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        print(x.shape, positions.shape)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras_nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create a single transformer encoder layer.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m encoder \u001b[38;5;241m=\u001b[39m keras_nlp\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mTransformerEncoder(\n\u001b[0;32m      3\u001b[0m     intermediate_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Create a simple model containing the encoder.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m64\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras_nlp' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a single transformer encoder layer.\n",
    "encoder = keras_nlp.layers.TransformerEncoder(\n",
    "    intermediate_dim=64, num_heads=8)\n",
    "\n",
    "# Create a simple model containing the encoder.\n",
    "input = keras.Input(shape=(10, 64))\n",
    "output = encoder(input)\n",
    "model = keras.Model(inputs=input, outputs=output)\n",
    "\n",
    "# Call encoder on the inputs.\n",
    "input_data = np.random.uniform(size=(2, 10, 64))\n",
    "output = model(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 60, 1629)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 5087314,
     "sourceId": 46105,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
