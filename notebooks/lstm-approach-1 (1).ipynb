{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#Load parquet data into dataset_parquet for training.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.regularizers import l2\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import random\n",
    "import time\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    # root = os.path.join(\"/\", \"kaggle\", \"input\", \"asl-signs\") \n",
    "    root = os.path.join(\".\")\n",
    "    DATA_LIMIT = 5000\n",
    "    BATCH_SIZE = 32\n",
    "    VIDEO_LENGTH = 25\n",
    "    TRAIN_VAL_SPLIT = 0.8\n",
    "    WANDB_RUN = \"mediapipe-asl-dataset\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPS :  40\n",
      "EYE_LEFT :  20\n",
      "EYE_RIGHT :  20\n",
      "LEFT_HAND :  21\n",
      "RIGHT_HAND :  21\n",
      "LEFT_POSE :  5\n",
      "RIGHT_POSE :  5\n",
      "132\n"
     ]
    }
   ],
   "source": [
    "LIPS_IDXS0 = np.array([\n",
    "        61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "        291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "        78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "        95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "    ])\n",
    "\n",
    "EYE_LEFT = np.array([33, 7, 246, 161, 160, 159, 158, 157, 173, 133, 155, 154, 153, 145, 144, 163, 471, 470, 469, 472])\n",
    "EYE_RIGHT = np.array([362, 398, 384, 385, 386, 387, 388, 466, 263, 249, 390, 373, 374, 380, 381, 382, 476, 475, 474, 477])\n",
    "# Landmark indices in original data\n",
    "LEFT_HAND_IDXS0 = np.arange(468,489)\n",
    "RIGHT_HAND_IDXS0 = np.arange(522,543)\n",
    "LEFT_POSE_IDXS0 = np.array([502, 504, 506, 508, 510])\n",
    "RIGHT_POSE_IDXS0 = np.array([503, 505, 507, 509, 511])\n",
    "\n",
    "print(\"LIPS : \",len(LIPS_IDXS0))\n",
    "print(\"EYE_LEFT : \",len(EYE_LEFT))\n",
    "print(\"EYE_RIGHT : \",len(EYE_RIGHT))\n",
    "print(\"LEFT_HAND : \",len(LEFT_HAND_IDXS0))\n",
    "print(\"RIGHT_HAND : \",len(RIGHT_HAND_IDXS0))\n",
    "print(\"LEFT_POSE : \",len(LEFT_POSE_IDXS0))\n",
    "print(\"RIGHT_POSE : \",len(RIGHT_POSE_IDXS0))\n",
    "\n",
    "all_selection = np.concatenate([LIPS_IDXS0, EYE_LEFT, EYE_RIGHT, LEFT_HAND_IDXS0, RIGHT_HAND_IDXS0, LEFT_POSE_IDXS0, RIGHT_POSE_IDXS0])\n",
    "print(len(all_selection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code sorts out a parquet files and rearrange the order to pose,face, left-hand, right-hand\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "\n",
    "ids = None\n",
    "\n",
    "order_global = {\"pose\" : 10000, \"face\" : 1000, \"left_hand\" : 100, \"right_hand\" : 10}\n",
    "\n",
    "def visualize_keypoints(frames : np.ndarray, point_size : int):\n",
    "    if len(frames.shape) == 1:\n",
    "        frames = np.array([frames])\n",
    "    \n",
    "    for frame in frames:\n",
    "        frame = frame.reshape(-1, 3)\n",
    "        sizes = point_size * np.ones(frame.shape[0])\n",
    "\n",
    "        fig = go.Figure(data=go.Scatter(x=frame[:,0], y=2.5 - frame[:,1], mode='markers',\n",
    "                                        marker=dict(\n",
    "                                            size=sizes\n",
    "                                            )))\n",
    "\n",
    "    # Customize the layout\n",
    "    fig.update_layout(title='visualization of human keypoints',\n",
    "                        xaxis_title='',\n",
    "                        yaxis_title='',\n",
    "                        width=1000,\n",
    "                        height=1600)\n",
    "\n",
    "    fig.update_xaxes(range=[-0.2, 1.4])  # Set x-axis range from 0 to 6\n",
    "    fig.update_yaxes(range=[0, 2.5])  # Set y-axis range from 10 to 20\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def process_parquet(ds, idxes = None):\n",
    "    ret = []\n",
    "    frames_unique = sorted(np.unique(ds[\"frame\"]))\n",
    "    for i,frame in enumerate(frames_unique):\n",
    "        frame_ds = ds[ds['frame'] == frame]\n",
    "        \n",
    "        order = []\n",
    "        for el in frame_ds[\"row_id\"]:\n",
    "            _frame, part, keypoint = el.split(\"-\")\n",
    "            order.append(order_global[part] - int(keypoint))\n",
    "\n",
    "        order = np.array(order)\n",
    "        frame_ds.iloc[:, 1] = order\n",
    "        frame_ds = frame_ds.sort_values(by=\"row_id\", ascending=False)\n",
    "    \n",
    "        vals = np.array(frame_ds[[\"x\", \"y\", \"z\"]])\n",
    "        if idxes is not None:\n",
    "            vals = vals[idxes]\n",
    "    \n",
    "        vals = vals.flatten()\n",
    "\n",
    "        ret.append(vals)\n",
    "        \n",
    "    return np.array(ret)\n",
    "\n",
    "\n",
    "def process_parquet2(ds, idxes = None):\n",
    "    ret = []    \n",
    "    frame_size = 543\n",
    "    it = len(ds) // frame_size\n",
    "    assert it == len(ds) / frame_size\n",
    "    \n",
    "    for i in range(it):\n",
    "        vals = ds.iloc[ i * frame_size : (i + 1 ) * frame_size ]        \n",
    "        \n",
    "        if idxes is not None:          \n",
    "            vals = ds.iloc[idxes]\n",
    "                        \n",
    "        ret.append(np.array(vals[[\"x\",\"y\", \"z\"]]).flatten())\n",
    "        \n",
    "    return np.array(np.array(ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94477/94477 [00:00<00:00, 245426.58it/s]\n",
      "Cacheing: 100%|██████████| 4000/4000 [01:41<00:00, 39.53it/s]\n",
      "100%|██████████| 94477/94477 [00:00<00:00, 241209.66it/s]\n",
      "Cacheing: 100%|██████████| 1000/1000 [00:25<00:00, 39.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cardinality of train : 4000, cardinality of validation : 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#custom class to load data from Parquet files for training ML models.\n",
    "class ParquetDatasetCached(keras.utils.Sequence):\n",
    "    def __init__(self, dataset_folder, csv_file : str, batch_size=CONFIG.BATCH_SIZE, \n",
    "                 data_limit :int= CONFIG.DATA_LIMIT, check_if_file_exists = True, \n",
    "                 preprocessing_func=None, frame_length :int = CONFIG.VIDEO_LENGTH,\n",
    "                 split : str = \"train\", train_val_split : float = CONFIG.TRAIN_VAL_SPLIT,\n",
    "                 sort_by_counts : bool = True, idxes=None, **kwargs\n",
    "                ):\n",
    "        super().__init__(**kwargs)\n",
    "        #taking keras sequence for .fit(), .evaluate(), .predict() methods\n",
    "        #load csv - it has the path to parquet file, and another to store label\n",
    "        self.csv_path = csv_file\n",
    "        self.root_folder = dataset_folder\n",
    "        self.batch_size = batch_size\n",
    "        #optional pre-processing function to the parquet files.\n",
    "        self.preprocessing_func = preprocessing_func\n",
    "        \n",
    "        self.csv_data = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        self.all_files = []\n",
    "        self.not_exists = []\n",
    "        self.frame_length = frame_length\n",
    "\n",
    "        \n",
    "        for path, label in tqdm(list(zip(self.csv_data[\"path\"], self.csv_data[\"sign\"]))):\n",
    "            prop_path = os.path.join(self.root_folder, path)\n",
    "            \n",
    "            if check_if_file_exists:\n",
    "                if os.path.exists(prop_path):\n",
    "                    self.all_files.append((prop_path, label))\n",
    "                else:\n",
    "                    self.not_exists.append(prop_path)\n",
    "            else:\n",
    "                self.all_files.append((prop_path, label))\n",
    "                \n",
    "                    \n",
    "        self.all_files = np.array(self.all_files)\n",
    "        self.unique_labels = np.unique(self.all_files[:, 1])\n",
    "        self.label_2_id = { key : i for i, key in enumerate(self.unique_labels)}\n",
    "    \n",
    "        # sort the values by popularity\n",
    "        if sort_by_counts:\n",
    "            cnt = Counter(self.all_files[:, 1])\n",
    "            vals = []\n",
    "            \n",
    "            for i,row in enumerate(self.all_files):\n",
    "                vals.append((int(1e6 * cnt[row[1]] + self.label_2_id [row[1]]),i))\n",
    "            \n",
    "            vals = np.array(sorted(vals)[::-1])\n",
    "            self.all_files = self.all_files[vals[:,1]]\n",
    "\n",
    "        \n",
    "        if data_limit < 0:\n",
    "            train_ds, val_ds = train_test_split(self.all_files, train_size=train_val_split, random_state=42)\n",
    "        else:\n",
    "            train_ds, val_ds = train_test_split(self.all_files[:data_limit], train_size=train_val_split, random_state=42)\n",
    "            self.unique_labels = np.unique(self.all_files[:data_limit, 1])\n",
    "            self.label_2_id = { key : i for i, key in enumerate(self.unique_labels)}\n",
    "            \n",
    "        if split.lower() == \"train\":\n",
    "            self.dataset = train_ds\n",
    "            \n",
    "        elif split.lower() == \"val\":\n",
    "            self.dataset = val_ds \n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"please specify split to be either train or val\")\n",
    "            \n",
    "        self.cache_data(idxes)\n",
    "                   \n",
    "\n",
    "    def cache_data(self, idxes):\n",
    "        self.cached_X, self.cached_Y = [], []\n",
    "        \n",
    "        pb = tqdm(range(len(self.dataset)), desc=\"Cacheing\")\n",
    "\n",
    "        for i in pb:\n",
    "            \n",
    "            path, label = self.dataset[i]\n",
    "            df = pd.read_parquet(path)\n",
    "                        \n",
    "            one_hot_encoded_label = np.zeros(len(self.unique_labels))\n",
    "            one_hot_encoded_label[self.label_2_id[label]] = 1\n",
    "\n",
    "            self.cached_X.append(process_parquet2(df, idxes=idxes)) \n",
    "            self.cached_Y.append(one_hot_encoded_label)        \n",
    "\n",
    "    def __len__(self):\n",
    "        # Assuming each Parquet file should be one batch; adjust if necessary\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.preprocessing_func(self.cached_X[idx]), self.cached_Y[idx]                \n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        # Shuffle files for the next epoch\n",
    "        np.random.shuffle(self.dataset)\n",
    "\n",
    "\n",
    "def preprocess(frames):\n",
    "    current_length, num_features = frames.shape\n",
    "\n",
    "    if current_length >= CONFIG.VIDEO_LENGTH:\n",
    "            # TODO: a better than uniform value ? Could place gaussian in the middle\n",
    "            random_start = random.randint(0, current_length - CONFIG.VIDEO_LENGTH)\n",
    "            return np.nan_to_num(frames[random_start : (random_start + CONFIG.VIDEO_LENGTH)])\n",
    "        \n",
    "    # padd the video to contain zeros \n",
    "    return np.concatenate([np.nan_to_num(frames), np.zeros((CONFIG.VIDEO_LENGTH - current_length, num_features))], axis=0)\n",
    "\n",
    "    \n",
    "# Usage example\n",
    "parquet_folder_path = CONFIG.root\n",
    "train_dataset_parquet = ParquetDatasetCached(parquet_folder_path, csv_file = os.path.join(CONFIG.root, \"train.csv\"), \n",
    "                                 batch_size=CONFIG.BATCH_SIZE, data_limit=CONFIG.DATA_LIMIT,\n",
    "                                 preprocessing_func=preprocess,\n",
    "                                check_if_file_exists = True,\n",
    "                                split=\"train\", idxes= all_selection)\n",
    "\n",
    "val_dataset_parquet = ParquetDatasetCached(parquet_folder_path, csv_file = os.path.join(CONFIG.root, \"train.csv\"), \n",
    "                                 batch_size=CONFIG.BATCH_SIZE, data_limit=CONFIG.DATA_LIMIT,\n",
    "                                 preprocessing_func=preprocess,\n",
    "                                 check_if_file_exists= True,\n",
    "                                 split=\"val\",idxes= all_selection)\n",
    "\n",
    "print(f\"cardinality of train : {len(train_dataset_parquet)}, cardinality of validation : {len(val_dataset_parquet)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_shape = (25, 396), Y_shape = (13,)\n"
     ]
    }
   ],
   "source": [
    "X_shape = train_dataset_parquet[0][0].shape\n",
    "Y_shape = train_dataset_parquet[0][1].shape\n",
    "print(f\"X_shape = {X_shape}, Y_shape = {Y_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_train_generator():\n",
    "    # Instantiate your existing dataset loader\n",
    "\n",
    "    for i in range(len(train_dataset_parquet)):\n",
    "        X_batch, Y_batch = train_dataset_parquet[i]\n",
    "        yield X_batch, Y_batch\n",
    "        \n",
    "def dataset_val_generator():\n",
    "    # Instantiate your existing dataset loader\n",
    "\n",
    "    for i in range(len(val_dataset_parquet)):\n",
    "        X_batch, Y_batch = val_dataset_parquet[i]\n",
    "        yield X_batch, Y_batch        \n",
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: dataset_train_generator(),\n",
    "    output_types=(tf.float32, tf.float32),  # Adjust types based on your actual data\n",
    "    output_shapes=(X_shape, Y_shape)\n",
    ").prefetch(tf.data.AUTOTUNE).batch(CONFIG.BATCH_SIZE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: dataset_val_generator(),\n",
    "    output_types=(tf.float32, tf.float32),  # Adjust types based on your actual data\n",
    "    output_shapes=(X_shape, Y_shape)\n",
    ").prefetch(tf.data.AUTOTUNE).batch(CONFIG.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [00:00, 235.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 25, 396) (8, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:01, 294.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating through dataset took : 1.6986s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([655., 320., 325., 318., 325., 666., 325., 323.,  90., 653.]),\n",
       " array([ 0. ,  1.2,  2.4,  3.6,  4.8,  6. ,  7.2,  8.4,  9.6, 10.8, 12. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time.time()\n",
    "isnans =False\n",
    "\n",
    "id2label = {v : k for k,v in train_dataset_parquet.label_2_id.items()}\n",
    "\n",
    "f = True\n",
    "labels_batches = []\n",
    "for el in tqdm(train_dataset):\n",
    "    if f:\n",
    "        print(el[0].shape, el[1].shape)\n",
    "        f = False\n",
    "\n",
    "    ls = np.argmax(el[1], axis=1)\n",
    "    for l in ls:\n",
    "        labels_batches.append(id2label[l])\n",
    "    \n",
    "    isnans |= np.any(np.isnan(el[0]))\n",
    "    if isnans:\n",
    "        print(\"FOUND NAN!\")\n",
    "        break\n",
    "\n",
    "print(f\"Iterating through dataset took : {round( time.time() - start , 4)}s\")\n",
    "plt.hist(labels_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, LeakyReLU\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras import backend as K\n",
    "from wandb.keras import WandbMetricsLogger\n",
    "from keras.callbacks import Callback\n",
    "import tensorflow as tf\n",
    "import wandb\n",
    "\n",
    "\n",
    "class CosineAnnealingLearningRateScheduler(Callback):\n",
    "    def __init__(self, max_lr, min_lr, T_max):\n",
    "        super(CosineAnnealingLearningRateScheduler, self).__init__()\n",
    "        self.max_lr = max_lr  # Maximum learning rate (i.e., start learning rate)\n",
    "        self.min_lr = min_lr  # Minimum learning rate\n",
    "        self.T_max = T_max    # Specifies the number of epochs per cycle\n",
    "        self.t = 0            # Current epoch\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.t += 1\n",
    "        cos = np.cos(np.pi * (self.t % self.T_max) / self.T_max)\n",
    "        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + cos)\n",
    "\n",
    "        keras.backend.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "def keras_train(model, filepath : str, max_lr = 1e-4, min_lr = 5e-5, T_max=51, epochs=100, run_name=\"\",\n",
    "                mediapipe_features = \"all\", USE_WANDB=True): \n",
    "    \n",
    "    \n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
    "                                                 monitor=\"val_categorical_accuracy\",\n",
    "                                                 verbose=0,\n",
    "                                                 save_best_only=True,\n",
    "                                                 mode=\"max\",\n",
    "                                                 save_freq=\"epoch\")\n",
    "    \n",
    "    cosine_annealer = CosineAnnealingLearningRateScheduler(max_lr=max_lr,\n",
    "                                                           min_lr=min_lr,\n",
    "                                                           T_max=T_max)\n",
    "    \n",
    "    #Adam Optimizer - fixed learning rate.\n",
    "    adam_optimizer = tf.keras.optimizers.Adam(learning_rate=max_lr, weight_decay=1e-5, clipnorm=1.)\n",
    "\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False,label_smoothing=0.2)\n",
    "    model.compile(optimizer=adam_optimizer, loss=loss, metrics=['categorical_accuracy'])\n",
    "    \n",
    "    \n",
    "    callbacks  = [checkpoint, cosine_annealer]\n",
    "    \n",
    "    if USE_WANDB:\n",
    "        wandb.init(project=CONFIG.WANDB_RUN,\n",
    "                        name=run_name,\n",
    "                        notes=\"Model summary : \\n\" + str(model),\n",
    "                        config={\"max_lr\" : max_lr, \n",
    "                                \"min_lr\" : 5e-5, \n",
    "                                \"scheduler\" : \"cosineAnnealer\", \n",
    "                                \"epochs\" : epochs, \n",
    "                                \"T_max\" : T_max, \n",
    "                                \"train_size\" : len(train_dataset_parquet.dataset),\n",
    "                                \"val_size\" : len(val_dataset_parquet.dataset),\n",
    "                                \"unique_classes\" : len(train_dataset_parquet.unique_labels), \n",
    "                                \"video_length\" : CONFIG.VIDEO_LENGTH,\n",
    "                                \"features\" : mediapipe_features\n",
    "                                })\n",
    "        callbacks.append(WandbMetricsLogger())\n",
    "\n",
    "\n",
    "    history = model.fit(train_dataset, epochs=epochs, validation_data = val_dataset, callbacks=callbacks)\n",
    "    \n",
    "    if USE_WANDB:      \n",
    "        wandb.finish()\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:g6nfl07x) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM128-Dense128-Dense256-LipsEyesHandsPose</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/g6nfl07x' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/g6nfl07x</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240416_135540-g6nfl07x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:g6nfl07x). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4e79b938c364732b91e1e43312f4fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113736778497696, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240416_135553-cwtokq0w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/cwtokq0w' target=\"_blank\">LSTM128-Dense128-Dense256-LipsEyesHandsPose_5k</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/cwtokq0w' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/cwtokq0w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "    500/Unknown - 10s 9ms/step - loss: 2.5549 - categorical_accuracy: 0.1443"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 13:56:09.198066: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8697503385697755158\n",
      "2024-04-16 13:56:09.198151: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 1180876524053387608\n",
      "2024-04-16 13:56:09.198165: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 6744866179856845432\n",
      "2024-04-16 13:56:09.198178: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 3461922473389982346\n",
      "2024-04-16 13:56:10.792785: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 88988061229745601\n",
      "2024-04-16 13:56:10.792851: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 6585685963913558590\n",
      "2024-04-16 13:56:10.792867: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 5556713438244639884\n",
      "2024-04-16 13:56:10.792904: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 16237532647568878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 24s 37ms/step - loss: 2.5549 - categorical_accuracy: 0.1443 - val_loss: 2.4364 - val_categorical_accuracy: 0.2090\n",
      "Epoch 2/100\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.4261 - categorical_accuracy: 0.1874INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 18s 35ms/step - loss: 2.4257 - categorical_accuracy: 0.1870 - val_loss: 2.3591 - val_categorical_accuracy: 0.2470\n",
      "Epoch 3/100\n",
      "498/500 [============================>.] - ETA: 0s - loss: 2.3719 - categorical_accuracy: 0.2209INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 18s 35ms/step - loss: 2.3720 - categorical_accuracy: 0.2205 - val_loss: 2.3150 - val_categorical_accuracy: 0.2630\n",
      "Epoch 4/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.3330 - categorical_accuracy: 0.2440INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 18s 35ms/step - loss: 2.3330 - categorical_accuracy: 0.2440 - val_loss: 2.2657 - val_categorical_accuracy: 0.2800\n",
      "Epoch 5/100\n",
      "499/500 [============================>.] - ETA: 0s - loss: 2.2913 - categorical_accuracy: 0.2811INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 18s 36ms/step - loss: 2.2911 - categorical_accuracy: 0.2815 - val_loss: 2.2263 - val_categorical_accuracy: 0.3110\n",
      "Epoch 6/100\n",
      "494/500 [============================>.] - ETA: 0s - loss: 2.2616 - categorical_accuracy: 0.2900INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 17s 35ms/step - loss: 2.2609 - categorical_accuracy: 0.2907 - val_loss: 2.2044 - val_categorical_accuracy: 0.3320\n",
      "Epoch 7/100\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.2317 - categorical_accuracy: 0.3151INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 18s 35ms/step - loss: 2.2305 - categorical_accuracy: 0.3160 - val_loss: 2.1783 - val_categorical_accuracy: 0.3440\n",
      "Epoch 8/100\n",
      "498/500 [============================>.] - ETA: 0s - loss: 2.2108 - categorical_accuracy: 0.3215INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 18s 35ms/step - loss: 2.2100 - categorical_accuracy: 0.3225 - val_loss: 2.1564 - val_categorical_accuracy: 0.3530\n",
      "Epoch 9/100\n",
      "498/500 [============================>.] - ETA: 0s - loss: 2.1902 - categorical_accuracy: 0.3363INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 18s 35ms/step - loss: 2.1894 - categorical_accuracy: 0.3372 - val_loss: 2.1472 - val_categorical_accuracy: 0.3550\n",
      "Epoch 10/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 2.1706 - categorical_accuracy: 0.3442 - val_loss: 2.1417 - val_categorical_accuracy: 0.3540\n",
      "Epoch 11/100\n",
      "495/500 [============================>.] - ETA: 0s - loss: 2.1504 - categorical_accuracy: 0.3588INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 18s 36ms/step - loss: 2.1488 - categorical_accuracy: 0.3605 - val_loss: 2.1377 - val_categorical_accuracy: 0.3780\n",
      "Epoch 12/100\n",
      "494/500 [============================>.] - ETA: 0s - loss: 2.1289 - categorical_accuracy: 0.3735INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 18s 35ms/step - loss: 2.1275 - categorical_accuracy: 0.3747 - val_loss: 2.1171 - val_categorical_accuracy: 0.3930\n",
      "Epoch 13/100\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.1079 - categorical_accuracy: 0.3919INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 18s 35ms/step - loss: 2.1065 - categorical_accuracy: 0.3925 - val_loss: 2.0961 - val_categorical_accuracy: 0.4230\n",
      "Epoch 14/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 2.0797 - categorical_accuracy: 0.4030 - val_loss: 2.0946 - val_categorical_accuracy: 0.4130\n",
      "Epoch 15/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 2.0504 - categorical_accuracy: 0.4243 - val_loss: 2.0809 - val_categorical_accuracy: 0.4200\n",
      "Epoch 16/100\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.0291 - categorical_accuracy: 0.4369INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 17s 34ms/step - loss: 2.0277 - categorical_accuracy: 0.4378 - val_loss: 2.0613 - val_categorical_accuracy: 0.4250\n",
      "Epoch 17/100\n",
      "498/500 [============================>.] - ETA: 0s - loss: 2.0088 - categorical_accuracy: 0.4506INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 19s 37ms/step - loss: 2.0081 - categorical_accuracy: 0.4512 - val_loss: 2.0255 - val_categorical_accuracy: 0.4480\n",
      "Epoch 18/100\n",
      "495/500 [============================>.] - ETA: 0s - loss: 1.9922 - categorical_accuracy: 0.4583INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 17s 33ms/step - loss: 1.9911 - categorical_accuracy: 0.4588 - val_loss: 2.0248 - val_categorical_accuracy: 0.4490\n",
      "Epoch 19/100\n",
      "495/500 [============================>.] - ETA: 0s - loss: 1.9671 - categorical_accuracy: 0.4816INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 18s 35ms/step - loss: 1.9662 - categorical_accuracy: 0.4818 - val_loss: 1.9979 - val_categorical_accuracy: 0.4640\n",
      "Epoch 20/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.9515 - categorical_accuracy: 0.4930 - val_loss: 1.9986 - val_categorical_accuracy: 0.4610\n",
      "Epoch 21/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.9307 - categorical_accuracy: 0.5102 - val_loss: 1.9992 - val_categorical_accuracy: 0.4530\n",
      "Epoch 22/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9155 - categorical_accuracy: 0.5205INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 19s 37ms/step - loss: 1.9155 - categorical_accuracy: 0.5205 - val_loss: 1.9327 - val_categorical_accuracy: 0.4900\n",
      "Epoch 23/100\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 1.9000 - categorical_accuracy: 0.5280 - val_loss: 1.9390 - val_categorical_accuracy: 0.4900\n",
      "Epoch 24/100\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.8876 - categorical_accuracy: 0.5320INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 17s 33ms/step - loss: 1.8875 - categorical_accuracy: 0.5320 - val_loss: 1.9061 - val_categorical_accuracy: 0.5170\n",
      "Epoch 25/100\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 1.8694 - categorical_accuracy: 0.5443 - val_loss: 1.9095 - val_categorical_accuracy: 0.5100\n",
      "Epoch 26/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.8552 - categorical_accuracy: 0.5510 - val_loss: 1.9101 - val_categorical_accuracy: 0.5160\n",
      "Epoch 27/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.8432 - categorical_accuracy: 0.5602 - val_loss: 1.9229 - val_categorical_accuracy: 0.5090\n",
      "Epoch 28/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.8275 - categorical_accuracy: 0.5710 - val_loss: 1.9480 - val_categorical_accuracy: 0.4990\n",
      "Epoch 29/100\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 1.8202 - categorical_accuracy: 0.5720 - val_loss: 1.9259 - val_categorical_accuracy: 0.5070\n",
      "Epoch 30/100\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.8102 - categorical_accuracy: 0.5822INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 18s 37ms/step - loss: 1.8103 - categorical_accuracy: 0.5820 - val_loss: 1.9318 - val_categorical_accuracy: 0.5220\n",
      "Epoch 31/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.7966 - categorical_accuracy: 0.5807 - val_loss: 1.9263 - val_categorical_accuracy: 0.5180\n",
      "Epoch 32/100\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 1.7826 - categorical_accuracy: 0.5920 - val_loss: 1.9319 - val_categorical_accuracy: 0.5090\n",
      "Epoch 33/100\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 1.7682 - categorical_accuracy: 0.6055 - val_loss: 1.9605 - val_categorical_accuracy: 0.5050\n",
      "Epoch 34/100\n",
      "499/500 [============================>.] - ETA: 0s - loss: 1.7619 - categorical_accuracy: 0.6060INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 17s 33ms/step - loss: 1.7612 - categorical_accuracy: 0.6062 - val_loss: 1.9210 - val_categorical_accuracy: 0.5240\n",
      "Epoch 35/100\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.7459 - categorical_accuracy: 0.6137INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 18s 37ms/step - loss: 1.7465 - categorical_accuracy: 0.6133 - val_loss: 1.9088 - val_categorical_accuracy: 0.5380\n",
      "Epoch 36/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.7414 - categorical_accuracy: 0.6137 - val_loss: 1.9171 - val_categorical_accuracy: 0.5350\n",
      "Epoch 37/100\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.7347 - categorical_accuracy: 0.6177INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 17s 34ms/step - loss: 1.7343 - categorical_accuracy: 0.6180 - val_loss: 1.8860 - val_categorical_accuracy: 0.5490\n",
      "Epoch 38/100\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 1.7249 - categorical_accuracy: 0.6250 - val_loss: 1.8955 - val_categorical_accuracy: 0.5450\n",
      "Epoch 39/100\n",
      "497/500 [============================>.] - ETA: 0s - loss: 1.7156 - categorical_accuracy: 0.6295INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 18s 35ms/step - loss: 1.7152 - categorical_accuracy: 0.6298 - val_loss: 1.8725 - val_categorical_accuracy: 0.5570\n",
      "Epoch 40/100\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 1.7130 - categorical_accuracy: 0.6323 - val_loss: 1.8992 - val_categorical_accuracy: 0.5490\n",
      "Epoch 41/100\n",
      "495/500 [============================>.] - ETA: 0s - loss: 1.7034 - categorical_accuracy: 0.6371INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 18s 37ms/step - loss: 1.7033 - categorical_accuracy: 0.6375 - val_loss: 1.8791 - val_categorical_accuracy: 0.5590\n",
      "Epoch 42/100\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 1.6868 - categorical_accuracy: 0.6455 - val_loss: 1.8964 - val_categorical_accuracy: 0.5470\n",
      "Epoch 43/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.6781 - categorical_accuracy: 0.6480 - val_loss: 1.9090 - val_categorical_accuracy: 0.5370\n",
      "Epoch 44/100\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 1.6795 - categorical_accuracy: 0.6470 - val_loss: 1.8832 - val_categorical_accuracy: 0.5590\n",
      "Epoch 45/100\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 1.6644 - categorical_accuracy: 0.6538 - val_loss: 1.9098 - val_categorical_accuracy: 0.5470\n",
      "Epoch 46/100\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.6605 - categorical_accuracy: 0.6550INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-1.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 17s 34ms/step - loss: 1.6608 - categorical_accuracy: 0.6555 - val_loss: 1.8814 - val_categorical_accuracy: 0.5640\n",
      "Epoch 47/100\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 1.6553 - categorical_accuracy: 0.6565 - val_loss: 1.9009 - val_categorical_accuracy: 0.5580\n",
      "Epoch 48/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.6518 - categorical_accuracy: 0.6660 - val_loss: 1.8978 - val_categorical_accuracy: 0.5590\n",
      "Epoch 49/100\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 1.6451 - categorical_accuracy: 0.6653 - val_loss: 1.9081 - val_categorical_accuracy: 0.5450\n",
      "Epoch 50/100\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 1.7349 - categorical_accuracy: 0.6160 - val_loss: 1.9325 - val_categorical_accuracy: 0.5400\n",
      "Epoch 51/100\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 1.7275 - categorical_accuracy: 0.6150 - val_loss: 2.0032 - val_categorical_accuracy: 0.4950\n",
      "Epoch 52/100\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 1.7261 - categorical_accuracy: 0.6227 - val_loss: 1.9376 - val_categorical_accuracy: 0.5170\n",
      "Epoch 53/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.7051 - categorical_accuracy: 0.6320 - val_loss: 1.9427 - val_categorical_accuracy: 0.5220\n",
      "Epoch 54/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.6827 - categorical_accuracy: 0.6492 - val_loss: 1.9294 - val_categorical_accuracy: 0.5380\n",
      "Epoch 55/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.6768 - categorical_accuracy: 0.6500 - val_loss: 1.9304 - val_categorical_accuracy: 0.5420\n",
      "Epoch 56/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.6794 - categorical_accuracy: 0.6438 - val_loss: 1.9679 - val_categorical_accuracy: 0.5340\n",
      "Epoch 57/100\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 1.6762 - categorical_accuracy: 0.6575 - val_loss: 1.9223 - val_categorical_accuracy: 0.5550\n",
      "Epoch 58/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.6632 - categorical_accuracy: 0.6585 - val_loss: 1.9783 - val_categorical_accuracy: 0.5370\n",
      "Epoch 59/100\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 1.6468 - categorical_accuracy: 0.6645 - val_loss: 1.9616 - val_categorical_accuracy: 0.5270\n",
      "Epoch 60/100\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 1.6441 - categorical_accuracy: 0.6685 - val_loss: 1.9527 - val_categorical_accuracy: 0.5340\n",
      "Epoch 61/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.6136 - categorical_accuracy: 0.6835 - val_loss: 1.9553 - val_categorical_accuracy: 0.5460\n",
      "Epoch 62/100\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 1.6208 - categorical_accuracy: 0.6770 - val_loss: 1.9552 - val_categorical_accuracy: 0.5400\n",
      "Epoch 63/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.6189 - categorical_accuracy: 0.6840 - val_loss: 1.9713 - val_categorical_accuracy: 0.5420\n",
      "Epoch 64/100\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 1.5896 - categorical_accuracy: 0.6952 - val_loss: 1.9466 - val_categorical_accuracy: 0.5430\n",
      "Epoch 65/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.6115 - categorical_accuracy: 0.6913 - val_loss: 2.0110 - val_categorical_accuracy: 0.5130\n",
      "Epoch 66/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.5920 - categorical_accuracy: 0.6963 - val_loss: 2.0024 - val_categorical_accuracy: 0.5310\n",
      "Epoch 67/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.5719 - categorical_accuracy: 0.7085 - val_loss: 2.0212 - val_categorical_accuracy: 0.5170\n",
      "Epoch 68/100\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 1.5659 - categorical_accuracy: 0.7078 - val_loss: 2.0284 - val_categorical_accuracy: 0.5250\n",
      "Epoch 69/100\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 1.5577 - categorical_accuracy: 0.7178 - val_loss: 2.0479 - val_categorical_accuracy: 0.5160\n",
      "Epoch 70/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.5313 - categorical_accuracy: 0.7372 - val_loss: 2.0360 - val_categorical_accuracy: 0.5160\n",
      "Epoch 71/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.5477 - categorical_accuracy: 0.7255 - val_loss: 2.0281 - val_categorical_accuracy: 0.5240\n",
      "Epoch 72/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.5204 - categorical_accuracy: 0.7390 - val_loss: 2.0664 - val_categorical_accuracy: 0.5140\n",
      "Epoch 73/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.5215 - categorical_accuracy: 0.7418 - val_loss: 2.0332 - val_categorical_accuracy: 0.5270\n",
      "Epoch 74/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.5281 - categorical_accuracy: 0.7423 - val_loss: 2.0509 - val_categorical_accuracy: 0.5350\n",
      "Epoch 75/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.5019 - categorical_accuracy: 0.7560 - val_loss: 2.0476 - val_categorical_accuracy: 0.5330\n",
      "Epoch 76/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.5159 - categorical_accuracy: 0.7458 - val_loss: 2.0720 - val_categorical_accuracy: 0.5100\n",
      "Epoch 77/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.5227 - categorical_accuracy: 0.7395 - val_loss: 2.0690 - val_categorical_accuracy: 0.5310\n",
      "Epoch 78/100\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 1.4886 - categorical_accuracy: 0.7648 - val_loss: 2.0737 - val_categorical_accuracy: 0.5230\n",
      "Epoch 79/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.4817 - categorical_accuracy: 0.7697 - val_loss: 2.0826 - val_categorical_accuracy: 0.5210\n",
      "Epoch 80/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.4707 - categorical_accuracy: 0.7772 - val_loss: 2.0995 - val_categorical_accuracy: 0.5180\n",
      "Epoch 81/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.4513 - categorical_accuracy: 0.7880 - val_loss: 2.0582 - val_categorical_accuracy: 0.5380\n",
      "Epoch 82/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.4439 - categorical_accuracy: 0.7930 - val_loss: 2.1116 - val_categorical_accuracy: 0.5270\n",
      "Epoch 83/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.4463 - categorical_accuracy: 0.7903 - val_loss: 2.1074 - val_categorical_accuracy: 0.5130\n",
      "Epoch 84/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.4225 - categorical_accuracy: 0.8033 - val_loss: 2.1216 - val_categorical_accuracy: 0.5040\n",
      "Epoch 85/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.4255 - categorical_accuracy: 0.8055 - val_loss: 2.1713 - val_categorical_accuracy: 0.4890\n",
      "Epoch 86/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.4234 - categorical_accuracy: 0.8102 - val_loss: 2.1315 - val_categorical_accuracy: 0.5070\n",
      "Epoch 87/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.4150 - categorical_accuracy: 0.8117 - val_loss: 2.1572 - val_categorical_accuracy: 0.5080\n",
      "Epoch 88/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.4128 - categorical_accuracy: 0.8092 - val_loss: 2.1452 - val_categorical_accuracy: 0.5120\n",
      "Epoch 89/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.4204 - categorical_accuracy: 0.8062 - val_loss: 2.1034 - val_categorical_accuracy: 0.5160\n",
      "Epoch 90/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.3921 - categorical_accuracy: 0.8248 - val_loss: 2.0858 - val_categorical_accuracy: 0.5340\n",
      "Epoch 91/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.3816 - categorical_accuracy: 0.8325 - val_loss: 2.1291 - val_categorical_accuracy: 0.5250\n",
      "Epoch 92/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.3654 - categorical_accuracy: 0.8397 - val_loss: 2.1012 - val_categorical_accuracy: 0.5290\n",
      "Epoch 93/100\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 1.3670 - categorical_accuracy: 0.8378 - val_loss: 2.1317 - val_categorical_accuracy: 0.5230\n",
      "Epoch 94/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.3565 - categorical_accuracy: 0.8470 - val_loss: 2.1333 - val_categorical_accuracy: 0.5240\n",
      "Epoch 95/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.3519 - categorical_accuracy: 0.8455 - val_loss: 2.1428 - val_categorical_accuracy: 0.5180\n",
      "Epoch 96/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.3422 - categorical_accuracy: 0.8540 - val_loss: 2.1455 - val_categorical_accuracy: 0.5250\n",
      "Epoch 97/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.3407 - categorical_accuracy: 0.8550 - val_loss: 2.1569 - val_categorical_accuracy: 0.5060\n",
      "Epoch 98/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.3248 - categorical_accuracy: 0.8618 - val_loss: 2.1541 - val_categorical_accuracy: 0.5270\n",
      "Epoch 99/100\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 1.3306 - categorical_accuracy: 0.8570 - val_loss: 2.1691 - val_categorical_accuracy: 0.5150\n",
      "Epoch 100/100\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 1.4173 - categorical_accuracy: 0.8055 - val_loss: 2.1781 - val_categorical_accuracy: 0.5010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁▂▂▃▃▃▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇██████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁███▇▇▇▆▆▅▅▄▄▃▃▂▂▁▁▁█</td></tr><tr><td>epoch/loss</td><td>█▇▆▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁▂▃▄▄▅▅▆▆▇▇▇▇▇▇█████▇▇▇▇██▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>epoch/val_loss</td><td>█▆▅▅▄▄▃▃▃▂▁▂▂▂▂▁▁▁▁▁▃▂▂▂▂▂▃▃▃▃▃▄▄▄▅▄▄▄▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.8055</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>1.41734</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.501</td></tr><tr><td>epoch/val_loss</td><td>2.1781</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM128-Dense128-Dense256-LipsEyesHandsPose_5k</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/cwtokq0w' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/cwtokq0w</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240416_135553-cwtokq0w/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f9c7ef6ef10>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(256, return_sequences=True, input_shape=(X_shape[0], X_shape[1]),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LSTM(64, return_sequences=False,\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"LSTM1-1.tf\"),\n",
    "            run_name=\"LSTM128-Dense128-Dense256-LipsEyesHandsPose_5k\",\n",
    "            mediapipe_features=\"reduced-LipsEyesHandsPose\",\n",
    "            USE_WANDB=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240416_140936-ktx8tmwx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/ktx8tmwx' target=\"_blank\">LSTM128-Dense128-Dense256-LipsEyesHandsPose_5k</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/ktx8tmwx' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/ktx8tmwx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "    492/Unknown - 7s 6ms/step - loss: 2.5615 - categorical_accuracy: 0.1148"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 14:09:44.743266: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 1486800163596208403\n",
      "2024-04-16 14:09:44.743337: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 1011830033887524483\n",
      "2024-04-16 14:09:44.743361: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 16098010830598473736\n",
      "2024-04-16 14:09:44.743378: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4153888742695321310\n",
      "2024-04-16 14:09:45.871767: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 3771462535022358857\n",
      "2024-04-16 14:09:45.871842: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 3371227081520258209\n",
      "2024-04-16 14:09:45.871866: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 13364783011415422641\n",
      "2024-04-16 14:09:45.871913: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12156306670906514424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 15s 22ms/step - loss: 2.5608 - categorical_accuracy: 0.1163 - val_loss: 2.4338 - val_categorical_accuracy: 0.1850\n",
      "Epoch 2/100\n",
      "492/500 [============================>.] - ETA: 0s - loss: 2.4119 - categorical_accuracy: 0.1865INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 2.4107 - categorical_accuracy: 0.1870 - val_loss: 2.3455 - val_categorical_accuracy: 0.2300\n",
      "Epoch 3/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.3586 - categorical_accuracy: 0.2167INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.3586 - categorical_accuracy: 0.2167 - val_loss: 2.3097 - val_categorical_accuracy: 0.2480\n",
      "Epoch 4/100\n",
      "492/500 [============================>.] - ETA: 0s - loss: 2.3197 - categorical_accuracy: 0.2360INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 2.3188 - categorical_accuracy: 0.2362 - val_loss: 2.2667 - val_categorical_accuracy: 0.2700\n",
      "Epoch 5/100\n",
      "494/500 [============================>.] - ETA: 0s - loss: 2.2914 - categorical_accuracy: 0.2546INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 23ms/step - loss: 2.2908 - categorical_accuracy: 0.2550 - val_loss: 2.2512 - val_categorical_accuracy: 0.2760\n",
      "Epoch 6/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 2.2600 - categorical_accuracy: 0.2767 - val_loss: 2.2381 - val_categorical_accuracy: 0.2760\n",
      "Epoch 7/100\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.2325 - categorical_accuracy: 0.2923INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.2316 - categorical_accuracy: 0.2928 - val_loss: 2.2100 - val_categorical_accuracy: 0.3060\n",
      "Epoch 8/100\n",
      "496/500 [============================>.] - ETA: 0s - loss: 2.2111 - categorical_accuracy: 0.3042INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.2101 - categorical_accuracy: 0.3043 - val_loss: 2.1729 - val_categorical_accuracy: 0.3300\n",
      "Epoch 9/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 2.1911 - categorical_accuracy: 0.3192 - val_loss: 2.1825 - val_categorical_accuracy: 0.3260\n",
      "Epoch 10/100\n",
      "492/500 [============================>.] - ETA: 0s - loss: 2.1640 - categorical_accuracy: 0.3262INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.1631 - categorical_accuracy: 0.3270 - val_loss: 2.1625 - val_categorical_accuracy: 0.3380\n",
      "Epoch 11/100\n",
      "495/500 [============================>.] - ETA: 0s - loss: 2.1517 - categorical_accuracy: 0.3404INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 2.1498 - categorical_accuracy: 0.3415 - val_loss: 2.1258 - val_categorical_accuracy: 0.3690\n",
      "Epoch 12/100\n",
      "498/500 [============================>.] - ETA: 0s - loss: 2.1335 - categorical_accuracy: 0.3509INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.1328 - categorical_accuracy: 0.3517 - val_loss: 2.1154 - val_categorical_accuracy: 0.3830\n",
      "Epoch 13/100\n",
      "493/500 [============================>.] - ETA: 0s - loss: 2.1179 - categorical_accuracy: 0.3600INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.1164 - categorical_accuracy: 0.3610 - val_loss: 2.0978 - val_categorical_accuracy: 0.3940\n",
      "Epoch 14/100\n",
      "499/500 [============================>.] - ETA: 0s - loss: 2.1033 - categorical_accuracy: 0.3720INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 2.1025 - categorical_accuracy: 0.3728 - val_loss: 2.0908 - val_categorical_accuracy: 0.4010\n",
      "Epoch 15/100\n",
      "495/500 [============================>.] - ETA: 0s - loss: 2.0797 - categorical_accuracy: 0.3823INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.0776 - categorical_accuracy: 0.3840 - val_loss: 2.0803 - val_categorical_accuracy: 0.4020\n",
      "Epoch 16/100\n",
      "494/500 [============================>.] - ETA: 0s - loss: 2.0581 - categorical_accuracy: 0.3965INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.0561 - categorical_accuracy: 0.3977 - val_loss: 2.0617 - val_categorical_accuracy: 0.4260\n",
      "Epoch 17/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 2.0276 - categorical_accuracy: 0.4232 - val_loss: 2.0551 - val_categorical_accuracy: 0.4090\n",
      "Epoch 18/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.0077 - categorical_accuracy: 0.4372INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.0077 - categorical_accuracy: 0.4372 - val_loss: 2.0502 - val_categorical_accuracy: 0.4300\n",
      "Epoch 19/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9846 - categorical_accuracy: 0.4498INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 1.9846 - categorical_accuracy: 0.4498 - val_loss: 2.0284 - val_categorical_accuracy: 0.4420\n",
      "Epoch 20/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.9625 - categorical_accuracy: 0.4580 - val_loss: 2.0276 - val_categorical_accuracy: 0.4400\n",
      "Epoch 21/100\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.9434 - categorical_accuracy: 0.4743INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 1.9423 - categorical_accuracy: 0.4748 - val_loss: 2.0065 - val_categorical_accuracy: 0.4600\n",
      "Epoch 22/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9224 - categorical_accuracy: 0.4843INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 1.9224 - categorical_accuracy: 0.4843 - val_loss: 2.0133 - val_categorical_accuracy: 0.4640\n",
      "Epoch 23/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.8994 - categorical_accuracy: 0.5040 - val_loss: 2.0144 - val_categorical_accuracy: 0.4500\n",
      "Epoch 24/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.8874 - categorical_accuracy: 0.5135 - val_loss: 1.9796 - val_categorical_accuracy: 0.4600\n",
      "Epoch 25/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.8630 - categorical_accuracy: 0.5213 - val_loss: 2.0062 - val_categorical_accuracy: 0.4490\n",
      "Epoch 26/100\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.8519 - categorical_accuracy: 0.5310INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 1.8516 - categorical_accuracy: 0.5307 - val_loss: 1.9819 - val_categorical_accuracy: 0.4740\n",
      "Epoch 27/100\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 1.8335 - categorical_accuracy: 0.5418 - val_loss: 1.9861 - val_categorical_accuracy: 0.4650\n",
      "Epoch 28/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.8175 - categorical_accuracy: 0.5490 - val_loss: 1.9831 - val_categorical_accuracy: 0.4740\n",
      "Epoch 29/100\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.8069 - categorical_accuracy: 0.5592INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 1.8071 - categorical_accuracy: 0.5585 - val_loss: 1.9749 - val_categorical_accuracy: 0.4840\n",
      "Epoch 30/100\n",
      "493/500 [============================>.] - ETA: 0s - loss: 1.7969 - categorical_accuracy: 0.5682INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 1.7967 - categorical_accuracy: 0.5680 - val_loss: 1.9357 - val_categorical_accuracy: 0.4920\n",
      "Epoch 31/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.7833 - categorical_accuracy: 0.5713 - val_loss: 1.9647 - val_categorical_accuracy: 0.4690\n",
      "Epoch 32/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.7725 - categorical_accuracy: 0.5857 - val_loss: 1.9586 - val_categorical_accuracy: 0.4710\n",
      "Epoch 33/100\n",
      "495/500 [============================>.] - ETA: 0s - loss: 1.7578 - categorical_accuracy: 0.5866INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 1.7579 - categorical_accuracy: 0.5865 - val_loss: 1.9575 - val_categorical_accuracy: 0.5010\n",
      "Epoch 34/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.7451 - categorical_accuracy: 0.5972 - val_loss: 1.9755 - val_categorical_accuracy: 0.4710\n",
      "Epoch 35/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.7301 - categorical_accuracy: 0.6012 - val_loss: 1.9636 - val_categorical_accuracy: 0.4850\n",
      "Epoch 36/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.7112 - categorical_accuracy: 0.6140 - val_loss: 1.9580 - val_categorical_accuracy: 0.4900\n",
      "Epoch 37/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.6901 - categorical_accuracy: 0.6215 - val_loss: 1.9769 - val_categorical_accuracy: 0.4960\n",
      "Epoch 38/100\n",
      "494/500 [============================>.] - ETA: 0s - loss: 1.6835 - categorical_accuracy: 0.6301INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 1.6833 - categorical_accuracy: 0.6302 - val_loss: 1.9598 - val_categorical_accuracy: 0.5060\n",
      "Epoch 39/100\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.6689 - categorical_accuracy: 0.6363INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 1.6686 - categorical_accuracy: 0.6373 - val_loss: 1.9248 - val_categorical_accuracy: 0.5170\n",
      "Epoch 40/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.6529 - categorical_accuracy: 0.6505 - val_loss: 1.9565 - val_categorical_accuracy: 0.5010\n",
      "Epoch 41/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.6555 - categorical_accuracy: 0.6470 - val_loss: 1.9409 - val_categorical_accuracy: 0.5160\n",
      "Epoch 42/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.6444 - categorical_accuracy: 0.6518 - val_loss: 1.9646 - val_categorical_accuracy: 0.4990\n",
      "Epoch 43/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.6323 - categorical_accuracy: 0.6612 - val_loss: 1.9499 - val_categorical_accuracy: 0.5120\n",
      "Epoch 44/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.6375 - categorical_accuracy: 0.6572 - val_loss: 1.9380 - val_categorical_accuracy: 0.5120\n",
      "Epoch 45/100\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.6233 - categorical_accuracy: 0.6687INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-2.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 1.6227 - categorical_accuracy: 0.6693 - val_loss: 1.9333 - val_categorical_accuracy: 0.5230\n",
      "Epoch 46/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.6044 - categorical_accuracy: 0.6758 - val_loss: 1.9456 - val_categorical_accuracy: 0.5120\n",
      "Epoch 47/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.5836 - categorical_accuracy: 0.6902 - val_loss: 1.9573 - val_categorical_accuracy: 0.5100\n",
      "Epoch 48/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.5832 - categorical_accuracy: 0.6910 - val_loss: 1.9497 - val_categorical_accuracy: 0.5120\n",
      "Epoch 49/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.5794 - categorical_accuracy: 0.6913 - val_loss: 1.9615 - val_categorical_accuracy: 0.5100\n",
      "Epoch 50/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.6909 - categorical_accuracy: 0.6180 - val_loss: 2.0180 - val_categorical_accuracy: 0.4730\n",
      "Epoch 51/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.6986 - categorical_accuracy: 0.6210 - val_loss: 2.0287 - val_categorical_accuracy: 0.4620\n",
      "Epoch 52/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.6847 - categorical_accuracy: 0.6280 - val_loss: 2.0231 - val_categorical_accuracy: 0.4740\n",
      "Epoch 53/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.6789 - categorical_accuracy: 0.6308 - val_loss: 2.0826 - val_categorical_accuracy: 0.4670\n",
      "Epoch 54/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.6524 - categorical_accuracy: 0.6482 - val_loss: 2.1339 - val_categorical_accuracy: 0.4440\n",
      "Epoch 55/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.6527 - categorical_accuracy: 0.6490 - val_loss: 2.0394 - val_categorical_accuracy: 0.4880\n",
      "Epoch 56/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.6411 - categorical_accuracy: 0.6628 - val_loss: 2.1075 - val_categorical_accuracy: 0.4510\n",
      "Epoch 57/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.6208 - categorical_accuracy: 0.6668 - val_loss: 2.0334 - val_categorical_accuracy: 0.4870\n",
      "Epoch 58/100\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 1.6143 - categorical_accuracy: 0.6708 - val_loss: 2.0018 - val_categorical_accuracy: 0.4930\n",
      "Epoch 59/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.6045 - categorical_accuracy: 0.6783 - val_loss: 2.0906 - val_categorical_accuracy: 0.4640\n",
      "Epoch 60/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.5962 - categorical_accuracy: 0.6798 - val_loss: 2.0706 - val_categorical_accuracy: 0.4720\n",
      "Epoch 61/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.5879 - categorical_accuracy: 0.6790 - val_loss: 2.0633 - val_categorical_accuracy: 0.4730\n",
      "Epoch 62/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.5805 - categorical_accuracy: 0.6970 - val_loss: 2.0883 - val_categorical_accuracy: 0.4870\n",
      "Epoch 63/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.5824 - categorical_accuracy: 0.6883 - val_loss: 2.1138 - val_categorical_accuracy: 0.4710\n",
      "Epoch 64/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.5530 - categorical_accuracy: 0.7097 - val_loss: 2.0844 - val_categorical_accuracy: 0.4650\n",
      "Epoch 65/100\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 1.5371 - categorical_accuracy: 0.7188 - val_loss: 2.1477 - val_categorical_accuracy: 0.4440\n",
      "Epoch 66/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.5338 - categorical_accuracy: 0.7155 - val_loss: 2.1253 - val_categorical_accuracy: 0.4790\n",
      "Epoch 67/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.5295 - categorical_accuracy: 0.7237 - val_loss: 2.0731 - val_categorical_accuracy: 0.4890\n",
      "Epoch 68/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.5034 - categorical_accuracy: 0.7360 - val_loss: 2.0766 - val_categorical_accuracy: 0.5020\n",
      "Epoch 69/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.4801 - categorical_accuracy: 0.7500 - val_loss: 2.1127 - val_categorical_accuracy: 0.4790\n",
      "Epoch 70/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.4986 - categorical_accuracy: 0.7442 - val_loss: 2.0308 - val_categorical_accuracy: 0.4990\n",
      "Epoch 71/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.4577 - categorical_accuracy: 0.7655 - val_loss: 2.1170 - val_categorical_accuracy: 0.4670\n",
      "Epoch 72/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.4665 - categorical_accuracy: 0.7585 - val_loss: 2.0446 - val_categorical_accuracy: 0.5060\n",
      "Epoch 73/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.4726 - categorical_accuracy: 0.7590 - val_loss: 2.1088 - val_categorical_accuracy: 0.4830\n",
      "Epoch 74/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.4589 - categorical_accuracy: 0.7688 - val_loss: 2.0832 - val_categorical_accuracy: 0.4980\n",
      "Epoch 75/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.4539 - categorical_accuracy: 0.7673 - val_loss: 2.1136 - val_categorical_accuracy: 0.5000\n",
      "Epoch 76/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.4322 - categorical_accuracy: 0.7770 - val_loss: 2.1040 - val_categorical_accuracy: 0.4980\n",
      "Epoch 77/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.4313 - categorical_accuracy: 0.7805 - val_loss: 2.1374 - val_categorical_accuracy: 0.4890\n",
      "Epoch 78/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.4180 - categorical_accuracy: 0.7875 - val_loss: 2.1274 - val_categorical_accuracy: 0.5030\n",
      "Epoch 79/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.4089 - categorical_accuracy: 0.7937 - val_loss: 2.1607 - val_categorical_accuracy: 0.4850\n",
      "Epoch 80/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.4039 - categorical_accuracy: 0.7987 - val_loss: 2.1774 - val_categorical_accuracy: 0.4710\n",
      "Epoch 81/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.3953 - categorical_accuracy: 0.8060 - val_loss: 2.1585 - val_categorical_accuracy: 0.4860\n",
      "Epoch 82/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.3934 - categorical_accuracy: 0.8062 - val_loss: 2.1623 - val_categorical_accuracy: 0.4880\n",
      "Epoch 83/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.3748 - categorical_accuracy: 0.8133 - val_loss: 2.1303 - val_categorical_accuracy: 0.5020\n",
      "Epoch 84/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.3584 - categorical_accuracy: 0.8270 - val_loss: 2.1547 - val_categorical_accuracy: 0.4860\n",
      "Epoch 85/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.3497 - categorical_accuracy: 0.8328 - val_loss: 2.1365 - val_categorical_accuracy: 0.4960\n",
      "Epoch 86/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.3426 - categorical_accuracy: 0.8363 - val_loss: 2.1453 - val_categorical_accuracy: 0.5160\n",
      "Epoch 87/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.3400 - categorical_accuracy: 0.8393 - val_loss: 2.1482 - val_categorical_accuracy: 0.5030\n",
      "Epoch 88/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.3480 - categorical_accuracy: 0.8378 - val_loss: 2.1737 - val_categorical_accuracy: 0.4970\n",
      "Epoch 89/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.3361 - categorical_accuracy: 0.8438 - val_loss: 2.1707 - val_categorical_accuracy: 0.4930\n",
      "Epoch 90/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.3241 - categorical_accuracy: 0.8493 - val_loss: 2.1771 - val_categorical_accuracy: 0.5040\n",
      "Epoch 91/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.3094 - categorical_accuracy: 0.8553 - val_loss: 2.1689 - val_categorical_accuracy: 0.4900\n",
      "Epoch 92/100\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 1.2981 - categorical_accuracy: 0.8643 - val_loss: 2.1857 - val_categorical_accuracy: 0.4900\n",
      "Epoch 93/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.2804 - categorical_accuracy: 0.8715 - val_loss: 2.1855 - val_categorical_accuracy: 0.5030\n",
      "Epoch 94/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.2690 - categorical_accuracy: 0.8815 - val_loss: 2.2197 - val_categorical_accuracy: 0.4890\n",
      "Epoch 95/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.2775 - categorical_accuracy: 0.8770 - val_loss: 2.2532 - val_categorical_accuracy: 0.4670\n",
      "Epoch 96/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.2664 - categorical_accuracy: 0.8798 - val_loss: 2.2330 - val_categorical_accuracy: 0.4780\n",
      "Epoch 97/100\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 1.2751 - categorical_accuracy: 0.8770 - val_loss: 2.2483 - val_categorical_accuracy: 0.4860\n",
      "Epoch 98/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.2622 - categorical_accuracy: 0.8867 - val_loss: 2.2343 - val_categorical_accuracy: 0.4930\n",
      "Epoch 99/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.2577 - categorical_accuracy: 0.8848 - val_loss: 2.2411 - val_categorical_accuracy: 0.4990\n",
      "Epoch 100/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.3915 - categorical_accuracy: 0.8058 - val_loss: 2.2444 - val_categorical_accuracy: 0.4800\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁▂▂▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇██████▇</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁███▇▇▇▆▆▅▅▄▄▃▃▂▂▁▁▁█</td></tr><tr><td>epoch/loss</td><td>█▇▆▆▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▂</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁▂▃▄▅▅▆▆▇▇▇▇▇▇▇█████▇▆▇▇▇▇▇▇██▇▇▇▇█▇▇▇▇▇</td></tr><tr><td>epoch/val_loss</td><td>█▆▅▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▂▂▄▄▃▃▃▃▄▃▃▄▄▄▄▄▄▅▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.80575</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>1.39152</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.48</td></tr><tr><td>epoch/val_loss</td><td>2.24442</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM128-Dense128-Dense256-LipsEyesHandsPose_5k</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/ktx8tmwx' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/ktx8tmwx</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240416_140936-ktx8tmwx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f9500eae310>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(256, return_sequences=False, input_shape=(X_shape[0], X_shape[1]),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"LSTM1-2.tf\"),\n",
    "            run_name=\"LSTM128-Dense128-Dense256-LipsEyesHandsPose_5k\",\n",
    "            mediapipe_features=\"reduced-LipsEyesHandsPose\",\n",
    "            USE_WANDB=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240416_142143-iu5byyhv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/iu5byyhv' target=\"_blank\">LSTM64-Dense128-Dense256-LipsEyesHandsPose_5k</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/iu5byyhv' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/iu5byyhv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "    494/Unknown - 6s 6ms/step - loss: 2.5751 - categorical_accuracy: 0.1043"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 14:21:49.586712: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7580302709240232395\n",
      "2024-04-16 14:21:49.586753: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 1486800163596208403\n",
      "2024-04-16 14:21:49.586767: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 1011830033887524483\n",
      "2024-04-16 14:21:49.586779: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4153888742695321310\n",
      "2024-04-16 14:21:50.744220: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 3771462535022358857\n",
      "2024-04-16 14:21:50.744267: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 3371227081520258209\n",
      "2024-04-16 14:21:50.744283: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12156306670906514424\n",
      "2024-04-16 14:21:50.744310: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7717513745239281288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 14s 22ms/step - loss: 2.5744 - categorical_accuracy: 0.1063 - val_loss: 2.5371 - val_categorical_accuracy: 0.1400\n",
      "Epoch 2/100\n",
      "498/500 [============================>.] - ETA: 0s - loss: 2.4465 - categorical_accuracy: 0.1574INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 2.4459 - categorical_accuracy: 0.1567 - val_loss: 2.3761 - val_categorical_accuracy: 0.1940\n",
      "Epoch 3/100\n",
      "493/500 [============================>.] - ETA: 0s - loss: 2.3787 - categorical_accuracy: 0.1833INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.3786 - categorical_accuracy: 0.1835 - val_loss: 2.3476 - val_categorical_accuracy: 0.2160\n",
      "Epoch 4/100\n",
      "496/500 [============================>.] - ETA: 0s - loss: 2.3495 - categorical_accuracy: 0.2036INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.3490 - categorical_accuracy: 0.2030 - val_loss: 2.3199 - val_categorical_accuracy: 0.2310\n",
      "Epoch 5/100\n",
      "494/500 [============================>.] - ETA: 0s - loss: 2.3254 - categorical_accuracy: 0.2234INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.3252 - categorical_accuracy: 0.2230 - val_loss: 2.2948 - val_categorical_accuracy: 0.2550\n",
      "Epoch 6/100\n",
      "498/500 [============================>.] - ETA: 0s - loss: 2.3054 - categorical_accuracy: 0.2415INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 2.3053 - categorical_accuracy: 0.2412 - val_loss: 2.2704 - val_categorical_accuracy: 0.2700\n",
      "Epoch 7/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 2.2840 - categorical_accuracy: 0.2447 - val_loss: 2.2611 - val_categorical_accuracy: 0.2690\n",
      "Epoch 8/100\n",
      "494/500 [============================>.] - ETA: 0s - loss: 2.2694 - categorical_accuracy: 0.2424INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.2693 - categorical_accuracy: 0.2428 - val_loss: 2.2539 - val_categorical_accuracy: 0.2750\n",
      "Epoch 9/100\n",
      "494/500 [============================>.] - ETA: 0s - loss: 2.2539 - categorical_accuracy: 0.2551INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 23ms/step - loss: 2.2537 - categorical_accuracy: 0.2548 - val_loss: 2.2384 - val_categorical_accuracy: 0.2790\n",
      "Epoch 10/100\n",
      "491/500 [============================>.] - ETA: 0s - loss: 2.2448 - categorical_accuracy: 0.2747INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.2444 - categorical_accuracy: 0.2755 - val_loss: 2.2252 - val_categorical_accuracy: 0.2910\n",
      "Epoch 11/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.2272 - categorical_accuracy: 0.2775INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 2.2272 - categorical_accuracy: 0.2775 - val_loss: 2.2113 - val_categorical_accuracy: 0.3020\n",
      "Epoch 12/100\n",
      "492/500 [============================>.] - ETA: 0s - loss: 2.2144 - categorical_accuracy: 0.2950INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.2131 - categorical_accuracy: 0.2962 - val_loss: 2.1871 - val_categorical_accuracy: 0.3120\n",
      "Epoch 13/100\n",
      "499/500 [============================>.] - ETA: 0s - loss: 2.1986 - categorical_accuracy: 0.3056INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.1982 - categorical_accuracy: 0.3060 - val_loss: 2.1749 - val_categorical_accuracy: 0.3290\n",
      "Epoch 14/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 2.1849 - categorical_accuracy: 0.3122 - val_loss: 2.1619 - val_categorical_accuracy: 0.3180\n",
      "Epoch 15/100\n",
      "492/500 [============================>.] - ETA: 0s - loss: 2.1735 - categorical_accuracy: 0.3211INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.1721 - categorical_accuracy: 0.3220 - val_loss: 2.1553 - val_categorical_accuracy: 0.3330\n",
      "Epoch 16/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 2.1567 - categorical_accuracy: 0.3368 - val_loss: 2.1553 - val_categorical_accuracy: 0.3200\n",
      "Epoch 17/100\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.1440 - categorical_accuracy: 0.3400INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.1425 - categorical_accuracy: 0.3408 - val_loss: 2.1429 - val_categorical_accuracy: 0.3410\n",
      "Epoch 18/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 2.1320 - categorical_accuracy: 0.3483 - val_loss: 2.1302 - val_categorical_accuracy: 0.3340\n",
      "Epoch 19/100\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.1225 - categorical_accuracy: 0.3521INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.1211 - categorical_accuracy: 0.3530 - val_loss: 2.1167 - val_categorical_accuracy: 0.3450\n",
      "Epoch 20/100\n",
      "498/500 [============================>.] - ETA: 0s - loss: 2.1092 - categorical_accuracy: 0.3577INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 2.1085 - categorical_accuracy: 0.3580 - val_loss: 2.1173 - val_categorical_accuracy: 0.3520\n",
      "Epoch 21/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 2.0984 - categorical_accuracy: 0.3655 - val_loss: 2.1079 - val_categorical_accuracy: 0.3430\n",
      "Epoch 22/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 2.0867 - categorical_accuracy: 0.3758 - val_loss: 2.1076 - val_categorical_accuracy: 0.3480\n",
      "Epoch 23/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 2.0776 - categorical_accuracy: 0.3823 - val_loss: 2.1120 - val_categorical_accuracy: 0.3500\n",
      "Epoch 24/100\n",
      "494/500 [============================>.] - ETA: 0s - loss: 2.0670 - categorical_accuracy: 0.3813INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.0656 - categorical_accuracy: 0.3825 - val_loss: 2.0905 - val_categorical_accuracy: 0.3650\n",
      "Epoch 25/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 2.0545 - categorical_accuracy: 0.3955 - val_loss: 2.0955 - val_categorical_accuracy: 0.3460\n",
      "Epoch 26/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 2.0429 - categorical_accuracy: 0.4060 - val_loss: 2.0900 - val_categorical_accuracy: 0.3590\n",
      "Epoch 27/100\n",
      "494/500 [============================>.] - ETA: 0s - loss: 2.0377 - categorical_accuracy: 0.4016INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 2.0369 - categorical_accuracy: 0.4022 - val_loss: 2.0765 - val_categorical_accuracy: 0.3730\n",
      "Epoch 28/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 2.0270 - categorical_accuracy: 0.4160 - val_loss: 2.0819 - val_categorical_accuracy: 0.3620\n",
      "Epoch 29/100\n",
      "498/500 [============================>.] - ETA: 0s - loss: 2.0123 - categorical_accuracy: 0.4275INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.0115 - categorical_accuracy: 0.4280 - val_loss: 2.0657 - val_categorical_accuracy: 0.3960\n",
      "Epoch 30/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 2.0050 - categorical_accuracy: 0.4297 - val_loss: 2.0684 - val_categorical_accuracy: 0.3890\n",
      "Epoch 31/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.9892 - categorical_accuracy: 0.4445 - val_loss: 2.0638 - val_categorical_accuracy: 0.3910\n",
      "Epoch 32/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.9824 - categorical_accuracy: 0.4502 - val_loss: 2.0563 - val_categorical_accuracy: 0.3950\n",
      "Epoch 33/100\n",
      "492/500 [============================>.] - ETA: 0s - loss: 1.9735 - categorical_accuracy: 0.4484INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 1.9726 - categorical_accuracy: 0.4485 - val_loss: 2.0426 - val_categorical_accuracy: 0.3990\n",
      "Epoch 34/100\n",
      "495/500 [============================>.] - ETA: 0s - loss: 1.9642 - categorical_accuracy: 0.4624INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 1.9631 - categorical_accuracy: 0.4625 - val_loss: 2.0376 - val_categorical_accuracy: 0.4090\n",
      "Epoch 35/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.9554 - categorical_accuracy: 0.4640 - val_loss: 2.0410 - val_categorical_accuracy: 0.3990\n",
      "Epoch 36/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.9462 - categorical_accuracy: 0.4737 - val_loss: 2.0567 - val_categorical_accuracy: 0.3940\n",
      "Epoch 37/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.9403 - categorical_accuracy: 0.4748 - val_loss: 2.0506 - val_categorical_accuracy: 0.3920\n",
      "Epoch 38/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.9330 - categorical_accuracy: 0.4782 - val_loss: 2.0266 - val_categorical_accuracy: 0.4050\n",
      "Epoch 39/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.9225 - categorical_accuracy: 0.4870 - val_loss: 2.0357 - val_categorical_accuracy: 0.3970\n",
      "Epoch 40/100\n",
      "495/500 [============================>.] - ETA: 0s - loss: 1.9169 - categorical_accuracy: 0.4854INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 1.9159 - categorical_accuracy: 0.4863 - val_loss: 2.0321 - val_categorical_accuracy: 0.4130\n",
      "Epoch 41/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.9114 - categorical_accuracy: 0.4877 - val_loss: 2.0368 - val_categorical_accuracy: 0.4110\n",
      "Epoch 42/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.9067 - categorical_accuracy: 0.4988 - val_loss: 2.0395 - val_categorical_accuracy: 0.4040\n",
      "Epoch 43/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.8968 - categorical_accuracy: 0.5005 - val_loss: 2.0267 - val_categorical_accuracy: 0.4100\n",
      "Epoch 44/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.8952 - categorical_accuracy: 0.4978 - val_loss: 2.0218 - val_categorical_accuracy: 0.4130\n",
      "Epoch 45/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.8929 - categorical_accuracy: 0.5015 - val_loss: 2.0344 - val_categorical_accuracy: 0.4060\n",
      "Epoch 46/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.8855 - categorical_accuracy: 0.5013 - val_loss: 2.0309 - val_categorical_accuracy: 0.4100\n",
      "Epoch 47/100\n",
      "499/500 [============================>.] - ETA: 0s - loss: 1.8845 - categorical_accuracy: 0.4997INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 1.8837 - categorical_accuracy: 0.5005 - val_loss: 2.0073 - val_categorical_accuracy: 0.4280\n",
      "Epoch 48/100\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.8736 - categorical_accuracy: 0.5154INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 1.8725 - categorical_accuracy: 0.5155 - val_loss: 2.0097 - val_categorical_accuracy: 0.4340\n",
      "Epoch 49/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.8667 - categorical_accuracy: 0.5135 - val_loss: 2.0133 - val_categorical_accuracy: 0.4250\n",
      "Epoch 50/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.9156 - categorical_accuracy: 0.4857 - val_loss: 2.0037 - val_categorical_accuracy: 0.4340\n",
      "Epoch 51/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.9073 - categorical_accuracy: 0.4910 - val_loss: 2.0392 - val_categorical_accuracy: 0.3960\n",
      "Epoch 52/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.9080 - categorical_accuracy: 0.4970 - val_loss: 2.0256 - val_categorical_accuracy: 0.4180\n",
      "Epoch 53/100\n",
      "494/500 [============================>.] - ETA: 0s - loss: 1.9019 - categorical_accuracy: 0.4909INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 1.9018 - categorical_accuracy: 0.4913 - val_loss: 1.9863 - val_categorical_accuracy: 0.4440\n",
      "Epoch 54/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.8826 - categorical_accuracy: 0.5033 - val_loss: 2.0288 - val_categorical_accuracy: 0.4320\n",
      "Epoch 55/100\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.8826 - categorical_accuracy: 0.5028INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 1.8816 - categorical_accuracy: 0.5040 - val_loss: 1.9954 - val_categorical_accuracy: 0.4530\n",
      "Epoch 56/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.8721 - categorical_accuracy: 0.5080 - val_loss: 2.0111 - val_categorical_accuracy: 0.4470\n",
      "Epoch 57/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.8705 - categorical_accuracy: 0.5182 - val_loss: 1.9979 - val_categorical_accuracy: 0.4500\n",
      "Epoch 58/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.8750 - categorical_accuracy: 0.5080 - val_loss: 1.9921 - val_categorical_accuracy: 0.4470\n",
      "Epoch 59/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.8504 - categorical_accuracy: 0.5255 - val_loss: 1.9855 - val_categorical_accuracy: 0.4470\n",
      "Epoch 60/100\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.8474 - categorical_accuracy: 0.5292INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 1.8461 - categorical_accuracy: 0.5300 - val_loss: 1.9910 - val_categorical_accuracy: 0.4550\n",
      "Epoch 61/100\n",
      "492/500 [============================>.] - ETA: 0s - loss: 1.8574 - categorical_accuracy: 0.5272INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 1.8573 - categorical_accuracy: 0.5270 - val_loss: 2.0022 - val_categorical_accuracy: 0.4610\n",
      "Epoch 62/100\n",
      "497/500 [============================>.] - ETA: 0s - loss: 1.8352 - categorical_accuracy: 0.5337INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 1.8335 - categorical_accuracy: 0.5345 - val_loss: 1.9828 - val_categorical_accuracy: 0.4670\n",
      "Epoch 63/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.8352 - categorical_accuracy: 0.5385 - val_loss: 1.9890 - val_categorical_accuracy: 0.4660\n",
      "Epoch 64/100\n",
      "492/500 [============================>.] - ETA: 0s - loss: 1.8333 - categorical_accuracy: 0.5368INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 1.8336 - categorical_accuracy: 0.5365 - val_loss: 1.9834 - val_categorical_accuracy: 0.4770\n",
      "Epoch 65/100\n",
      "499/500 [============================>.] - ETA: 0s - loss: 1.8170 - categorical_accuracy: 0.5433INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 1.8162 - categorical_accuracy: 0.5440 - val_loss: 1.9597 - val_categorical_accuracy: 0.4840\n",
      "Epoch 66/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.8123 - categorical_accuracy: 0.5487 - val_loss: 2.0008 - val_categorical_accuracy: 0.4700\n",
      "Epoch 67/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.8097 - categorical_accuracy: 0.5477 - val_loss: 2.0154 - val_categorical_accuracy: 0.4480\n",
      "Epoch 68/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.7971 - categorical_accuracy: 0.5515 - val_loss: 1.9714 - val_categorical_accuracy: 0.4740\n",
      "Epoch 69/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.7853 - categorical_accuracy: 0.5660 - val_loss: 1.9674 - val_categorical_accuracy: 0.4730\n",
      "Epoch 70/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.7875 - categorical_accuracy: 0.5620 - val_loss: 2.0018 - val_categorical_accuracy: 0.4650\n",
      "Epoch 71/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.7710 - categorical_accuracy: 0.5690 - val_loss: 1.9898 - val_categorical_accuracy: 0.4800\n",
      "Epoch 72/100\n",
      "494/500 [============================>.] - ETA: 0s - loss: 1.7739 - categorical_accuracy: 0.5744INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 1.7738 - categorical_accuracy: 0.5745 - val_loss: 1.9736 - val_categorical_accuracy: 0.4860\n",
      "Epoch 73/100\n",
      "491/500 [============================>.] - ETA: 0s - loss: 1.7643 - categorical_accuracy: 0.5782INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 1.7642 - categorical_accuracy: 0.5775 - val_loss: 1.9471 - val_categorical_accuracy: 0.4960\n",
      "Epoch 74/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.7610 - categorical_accuracy: 0.5828 - val_loss: 1.9566 - val_categorical_accuracy: 0.4900\n",
      "Epoch 75/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.7545 - categorical_accuracy: 0.5850 - val_loss: 1.9730 - val_categorical_accuracy: 0.4930\n",
      "Epoch 76/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.7455 - categorical_accuracy: 0.5895 - val_loss: 1.9685 - val_categorical_accuracy: 0.4940\n",
      "Epoch 77/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.7349 - categorical_accuracy: 0.5938INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 1.7349 - categorical_accuracy: 0.5938 - val_loss: 1.9566 - val_categorical_accuracy: 0.4980\n",
      "Epoch 78/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.7346 - categorical_accuracy: 0.6010 - val_loss: 1.9772 - val_categorical_accuracy: 0.4900\n",
      "Epoch 79/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.7297 - categorical_accuracy: 0.6012 - val_loss: 2.0076 - val_categorical_accuracy: 0.4850\n",
      "Epoch 80/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.7294 - categorical_accuracy: 0.6035 - val_loss: 1.9587 - val_categorical_accuracy: 0.4950\n",
      "Epoch 81/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.7206 - categorical_accuracy: 0.6003 - val_loss: 1.9667 - val_categorical_accuracy: 0.4930\n",
      "Epoch 82/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.7164 - categorical_accuracy: 0.6093INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 1.7164 - categorical_accuracy: 0.6093 - val_loss: 1.9402 - val_categorical_accuracy: 0.5050\n",
      "Epoch 83/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.7155 - categorical_accuracy: 0.6120 - val_loss: 1.9724 - val_categorical_accuracy: 0.4890\n",
      "Epoch 84/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.7063 - categorical_accuracy: 0.6160 - val_loss: 1.9633 - val_categorical_accuracy: 0.4930\n",
      "Epoch 85/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.7045 - categorical_accuracy: 0.6080 - val_loss: 1.9726 - val_categorical_accuracy: 0.4890\n",
      "Epoch 86/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.6985 - categorical_accuracy: 0.6175 - val_loss: 1.9674 - val_categorical_accuracy: 0.4920\n",
      "Epoch 87/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.6974 - categorical_accuracy: 0.6173 - val_loss: 1.9541 - val_categorical_accuracy: 0.5030\n",
      "Epoch 88/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.6922 - categorical_accuracy: 0.6168 - val_loss: 1.9422 - val_categorical_accuracy: 0.4900\n",
      "Epoch 89/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.6891 - categorical_accuracy: 0.6177 - val_loss: 1.9528 - val_categorical_accuracy: 0.5030\n",
      "Epoch 90/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.6817 - categorical_accuracy: 0.6233 - val_loss: 1.9250 - val_categorical_accuracy: 0.5030\n",
      "Epoch 91/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.6753 - categorical_accuracy: 0.6290 - val_loss: 1.9459 - val_categorical_accuracy: 0.4980\n",
      "Epoch 92/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.6737 - categorical_accuracy: 0.6290 - val_loss: 1.9785 - val_categorical_accuracy: 0.4870\n",
      "Epoch 93/100\n",
      "497/500 [============================>.] - ETA: 0s - loss: 1.6767 - categorical_accuracy: 0.6215INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 1.6748 - categorical_accuracy: 0.6230 - val_loss: 1.9523 - val_categorical_accuracy: 0.5070\n",
      "Epoch 94/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.6754 - categorical_accuracy: 0.6200 - val_loss: 1.9660 - val_categorical_accuracy: 0.5010\n",
      "Epoch 95/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.6653 - categorical_accuracy: 0.6280 - val_loss: 1.9647 - val_categorical_accuracy: 0.5040\n",
      "Epoch 96/100\n",
      "499/500 [============================>.] - ETA: 0s - loss: 1.6635 - categorical_accuracy: 0.6335INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 1.6625 - categorical_accuracy: 0.6340 - val_loss: 1.9257 - val_categorical_accuracy: 0.5210\n",
      "Epoch 97/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.6668 - categorical_accuracy: 0.6308 - val_loss: 1.9498 - val_categorical_accuracy: 0.5150\n",
      "Epoch 98/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.6534 - categorical_accuracy: 0.6415 - val_loss: 1.9518 - val_categorical_accuracy: 0.5090\n",
      "Epoch 99/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.6566 - categorical_accuracy: 0.6370 - val_loss: 1.9675 - val_categorical_accuracy: 0.4930\n",
      "Epoch 100/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.7232 - categorical_accuracy: 0.6025 - val_loss: 1.9353 - val_categorical_accuracy: 0.4980\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b099dbe65c1045bc86545b8fedf8870d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁▂▃▃▃▄▄▄▄▅▅▅▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇██████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁███▇▇▇▆▆▅▅▄▄▃▃▂▂▁▁▁█</td></tr><tr><td>epoch/loss</td><td>█▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁▂▃▄▄▅▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇██▇████▇███</td></tr><tr><td>epoch/val_loss</td><td>█▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.6025</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>1.72324</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.498</td></tr><tr><td>epoch/val_loss</td><td>1.93533</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM64-Dense128-Dense256-LipsEyesHandsPose_5k</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/iu5byyhv' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/iu5byyhv</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240416_142143-iu5byyhv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f954c786a90>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(64, return_sequences=False, input_shape=(X_shape[0], X_shape[1]),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"LSTM1-3.tf\"),\n",
    "            run_name=\"LSTM64-Dense128-Dense256-LipsEyesHandsPose_5k\",\n",
    "            mediapipe_features=\"reduced-LipsEyesHandsPose\",\n",
    "            USE_WANDB=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(256, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, 1629),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"LSTM256-Dense128-Dense256-LipsEyesHandsPose_5k.tf\"),\n",
    "            run_name=\"LSTM256-Dense128-Dense256-LipsEyesHandsPose_5k\",\n",
    "            mediapipe_features=\"reduced-LipsEyesHandsPose\",\n",
    "            USE_WANDB=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(128, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, 1629),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"LSTM128-Dense256-allfeatures.tf\"),\n",
    "            run_name=\"LSTM128-Dense256-allfeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(128, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, 1629),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"LSTM128-Dense128-allfeatures.tf\"),\n",
    "            run_name=\"LSTM128-Dense128-allfeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(128, return_sequences=False, activation='relu', input_shape=(CONFIG.VIDEO_LENGTH, 1629),\n",
    "               kernel_regularizer=l2(0.0000001), \n",
    "               activity_regularizer=l2(0.0000001)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"LSTM128_l2-Dense128-Dense256-allfeatures.tf\"),\n",
    "            run_name=\"LSTM128_l2-Dense128-Dense256-allfeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:g1jztv1q) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁</td></tr><tr><td>epoch/epoch</td><td>▁</td></tr><tr><td>epoch/learning_rate</td><td>▁</td></tr><tr><td>epoch/loss</td><td>▁</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁</td></tr><tr><td>epoch/val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.36889</td></tr><tr><td>epoch/epoch</td><td>0</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>2.82214</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.45</td></tr><tr><td>epoch/val_loss</td><td>2.39018</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM64-Dense128-Dense256-allfeatures_bigger_reg</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/g1jztv1q' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/g1jztv1q</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240408_085721-g1jztv1q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:g1jztv1q). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae32d08edf64e04a8009a32b4a48b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113070862160788, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240408_085904-b2cvthn0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/b2cvthn0' target=\"_blank\">LSTM64-Dense128-Dense256-allfeatures_bigger_reg</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/b2cvthn0' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/b2cvthn0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 2.8411 - categorical_accuracy: 0.4000INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 60s 499ms/step - loss: 2.8411 - categorical_accuracy: 0.4000 - val_loss: 2.4088 - val_categorical_accuracy: 0.4200\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 2.1274 - categorical_accuracy: 0.4056INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 502ms/step - loss: 2.1274 - categorical_accuracy: 0.4056 - val_loss: 1.8624 - val_categorical_accuracy: 0.4300\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - 49s 439ms/step - loss: 1.7043 - categorical_accuracy: 0.4044 - val_loss: 1.5451 - val_categorical_accuracy: 0.4300\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 1.4585 - categorical_accuracy: 0.4056 - val_loss: 1.3563 - val_categorical_accuracy: 0.4400\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - 50s 441ms/step - loss: 1.3106 - categorical_accuracy: 0.4056 - val_loss: 1.2412 - val_categorical_accuracy: 0.4300\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.2187 - categorical_accuracy: 0.4167INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 503ms/step - loss: 1.2187 - categorical_accuracy: 0.4167 - val_loss: 1.1653 - val_categorical_accuracy: 0.4200\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.1668 - categorical_accuracy: 0.4211INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 505ms/step - loss: 1.1668 - categorical_accuracy: 0.4211 - val_loss: 1.0656 - val_categorical_accuracy: 0.5000\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - 50s 442ms/step - loss: 1.1402 - categorical_accuracy: 0.4167 - val_loss: 1.0608 - val_categorical_accuracy: 0.5100\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0638 - categorical_accuracy: 0.4833INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 505ms/step - loss: 1.0638 - categorical_accuracy: 0.4833 - val_loss: 0.9921 - val_categorical_accuracy: 0.5600\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0240 - categorical_accuracy: 0.4889INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 500ms/step - loss: 1.0240 - categorical_accuracy: 0.4889 - val_loss: 1.0163 - val_categorical_accuracy: 0.5200\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0002 - categorical_accuracy: 0.5100INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 512ms/step - loss: 1.0002 - categorical_accuracy: 0.5100 - val_loss: 0.9776 - val_categorical_accuracy: 0.5200\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.9623 - categorical_accuracy: 0.5144INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 512ms/step - loss: 0.9623 - categorical_accuracy: 0.5144 - val_loss: 0.9214 - val_categorical_accuracy: 0.5500\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.9209 - categorical_accuracy: 0.5544INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 505ms/step - loss: 0.9209 - categorical_accuracy: 0.5544 - val_loss: 0.8942 - val_categorical_accuracy: 0.5600\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.9315 - categorical_accuracy: 0.5333 - val_loss: 0.9100 - val_categorical_accuracy: 0.5400\n",
      "Epoch 15/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8799 - categorical_accuracy: 0.5733INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 512ms/step - loss: 0.8799 - categorical_accuracy: 0.5733 - val_loss: 0.8732 - val_categorical_accuracy: 0.5500\n",
      "Epoch 16/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.8731 - categorical_accuracy: 0.5589 - val_loss: 0.8565 - val_categorical_accuracy: 0.5900\n",
      "Epoch 17/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.8757 - categorical_accuracy: 0.5556 - val_loss: 0.8595 - val_categorical_accuracy: 0.5800\n",
      "Epoch 18/100\n",
      "113/113 [==============================] - 50s 444ms/step - loss: 0.8616 - categorical_accuracy: 0.5733 - val_loss: 0.8362 - val_categorical_accuracy: 0.6100\n",
      "Epoch 19/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8751 - categorical_accuracy: 0.5800INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 505ms/step - loss: 0.8751 - categorical_accuracy: 0.5800 - val_loss: 0.8408 - val_categorical_accuracy: 0.5700\n",
      "Epoch 20/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8239 - categorical_accuracy: 0.6022INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 56s 502ms/step - loss: 0.8239 - categorical_accuracy: 0.6022 - val_loss: 0.8111 - val_categorical_accuracy: 0.6000\n",
      "Epoch 21/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8207 - categorical_accuracy: 0.6067INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 511ms/step - loss: 0.8207 - categorical_accuracy: 0.6067 - val_loss: 0.8012 - val_categorical_accuracy: 0.6100\n",
      "Epoch 22/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7945 - categorical_accuracy: 0.6511INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 510ms/step - loss: 0.7945 - categorical_accuracy: 0.6511 - val_loss: 0.7536 - val_categorical_accuracy: 0.6500\n",
      "Epoch 23/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.7951 - categorical_accuracy: 0.6400 - val_loss: 0.9351 - val_categorical_accuracy: 0.6400\n",
      "Epoch 24/100\n",
      "113/113 [==============================] - 50s 449ms/step - loss: 0.8354 - categorical_accuracy: 0.6267 - val_loss: 1.3834 - val_categorical_accuracy: 0.3300\n",
      "Epoch 25/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7755 - categorical_accuracy: 0.6633INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 56s 499ms/step - loss: 0.7755 - categorical_accuracy: 0.6633 - val_loss: 0.7559 - val_categorical_accuracy: 0.6800\n",
      "Epoch 26/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7431 - categorical_accuracy: 0.7156INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 515ms/step - loss: 0.7431 - categorical_accuracy: 0.7156 - val_loss: 0.8453 - val_categorical_accuracy: 0.5700\n",
      "Epoch 27/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7037 - categorical_accuracy: 0.7278INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 56s 502ms/step - loss: 0.7037 - categorical_accuracy: 0.7278 - val_loss: 0.7106 - val_categorical_accuracy: 0.7100\n",
      "Epoch 28/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6386 - categorical_accuracy: 0.7778INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 502ms/step - loss: 0.6386 - categorical_accuracy: 0.7778 - val_loss: 0.7482 - val_categorical_accuracy: 0.6600\n",
      "Epoch 29/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.6493 - categorical_accuracy: 0.7622 - val_loss: 0.5469 - val_categorical_accuracy: 0.8400\n",
      "Epoch 30/100\n",
      "113/113 [==============================] - 50s 441ms/step - loss: 0.6398 - categorical_accuracy: 0.7711 - val_loss: 0.5749 - val_categorical_accuracy: 0.7700\n",
      "Epoch 31/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6062 - categorical_accuracy: 0.7833INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 508ms/step - loss: 0.6062 - categorical_accuracy: 0.7833 - val_loss: 0.7759 - val_categorical_accuracy: 0.6900\n",
      "Epoch 32/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5734 - categorical_accuracy: 0.8022INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 510ms/step - loss: 0.5734 - categorical_accuracy: 0.8022 - val_loss: 0.6057 - val_categorical_accuracy: 0.7800\n",
      "Epoch 33/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5675 - categorical_accuracy: 0.8044INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 506ms/step - loss: 0.5675 - categorical_accuracy: 0.8044 - val_loss: 0.7343 - val_categorical_accuracy: 0.7800\n",
      "Epoch 34/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.5910 - categorical_accuracy: 0.7911 - val_loss: 0.5328 - val_categorical_accuracy: 0.8200\n",
      "Epoch 35/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.6100 - categorical_accuracy: 0.7944 - val_loss: 0.9632 - val_categorical_accuracy: 0.7100\n",
      "Epoch 36/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.6669 - categorical_accuracy: 0.7811 - val_loss: 0.5713 - val_categorical_accuracy: 0.8100\n",
      "Epoch 37/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.5737 - categorical_accuracy: 0.8033 - val_loss: 0.8288 - val_categorical_accuracy: 0.6900\n",
      "Epoch 38/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5330 - categorical_accuracy: 0.8344INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 510ms/step - loss: 0.5330 - categorical_accuracy: 0.8344 - val_loss: 0.8726 - val_categorical_accuracy: 0.6300\n",
      "Epoch 39/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.5579 - categorical_accuracy: 0.8111 - val_loss: 0.7435 - val_categorical_accuracy: 0.7500\n",
      "Epoch 40/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.5458 - categorical_accuracy: 0.8233 - val_loss: 0.8340 - val_categorical_accuracy: 0.7000\n",
      "Epoch 41/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.6143 - categorical_accuracy: 0.7856 - val_loss: 0.7396 - val_categorical_accuracy: 0.7700\n",
      "Epoch 42/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.5929 - categorical_accuracy: 0.7967 - val_loss: 0.7475 - val_categorical_accuracy: 0.6900\n",
      "Epoch 43/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.5422 - categorical_accuracy: 0.8333 - val_loss: 0.6000 - val_categorical_accuracy: 0.8200\n",
      "Epoch 44/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4798 - categorical_accuracy: 0.8567INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 507ms/step - loss: 0.4798 - categorical_accuracy: 0.8567 - val_loss: 0.6378 - val_categorical_accuracy: 0.7800\n",
      "Epoch 45/100\n",
      "113/113 [==============================] - 49s 438ms/step - loss: 0.5517 - categorical_accuracy: 0.8100 - val_loss: 0.6610 - val_categorical_accuracy: 0.8000\n",
      "Epoch 46/100\n",
      "113/113 [==============================] - 50s 444ms/step - loss: 0.5207 - categorical_accuracy: 0.8422 - val_loss: 0.5422 - val_categorical_accuracy: 0.8600\n",
      "Epoch 47/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.5088 - categorical_accuracy: 0.8489 - val_loss: 0.5806 - val_categorical_accuracy: 0.7900\n",
      "Epoch 48/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.5605 - categorical_accuracy: 0.8167 - val_loss: 0.4904 - val_categorical_accuracy: 0.8400\n",
      "Epoch 49/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.5473 - categorical_accuracy: 0.8278 - val_loss: 0.6565 - val_categorical_accuracy: 0.7600\n",
      "Epoch 50/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.5422 - categorical_accuracy: 0.8378 - val_loss: 0.6482 - val_categorical_accuracy: 0.8100\n",
      "Epoch 51/100\n",
      "113/113 [==============================] - 50s 444ms/step - loss: 0.4920 - categorical_accuracy: 0.8533 - val_loss: 0.9195 - val_categorical_accuracy: 0.7200\n",
      "Epoch 52/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.5176 - categorical_accuracy: 0.8456 - val_loss: 0.4050 - val_categorical_accuracy: 0.8900\n",
      "Epoch 53/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.5224 - categorical_accuracy: 0.8367 - val_loss: 0.6238 - val_categorical_accuracy: 0.8300\n",
      "Epoch 54/100\n",
      "113/113 [==============================] - 51s 446ms/step - loss: 0.4998 - categorical_accuracy: 0.8511 - val_loss: 0.7593 - val_categorical_accuracy: 0.6900\n",
      "Epoch 55/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4259 - categorical_accuracy: 0.8800INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 505ms/step - loss: 0.4259 - categorical_accuracy: 0.8800 - val_loss: 0.6574 - val_categorical_accuracy: 0.7800\n",
      "Epoch 56/100\n",
      "113/113 [==============================] - 51s 446ms/step - loss: 0.5203 - categorical_accuracy: 0.8444 - val_loss: 0.5315 - val_categorical_accuracy: 0.8300\n",
      "Epoch 57/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.5094 - categorical_accuracy: 0.8478 - val_loss: 0.6249 - val_categorical_accuracy: 0.8100\n",
      "Epoch 58/100\n",
      "113/113 [==============================] - 51s 447ms/step - loss: 0.5697 - categorical_accuracy: 0.8200 - val_loss: 0.4367 - val_categorical_accuracy: 0.9000\n",
      "Epoch 59/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.4575 - categorical_accuracy: 0.8767 - val_loss: 0.6292 - val_categorical_accuracy: 0.8000\n",
      "Epoch 65/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.4987 - categorical_accuracy: 0.8511 - val_loss: 0.5974 - val_categorical_accuracy: 0.8300\n",
      "Epoch 66/100\n",
      "113/113 [==============================] - 50s 448ms/step - loss: 0.5852 - categorical_accuracy: 0.8289 - val_loss: 0.6959 - val_categorical_accuracy: 0.7500\n",
      "Epoch 67/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.5027 - categorical_accuracy: 0.8533 - val_loss: 0.5093 - val_categorical_accuracy: 0.8500\n",
      "Epoch 68/100\n",
      "113/113 [==============================] - 50s 441ms/step - loss: 0.4957 - categorical_accuracy: 0.8622 - val_loss: 0.4846 - val_categorical_accuracy: 0.8500\n",
      "Epoch 69/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.5055 - categorical_accuracy: 0.8533 - val_loss: 0.5980 - val_categorical_accuracy: 0.8600\n",
      "Epoch 70/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.5464 - categorical_accuracy: 0.8367 - val_loss: 0.5365 - val_categorical_accuracy: 0.8400\n",
      "Epoch 71/100\n",
      "113/113 [==============================] - 50s 448ms/step - loss: 0.4602 - categorical_accuracy: 0.8689 - val_loss: 0.4441 - val_categorical_accuracy: 0.8800\n",
      "Epoch 72/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.5463 - categorical_accuracy: 0.8189 - val_loss: 0.5223 - val_categorical_accuracy: 0.8000\n",
      "Epoch 73/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4312 - categorical_accuracy: 0.8878INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 513ms/step - loss: 0.4312 - categorical_accuracy: 0.8878 - val_loss: 0.4676 - val_categorical_accuracy: 0.8600\n",
      "Epoch 74/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.4691 - categorical_accuracy: 0.8589 - val_loss: 0.4604 - val_categorical_accuracy: 0.8700\n",
      "Epoch 75/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.4864 - categorical_accuracy: 0.8567 - val_loss: 0.4465 - val_categorical_accuracy: 0.9000\n",
      "Epoch 76/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.4778 - categorical_accuracy: 0.8678 - val_loss: 0.4316 - val_categorical_accuracy: 0.8800\n",
      "Epoch 77/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.4969 - categorical_accuracy: 0.8444 - val_loss: 0.4481 - val_categorical_accuracy: 0.8800\n",
      "Epoch 78/100\n",
      "113/113 [==============================] - 50s 448ms/step - loss: 0.4584 - categorical_accuracy: 0.8644 - val_loss: 0.4431 - val_categorical_accuracy: 0.9100\n",
      "Epoch 79/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4070 - categorical_accuracy: 0.8900INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 505ms/step - loss: 0.4070 - categorical_accuracy: 0.8900 - val_loss: 0.4321 - val_categorical_accuracy: 0.8800\n",
      "Epoch 80/100\n",
      "113/113 [==============================] - 50s 442ms/step - loss: 0.4880 - categorical_accuracy: 0.8633 - val_loss: 0.4699 - val_categorical_accuracy: 0.8800\n",
      "Epoch 81/100\n",
      "113/113 [==============================] - 50s 439ms/step - loss: 0.4324 - categorical_accuracy: 0.8822 - val_loss: 0.4674 - val_categorical_accuracy: 0.8600\n",
      "Epoch 82/100\n",
      "113/113 [==============================] - 50s 439ms/step - loss: 0.4607 - categorical_accuracy: 0.8722 - val_loss: 0.4195 - val_categorical_accuracy: 0.9000\n",
      "Epoch 83/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.5813 - categorical_accuracy: 0.8144 - val_loss: 0.7107 - val_categorical_accuracy: 0.8300\n",
      "Epoch 84/100\n",
      "113/113 [==============================] - 50s 440ms/step - loss: 0.5578 - categorical_accuracy: 0.8311 - val_loss: 0.4452 - val_categorical_accuracy: 0.8800\n",
      "Epoch 85/100\n",
      "113/113 [==============================] - 50s 439ms/step - loss: 0.4656 - categorical_accuracy: 0.8722 - val_loss: 0.4972 - val_categorical_accuracy: 0.8600\n",
      "Epoch 86/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.4155 - categorical_accuracy: 0.8800 - val_loss: 0.7409 - val_categorical_accuracy: 0.8000\n",
      "Epoch 87/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.5258 - categorical_accuracy: 0.8367 - val_loss: 0.4980 - val_categorical_accuracy: 0.8300\n",
      "Epoch 88/100\n",
      "113/113 [==============================] - 50s 442ms/step - loss: 0.4853 - categorical_accuracy: 0.8689 - val_loss: 0.5915 - val_categorical_accuracy: 0.8200\n",
      "Epoch 89/100\n",
      "113/113 [==============================] - 50s 439ms/step - loss: 0.4376 - categorical_accuracy: 0.8811 - val_loss: 0.4558 - val_categorical_accuracy: 0.8700\n",
      "Epoch 90/100\n",
      "113/113 [==============================] - 50s 441ms/step - loss: 0.4365 - categorical_accuracy: 0.8822 - val_loss: 0.5814 - val_categorical_accuracy: 0.8300\n",
      "Epoch 91/100\n",
      "113/113 [==============================] - 50s 440ms/step - loss: 0.5081 - categorical_accuracy: 0.8378 - val_loss: 0.7659 - val_categorical_accuracy: 0.7900\n",
      "Epoch 92/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.4477 - categorical_accuracy: 0.8767 - val_loss: 0.4377 - val_categorical_accuracy: 0.8800\n",
      "Epoch 93/100\n",
      "113/113 [==============================] - 50s 438ms/step - loss: 0.4153 - categorical_accuracy: 0.8900 - val_loss: 0.3688 - val_categorical_accuracy: 0.9100\n",
      "Epoch 94/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4063 - categorical_accuracy: 0.8922INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 56s 498ms/step - loss: 0.4063 - categorical_accuracy: 0.8922 - val_loss: 0.4536 - val_categorical_accuracy: 0.8800\n",
      "Epoch 95/100\n",
      "113/113 [==============================] - 51s 444ms/step - loss: 0.4751 - categorical_accuracy: 0.8656 - val_loss: 0.5127 - val_categorical_accuracy: 0.8400\n",
      "Epoch 96/100\n",
      "113/113 [==============================] - 50s 442ms/step - loss: 0.4397 - categorical_accuracy: 0.8767 - val_loss: 0.4737 - val_categorical_accuracy: 0.8700\n",
      "Epoch 97/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.4376 - categorical_accuracy: 0.8722 - val_loss: 0.4013 - val_categorical_accuracy: 0.8800\n",
      "Epoch 98/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.4417 - categorical_accuracy: 0.8722 - val_loss: 0.4582 - val_categorical_accuracy: 0.8600\n",
      "Epoch 99/100\n",
      "113/113 [==============================] - 50s 441ms/step - loss: 0.4192 - categorical_accuracy: 0.8867 - val_loss: 0.4077 - val_categorical_accuracy: 0.9000\n",
      "Epoch 100/100\n",
      "113/113 [==============================] - 50s 444ms/step - loss: 0.5048 - categorical_accuracy: 0.8467 - val_loss: 0.6696 - val_categorical_accuracy: 0.8000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁▁▁▁▃▃▃▃▄▄▅▆▆▇▆▇▆▇▇▇▇▇▇█▇█▇▇▇█▇██▇▇████▇</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▅▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁▁▁▂▂▃▃▄▄▄▃▅▅▇▇▆▆▆▇▆▅▅▇▇▇▇▇▇▇█████▇████▇</td></tr><tr><td>epoch/val_loss</td><td>█▅▄▃▃▃▃▃▂▃▃▂▂▁▂▂▂▂▁▂▃▂▁▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.84667</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>0.50475</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.8</td></tr><tr><td>epoch/val_loss</td><td>0.66957</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM64-Dense128-Dense256-allfeatures_bigger_reg</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/b2cvthn0' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/b2cvthn0</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240408_085904-b2cvthn0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fda6bc02690>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(64, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, 1629),\n",
    "               kernel_regularizer=l2(0.005), \n",
    "               activity_regularizer=l2(0.005)))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"LSTM-L64-D128-D256-reg=0.005.tf\"),\n",
    "            run_name=\"LSTM64-Dense128-Dense256-allfeatures_bigger_reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ehqx0ymo) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.010 MB uploaded\\r'), FloatProgress(value=0.5380860274477296, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM128-Dense128-Dense256-allfeatures</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/ehqx0ymo' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/ehqx0ymo</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240408_102806-ehqx0ymo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ehqx0ymo). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02c443cf9c24d67afce99e0dd73feaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111298907134268, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240408_102839-tfbow8kv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/tfbow8kv' target=\"_blank\">LSTM128-Dense128-Dense256-allfeatures</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/tfbow8kv' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/tfbow8kv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.1593 - categorical_accuracy: 0.4111INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 59s 497ms/step - loss: 1.1593 - categorical_accuracy: 0.4111 - val_loss: 1.1264 - val_categorical_accuracy: 0.4300\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - 50s 440ms/step - loss: 1.1159 - categorical_accuracy: 0.4122 - val_loss: 1.0794 - val_categorical_accuracy: 0.4300\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - 50s 444ms/step - loss: 1.0872 - categorical_accuracy: 0.4156 - val_loss: 1.0639 - val_categorical_accuracy: 0.4300\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0789 - categorical_accuracy: 0.4222INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 504ms/step - loss: 1.0789 - categorical_accuracy: 0.4222 - val_loss: 1.0364 - val_categorical_accuracy: 0.4500\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0382 - categorical_accuracy: 0.4522INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 504ms/step - loss: 1.0382 - categorical_accuracy: 0.4522 - val_loss: 0.9582 - val_categorical_accuracy: 0.4800\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.9295 - categorical_accuracy: 0.5167INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 56s 500ms/step - loss: 0.9295 - categorical_accuracy: 0.5167 - val_loss: 0.8747 - val_categorical_accuracy: 0.5400\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.8825 - categorical_accuracy: 0.5411 - val_loss: 0.8312 - val_categorical_accuracy: 0.5300\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8269 - categorical_accuracy: 0.5622INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 509ms/step - loss: 0.8269 - categorical_accuracy: 0.5622 - val_loss: 0.8254 - val_categorical_accuracy: 0.5500\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8556 - categorical_accuracy: 0.5433INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 507ms/step - loss: 0.8556 - categorical_accuracy: 0.5433 - val_loss: 0.8159 - val_categorical_accuracy: 0.6200\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.8348 - categorical_accuracy: 0.5756 - val_loss: 0.7958 - val_categorical_accuracy: 0.5500\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - 50s 442ms/step - loss: 0.8885 - categorical_accuracy: 0.5233 - val_loss: 0.9448 - val_categorical_accuracy: 0.5100\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.8151 - categorical_accuracy: 0.5722 - val_loss: 0.8031 - val_categorical_accuracy: 0.5500\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.7978 - categorical_accuracy: 0.5844 - val_loss: 0.8093 - val_categorical_accuracy: 0.5400\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - 50s 444ms/step - loss: 0.7737 - categorical_accuracy: 0.5944 - val_loss: 0.7904 - val_categorical_accuracy: 0.5900\n",
      "Epoch 15/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.7715 - categorical_accuracy: 0.6011 - val_loss: 0.7445 - val_categorical_accuracy: 0.6100\n",
      "Epoch 16/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.7729 - categorical_accuracy: 0.6289 - val_loss: 0.7602 - val_categorical_accuracy: 0.6000\n",
      "Epoch 17/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7333 - categorical_accuracy: 0.6744INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 506ms/step - loss: 0.7333 - categorical_accuracy: 0.6744 - val_loss: 0.7445 - val_categorical_accuracy: 0.6900\n",
      "Epoch 18/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6788 - categorical_accuracy: 0.7311INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 503ms/step - loss: 0.6788 - categorical_accuracy: 0.7311 - val_loss: 0.6787 - val_categorical_accuracy: 0.7300\n",
      "Epoch 19/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6468 - categorical_accuracy: 0.7200INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 508ms/step - loss: 0.6468 - categorical_accuracy: 0.7200 - val_loss: 0.5513 - val_categorical_accuracy: 0.8400\n",
      "Epoch 20/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.6418 - categorical_accuracy: 0.7500 - val_loss: 0.5769 - val_categorical_accuracy: 0.7900\n",
      "Epoch 21/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.5473 - categorical_accuracy: 0.8067 - val_loss: 0.5113 - val_categorical_accuracy: 0.8200\n",
      "Epoch 22/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.5497 - categorical_accuracy: 0.8022 - val_loss: 0.4657 - val_categorical_accuracy: 0.8200\n",
      "Epoch 23/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5541 - categorical_accuracy: 0.7900INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 509ms/step - loss: 0.5541 - categorical_accuracy: 0.7900 - val_loss: 0.5049 - val_categorical_accuracy: 0.8500\n",
      "Epoch 24/100\n",
      "113/113 [==============================] - 50s 444ms/step - loss: 0.5154 - categorical_accuracy: 0.7989 - val_loss: 0.5346 - val_categorical_accuracy: 0.7900\n",
      "Epoch 25/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.5237 - categorical_accuracy: 0.8133 - val_loss: 0.4664 - val_categorical_accuracy: 0.8300\n",
      "Epoch 26/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.5831 - categorical_accuracy: 0.8033 - val_loss: 0.5966 - val_categorical_accuracy: 0.8000\n",
      "Epoch 27/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.5225 - categorical_accuracy: 0.8033 - val_loss: 0.7948 - val_categorical_accuracy: 0.7300\n",
      "Epoch 28/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.5959 - categorical_accuracy: 0.7822 - val_loss: 0.6207 - val_categorical_accuracy: 0.7900\n",
      "Epoch 29/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4713 - categorical_accuracy: 0.8367INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 512ms/step - loss: 0.4713 - categorical_accuracy: 0.8367 - val_loss: 0.4706 - val_categorical_accuracy: 0.8600\n",
      "Epoch 30/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.4891 - categorical_accuracy: 0.8333 - val_loss: 0.4781 - val_categorical_accuracy: 0.8400\n",
      "Epoch 31/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.5176 - categorical_accuracy: 0.8200 - val_loss: 0.5093 - val_categorical_accuracy: 0.8400\n",
      "Epoch 32/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.4791 - categorical_accuracy: 0.8400 - val_loss: 0.4815 - val_categorical_accuracy: 0.8500\n",
      "Epoch 33/100\n",
      "113/113 [==============================] - 50s 448ms/step - loss: 0.4326 - categorical_accuracy: 0.8689 - val_loss: 0.5219 - val_categorical_accuracy: 0.8000\n",
      "Epoch 34/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4221 - categorical_accuracy: 0.8611INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 509ms/step - loss: 0.4221 - categorical_accuracy: 0.8611 - val_loss: 0.4338 - val_categorical_accuracy: 0.8700\n",
      "Epoch 35/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.4176 - categorical_accuracy: 0.8600 - val_loss: 0.5111 - val_categorical_accuracy: 0.8100\n",
      "Epoch 36/100\n",
      "113/113 [==============================] - 50s 441ms/step - loss: 0.3978 - categorical_accuracy: 0.8722 - val_loss: 0.4874 - val_categorical_accuracy: 0.8100\n",
      "Epoch 37/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4454 - categorical_accuracy: 0.8511INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 504ms/step - loss: 0.4454 - categorical_accuracy: 0.8511 - val_loss: 0.3816 - val_categorical_accuracy: 0.8900\n",
      "Epoch 38/100\n",
      "113/113 [==============================] - 49s 437ms/step - loss: 0.3786 - categorical_accuracy: 0.8889 - val_loss: 0.4801 - val_categorical_accuracy: 0.8500\n",
      "Epoch 39/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.3902 - categorical_accuracy: 0.8922 - val_loss: 0.5937 - val_categorical_accuracy: 0.8000\n",
      "Epoch 40/100\n",
      "113/113 [==============================] - 50s 441ms/step - loss: 0.3852 - categorical_accuracy: 0.8889 - val_loss: 0.4676 - val_categorical_accuracy: 0.8500\n",
      "Epoch 41/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.3807 - categorical_accuracy: 0.8856 - val_loss: 0.4871 - val_categorical_accuracy: 0.8300\n",
      "Epoch 42/100\n",
      "113/113 [==============================] - 50s 444ms/step - loss: 0.3750 - categorical_accuracy: 0.8822 - val_loss: 0.4893 - val_categorical_accuracy: 0.8600\n",
      "Epoch 43/100\n",
      "113/113 [==============================] - 50s 441ms/step - loss: 0.4044 - categorical_accuracy: 0.8911 - val_loss: 0.6310 - val_categorical_accuracy: 0.7900\n",
      "Epoch 44/100\n",
      "113/113 [==============================] - 50s 442ms/step - loss: 0.3771 - categorical_accuracy: 0.8867 - val_loss: 0.4822 - val_categorical_accuracy: 0.8500\n",
      "Epoch 45/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.3903 - categorical_accuracy: 0.8844INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 56s 499ms/step - loss: 0.3903 - categorical_accuracy: 0.8844 - val_loss: 0.3689 - val_categorical_accuracy: 0.9000\n",
      "Epoch 46/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.3459 - categorical_accuracy: 0.9078 - val_loss: 0.3768 - val_categorical_accuracy: 0.9000\n",
      "Epoch 47/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3704 - categorical_accuracy: 0.8956 - val_loss: 0.4289 - val_categorical_accuracy: 0.8800\n",
      "Epoch 48/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.3126 - categorical_accuracy: 0.9122 - val_loss: 0.4029 - val_categorical_accuracy: 0.8800\n",
      "Epoch 49/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3530 - categorical_accuracy: 0.8922 - val_loss: 0.4759 - val_categorical_accuracy: 0.8700\n",
      "Epoch 50/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.4171 - categorical_accuracy: 0.8767 - val_loss: 0.6324 - val_categorical_accuracy: 0.7900\n",
      "Epoch 51/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.4903 - categorical_accuracy: 0.8400 - val_loss: 0.4580 - val_categorical_accuracy: 0.8600\n",
      "Epoch 52/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.5004 - categorical_accuracy: 0.8322 - val_loss: 0.7386 - val_categorical_accuracy: 0.7800\n",
      "Epoch 53/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.5495 - categorical_accuracy: 0.8144 - val_loss: 0.5707 - val_categorical_accuracy: 0.8200\n",
      "Epoch 54/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.4326 - categorical_accuracy: 0.8522 - val_loss: 0.4949 - val_categorical_accuracy: 0.8600\n",
      "Epoch 55/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.4168 - categorical_accuracy: 0.8711 - val_loss: 0.3718 - val_categorical_accuracy: 0.8700\n",
      "Epoch 56/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.4029 - categorical_accuracy: 0.8656 - val_loss: 0.3996 - val_categorical_accuracy: 0.8900\n",
      "Epoch 57/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.3701 - categorical_accuracy: 0.8833 - val_loss: 0.4646 - val_categorical_accuracy: 0.8700\n",
      "Epoch 58/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.3946 - categorical_accuracy: 0.8789 - val_loss: 0.7404 - val_categorical_accuracy: 0.7200\n",
      "Epoch 59/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.3892 - categorical_accuracy: 0.8789 - val_loss: 0.4167 - val_categorical_accuracy: 0.8900\n",
      "Epoch 60/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.3286 - categorical_accuracy: 0.8978 - val_loss: 0.4500 - val_categorical_accuracy: 0.8700\n",
      "Epoch 61/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.4057 - categorical_accuracy: 0.8778 - val_loss: 0.5199 - val_categorical_accuracy: 0.8400\n",
      "Epoch 62/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.4515 - categorical_accuracy: 0.8611 - val_loss: 0.4424 - val_categorical_accuracy: 0.8700\n",
      "Epoch 63/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.4480 - categorical_accuracy: 0.8656 - val_loss: 0.4351 - val_categorical_accuracy: 0.8900\n",
      "Epoch 64/100\n",
      "113/113 [==============================] - 51s 447ms/step - loss: 0.3490 - categorical_accuracy: 0.8967 - val_loss: 0.5264 - val_categorical_accuracy: 0.8400\n",
      "Epoch 65/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3577 - categorical_accuracy: 0.9011 - val_loss: 0.4346 - val_categorical_accuracy: 0.8300\n",
      "Epoch 66/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.3784 - categorical_accuracy: 0.8800 - val_loss: 0.4018 - val_categorical_accuracy: 0.8700\n",
      "Epoch 67/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.3576 - categorical_accuracy: 0.8956 - val_loss: 0.5138 - val_categorical_accuracy: 0.8200\n",
      "Epoch 68/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.3531 - categorical_accuracy: 0.8933 - val_loss: 0.4497 - val_categorical_accuracy: 0.8500\n",
      "Epoch 69/100\n",
      "113/113 [==============================] - 51s 446ms/step - loss: 0.3818 - categorical_accuracy: 0.8944 - val_loss: 0.5373 - val_categorical_accuracy: 0.8200\n",
      "Epoch 70/100\n",
      "113/113 [==============================] - 51s 447ms/step - loss: 0.3838 - categorical_accuracy: 0.8922 - val_loss: 0.5568 - val_categorical_accuracy: 0.8300\n",
      "Epoch 71/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.2919 - categorical_accuracy: 0.9233 - val_loss: 0.4551 - val_categorical_accuracy: 0.8700\n",
      "Epoch 72/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3211 - categorical_accuracy: 0.9078 - val_loss: 0.4649 - val_categorical_accuracy: 0.8700\n",
      "Epoch 73/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.3066 - categorical_accuracy: 0.9122 - val_loss: 0.4409 - val_categorical_accuracy: 0.8600\n",
      "Epoch 74/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.3514 - categorical_accuracy: 0.8922 - val_loss: 0.4000 - val_categorical_accuracy: 0.8900\n",
      "Epoch 75/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.3193 - categorical_accuracy: 0.9122 - val_loss: 0.3849 - val_categorical_accuracy: 0.8700\n",
      "Epoch 76/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.3559 - categorical_accuracy: 0.8922 - val_loss: 0.4093 - val_categorical_accuracy: 0.8900\n",
      "Epoch 77/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.2585 - categorical_accuracy: 0.9344 - val_loss: 0.3891 - val_categorical_accuracy: 0.8900\n",
      "Epoch 78/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.2845 - categorical_accuracy: 0.9200INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 506ms/step - loss: 0.2845 - categorical_accuracy: 0.9200 - val_loss: 0.3175 - val_categorical_accuracy: 0.9200\n",
      "Epoch 79/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.3147 - categorical_accuracy: 0.9156 - val_loss: 0.3852 - val_categorical_accuracy: 0.8900\n",
      "Epoch 80/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.2673 - categorical_accuracy: 0.9200 - val_loss: 0.3926 - val_categorical_accuracy: 0.9000\n",
      "Epoch 81/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.2707 - categorical_accuracy: 0.9256 - val_loss: 0.3431 - val_categorical_accuracy: 0.9000\n",
      "Epoch 82/100\n",
      "  6/113 [>.............................] - ETA: 38s - loss: 0.2516 - categorical_accuracy: 0.9375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(128, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, 1629),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"LSTM128-Dense128-Dense256-allfeatures.tf\"),\n",
    "            run_name=\"LSTM128-Dense128-Dense256-allfeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:vxwg2tiq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁</td></tr><tr><td>epoch/epoch</td><td>▁</td></tr><tr><td>epoch/learning_rate</td><td>▁</td></tr><tr><td>epoch/loss</td><td>▁</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁</td></tr><tr><td>epoch/val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.41667</td></tr><tr><td>epoch/epoch</td><td>0</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>1.13316</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.44</td></tr><tr><td>epoch/val_loss</td><td>1.11618</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM64-Dense64-allfeatures</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/vxwg2tiq' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/vxwg2tiq</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240408_124116-vxwg2tiq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:vxwg2tiq). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a843215891450f8950dbbea742785b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112615217765172, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240408_124402-tyh5w2ae</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/tyh5w2ae' target=\"_blank\">LSTM64-Dense64-allfeatures</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/tyh5w2ae' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/tyh5w2ae</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.1343 - categorical_accuracy: 0.4444INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 59s 497ms/step - loss: 1.1343 - categorical_accuracy: 0.4444 - val_loss: 1.1175 - val_categorical_accuracy: 0.4300\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 1.1170 - categorical_accuracy: 0.4256 - val_loss: 1.0923 - val_categorical_accuracy: 0.4300\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - 52s 454ms/step - loss: 1.0976 - categorical_accuracy: 0.4200 - val_loss: 1.0688 - val_categorical_accuracy: 0.4300\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 1.0772 - categorical_accuracy: 0.4267 - val_loss: 1.0476 - val_categorical_accuracy: 0.4300\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 1.0660 - categorical_accuracy: 0.4222 - val_loss: 1.0532 - val_categorical_accuracy: 0.4300\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 1.0527 - categorical_accuracy: 0.4122 - val_loss: 1.0162 - val_categorical_accuracy: 0.4300\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0104 - categorical_accuracy: 0.4322INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 518ms/step - loss: 1.0104 - categorical_accuracy: 0.4322 - val_loss: 0.9230 - val_categorical_accuracy: 0.5300\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.9788 - categorical_accuracy: 0.4911 - val_loss: 1.0028 - val_categorical_accuracy: 0.4700\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.9554 - categorical_accuracy: 0.5089INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 512ms/step - loss: 0.9554 - categorical_accuracy: 0.5089 - val_loss: 0.9205 - val_categorical_accuracy: 0.5400\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8856 - categorical_accuracy: 0.5389INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 514ms/step - loss: 0.8856 - categorical_accuracy: 0.5389 - val_loss: 0.8232 - val_categorical_accuracy: 0.5600\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.8843 - categorical_accuracy: 0.5400 - val_loss: 0.8389 - val_categorical_accuracy: 0.5600\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.8609 - categorical_accuracy: 0.5311 - val_loss: 0.8181 - val_categorical_accuracy: 0.5600\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.8422 - categorical_accuracy: 0.5622 - val_loss: 0.8075 - val_categorical_accuracy: 0.5500\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.8400 - categorical_accuracy: 0.5556 - val_loss: 0.8344 - val_categorical_accuracy: 0.5600\n",
      "Epoch 15/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.8375 - categorical_accuracy: 0.5533 - val_loss: 0.8198 - val_categorical_accuracy: 0.5400\n",
      "Epoch 16/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.8186 - categorical_accuracy: 0.5600 - val_loss: 0.8137 - val_categorical_accuracy: 0.5500\n",
      "Epoch 17/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.8218 - categorical_accuracy: 0.5478 - val_loss: 0.8052 - val_categorical_accuracy: 0.5300\n",
      "Epoch 18/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.8168 - categorical_accuracy: 0.5567 - val_loss: 0.8125 - val_categorical_accuracy: 0.5600\n",
      "Epoch 19/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7975 - categorical_accuracy: 0.5889INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 514ms/step - loss: 0.7975 - categorical_accuracy: 0.5889 - val_loss: 0.7677 - val_categorical_accuracy: 0.5700\n",
      "Epoch 20/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7678 - categorical_accuracy: 0.5900INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 510ms/step - loss: 0.7678 - categorical_accuracy: 0.5900 - val_loss: 0.7368 - val_categorical_accuracy: 0.6100\n",
      "Epoch 21/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.7924 - categorical_accuracy: 0.5856 - val_loss: 0.8150 - val_categorical_accuracy: 0.5900\n",
      "Epoch 22/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.7752 - categorical_accuracy: 0.6200 - val_loss: 0.8004 - val_categorical_accuracy: 0.5900\n",
      "Epoch 23/100\n",
      "113/113 [==============================] - 51s 446ms/step - loss: 0.7590 - categorical_accuracy: 0.6222 - val_loss: 0.7709 - val_categorical_accuracy: 0.6000\n",
      "Epoch 24/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7470 - categorical_accuracy: 0.6400INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 503ms/step - loss: 0.7470 - categorical_accuracy: 0.6400 - val_loss: 0.7407 - val_categorical_accuracy: 0.6800\n",
      "Epoch 25/100\n",
      "113/113 [==============================] - 51s 456ms/step - loss: 0.6949 - categorical_accuracy: 0.6856 - val_loss: 0.6845 - val_categorical_accuracy: 0.6800\n",
      "Epoch 26/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7088 - categorical_accuracy: 0.7156INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 515ms/step - loss: 0.7088 - categorical_accuracy: 0.7156 - val_loss: 0.6359 - val_categorical_accuracy: 0.7800\n",
      "Epoch 27/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.6057 - categorical_accuracy: 0.7789 - val_loss: 0.6122 - val_categorical_accuracy: 0.7800\n",
      "Epoch 28/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.5726 - categorical_accuracy: 0.7922 - val_loss: 0.5490 - val_categorical_accuracy: 0.7800\n",
      "Epoch 29/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.5773 - categorical_accuracy: 0.7700 - val_loss: 0.6536 - val_categorical_accuracy: 0.7500\n",
      "Epoch 30/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.6203 - categorical_accuracy: 0.7578 - val_loss: 0.5957 - val_categorical_accuracy: 0.7600\n",
      "Epoch 31/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5099 - categorical_accuracy: 0.8278INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 59s 525ms/step - loss: 0.5099 - categorical_accuracy: 0.8278 - val_loss: 0.5750 - val_categorical_accuracy: 0.7900\n",
      "Epoch 32/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.5296 - categorical_accuracy: 0.8056 - val_loss: 0.6380 - val_categorical_accuracy: 0.7300\n",
      "Epoch 33/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.5025 - categorical_accuracy: 0.8167 - val_loss: 0.6393 - val_categorical_accuracy: 0.7700\n",
      "Epoch 34/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.5316 - categorical_accuracy: 0.8089 - val_loss: 0.6577 - val_categorical_accuracy: 0.7400\n",
      "Epoch 35/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5025 - categorical_accuracy: 0.8278INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 503ms/step - loss: 0.5025 - categorical_accuracy: 0.8278 - val_loss: 0.5503 - val_categorical_accuracy: 0.8000\n",
      "Epoch 36/100\n",
      "113/113 [==============================] - 52s 460ms/step - loss: 0.4798 - categorical_accuracy: 0.8322 - val_loss: 0.5658 - val_categorical_accuracy: 0.7900\n",
      "Epoch 37/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4456 - categorical_accuracy: 0.8544INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 510ms/step - loss: 0.4456 - categorical_accuracy: 0.8544 - val_loss: 0.5746 - val_categorical_accuracy: 0.8100\n",
      "Epoch 38/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.4612 - categorical_accuracy: 0.8422 - val_loss: 0.6220 - val_categorical_accuracy: 0.7800\n",
      "Epoch 39/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.4847 - categorical_accuracy: 0.8411 - val_loss: 0.5608 - val_categorical_accuracy: 0.7900\n",
      "Epoch 40/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.4923 - categorical_accuracy: 0.8311 - val_loss: 0.5668 - val_categorical_accuracy: 0.7900\n",
      "Epoch 41/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.4842 - categorical_accuracy: 0.8278 - val_loss: 0.7195 - val_categorical_accuracy: 0.7000\n",
      "Epoch 42/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.4352 - categorical_accuracy: 0.8600 - val_loss: 0.5334 - val_categorical_accuracy: 0.8000\n",
      "Epoch 43/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4023 - categorical_accuracy: 0.8844INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 59s 523ms/step - loss: 0.4023 - categorical_accuracy: 0.8844 - val_loss: 0.4396 - val_categorical_accuracy: 0.8700\n",
      "Epoch 44/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3983 - categorical_accuracy: 0.8722 - val_loss: 0.6895 - val_categorical_accuracy: 0.7600\n",
      "Epoch 45/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.4343 - categorical_accuracy: 0.8678 - val_loss: 0.4985 - val_categorical_accuracy: 0.8100\n",
      "Epoch 46/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.3535 - categorical_accuracy: 0.8922 - val_loss: 0.7079 - val_categorical_accuracy: 0.7500\n",
      "Epoch 47/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.4056 - categorical_accuracy: 0.8678 - val_loss: 0.4280 - val_categorical_accuracy: 0.8600\n",
      "Epoch 48/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.3670 - categorical_accuracy: 0.8867 - val_loss: 0.6978 - val_categorical_accuracy: 0.7400\n",
      "Epoch 49/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.4121 - categorical_accuracy: 0.8656 - val_loss: 0.7472 - val_categorical_accuracy: 0.7600\n",
      "Epoch 50/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.3920 - categorical_accuracy: 0.8778 - val_loss: 0.4649 - val_categorical_accuracy: 0.8300\n",
      "Epoch 51/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.3776 - categorical_accuracy: 0.8800 - val_loss: 0.5739 - val_categorical_accuracy: 0.8000\n",
      "Epoch 52/100\n",
      "113/113 [==============================] - 51s 456ms/step - loss: 0.3885 - categorical_accuracy: 0.8822 - val_loss: 0.6278 - val_categorical_accuracy: 0.7800\n",
      "Epoch 53/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.3540 - categorical_accuracy: 0.8956 - val_loss: 0.5146 - val_categorical_accuracy: 0.8400\n",
      "Epoch 54/100\n",
      "113/113 [==============================] - 52s 463ms/step - loss: 0.3355 - categorical_accuracy: 0.9022 - val_loss: 0.4200 - val_categorical_accuracy: 0.8600\n",
      "Epoch 55/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.3524 - categorical_accuracy: 0.8911 - val_loss: 0.4268 - val_categorical_accuracy: 0.8500\n",
      "Epoch 56/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.3640 - categorical_accuracy: 0.8911 - val_loss: 0.4965 - val_categorical_accuracy: 0.8400\n",
      "Epoch 57/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.3157 - categorical_accuracy: 0.9111 - val_loss: 0.4252 - val_categorical_accuracy: 0.8500\n",
      "Epoch 58/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.3352 - categorical_accuracy: 0.9033 - val_loss: 0.7226 - val_categorical_accuracy: 0.7800\n",
      "Epoch 59/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.3047 - categorical_accuracy: 0.9144 - val_loss: 0.4384 - val_categorical_accuracy: 0.8700\n",
      "Epoch 60/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.3595 - categorical_accuracy: 0.8933 - val_loss: 0.4577 - val_categorical_accuracy: 0.8600\n",
      "Epoch 61/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.2885 - categorical_accuracy: 0.9200INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 508ms/step - loss: 0.2885 - categorical_accuracy: 0.9200 - val_loss: 0.4066 - val_categorical_accuracy: 0.8900\n",
      "Epoch 62/100\n",
      "113/113 [==============================] - 52s 460ms/step - loss: 0.3012 - categorical_accuracy: 0.9133 - val_loss: 0.4599 - val_categorical_accuracy: 0.8600\n",
      "Epoch 63/100\n",
      "113/113 [==============================] - 51s 457ms/step - loss: 0.2946 - categorical_accuracy: 0.9133 - val_loss: 0.5087 - val_categorical_accuracy: 0.8200\n",
      "Epoch 64/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.2807 - categorical_accuracy: 0.9178 - val_loss: 0.4382 - val_categorical_accuracy: 0.8700\n",
      "Epoch 65/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.2937 - categorical_accuracy: 0.9189 - val_loss: 0.4842 - val_categorical_accuracy: 0.8600\n",
      "Epoch 66/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.3033 - categorical_accuracy: 0.9200 - val_loss: 0.4485 - val_categorical_accuracy: 0.8700\n",
      "Epoch 67/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.3023 - categorical_accuracy: 0.9122 - val_loss: 0.4437 - val_categorical_accuracy: 0.8600\n",
      "Epoch 68/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.2777 - categorical_accuracy: 0.9244 - val_loss: 0.4381 - val_categorical_accuracy: 0.8700\n",
      "Epoch 69/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.2983 - categorical_accuracy: 0.9200 - val_loss: 0.3792 - val_categorical_accuracy: 0.8900\n",
      "Epoch 70/100\n",
      "113/113 [==============================] - 51s 456ms/step - loss: 0.3051 - categorical_accuracy: 0.9122 - val_loss: 0.4292 - val_categorical_accuracy: 0.8800\n",
      "Epoch 71/100\n",
      "113/113 [==============================] - 52s 457ms/step - loss: 0.3089 - categorical_accuracy: 0.9144 - val_loss: 0.4018 - val_categorical_accuracy: 0.8800\n",
      "Epoch 72/100\n",
      "113/113 [==============================] - 52s 457ms/step - loss: 0.2753 - categorical_accuracy: 0.9244 - val_loss: 0.4442 - val_categorical_accuracy: 0.8700\n",
      "Epoch 73/100\n",
      "113/113 [==============================] - 52s 459ms/step - loss: 0.3125 - categorical_accuracy: 0.9044 - val_loss: 0.5365 - val_categorical_accuracy: 0.8400\n",
      "Epoch 74/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.2727 - categorical_accuracy: 0.9278 - val_loss: 0.4091 - val_categorical_accuracy: 0.8700\n",
      "Epoch 75/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.4557 - categorical_accuracy: 0.8600 - val_loss: 0.4815 - val_categorical_accuracy: 0.8500\n",
      "Epoch 76/100\n",
      "113/113 [==============================] - 51s 456ms/step - loss: 0.4240 - categorical_accuracy: 0.8578 - val_loss: 0.4209 - val_categorical_accuracy: 0.8900\n",
      "Epoch 77/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.4365 - categorical_accuracy: 0.8600 - val_loss: 0.6409 - val_categorical_accuracy: 0.8000\n",
      "Epoch 78/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.4681 - categorical_accuracy: 0.8456 - val_loss: 0.7268 - val_categorical_accuracy: 0.7500\n",
      "Epoch 79/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.4132 - categorical_accuracy: 0.8644 - val_loss: 0.4595 - val_categorical_accuracy: 0.8700\n",
      "Epoch 80/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.4216 - categorical_accuracy: 0.8600 - val_loss: 0.9288 - val_categorical_accuracy: 0.6800\n",
      "Epoch 81/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.4877 - categorical_accuracy: 0.8344 - val_loss: 0.6403 - val_categorical_accuracy: 0.7500\n",
      "Epoch 82/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.4549 - categorical_accuracy: 0.8511 - val_loss: 0.4718 - val_categorical_accuracy: 0.8200\n",
      "Epoch 83/100\n",
      "113/113 [==============================] - 52s 459ms/step - loss: 0.4132 - categorical_accuracy: 0.8622 - val_loss: 0.5724 - val_categorical_accuracy: 0.8000\n",
      "Epoch 84/100\n",
      "113/113 [==============================] - 52s 461ms/step - loss: 0.4099 - categorical_accuracy: 0.8733 - val_loss: 0.3596 - val_categorical_accuracy: 0.8900\n",
      "Epoch 85/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.3584 - categorical_accuracy: 0.8867 - val_loss: 0.5718 - val_categorical_accuracy: 0.8200\n",
      "Epoch 86/100\n",
      "113/113 [==============================] - 52s 457ms/step - loss: 0.3383 - categorical_accuracy: 0.8967 - val_loss: 0.5884 - val_categorical_accuracy: 0.8100\n",
      "Epoch 87/100\n",
      "113/113 [==============================] - 52s 459ms/step - loss: 0.3716 - categorical_accuracy: 0.8844 - val_loss: 0.4360 - val_categorical_accuracy: 0.8800\n",
      "Epoch 88/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.3714 - categorical_accuracy: 0.8889 - val_loss: 0.4109 - val_categorical_accuracy: 0.8600\n",
      "Epoch 89/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.3686 - categorical_accuracy: 0.8833 - val_loss: 0.4547 - val_categorical_accuracy: 0.8600\n",
      "Epoch 90/100\n",
      "113/113 [==============================] - 52s 462ms/step - loss: 0.3794 - categorical_accuracy: 0.8800 - val_loss: 0.4483 - val_categorical_accuracy: 0.8600\n",
      "Epoch 91/100\n",
      "113/113 [==============================] - 52s 460ms/step - loss: 0.3467 - categorical_accuracy: 0.8967 - val_loss: 0.4917 - val_categorical_accuracy: 0.8300\n",
      "Epoch 92/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4276 - categorical_accuracy: 0.8656INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 514ms/step - loss: 0.4276 - categorical_accuracy: 0.8656 - val_loss: 0.3346 - val_categorical_accuracy: 0.9000\n",
      "Epoch 93/100\n",
      "113/113 [==============================] - 52s 459ms/step - loss: 0.3803 - categorical_accuracy: 0.8844 - val_loss: 0.5051 - val_categorical_accuracy: 0.8300\n",
      "Epoch 94/100\n",
      "113/113 [==============================] - 51s 456ms/step - loss: 0.3907 - categorical_accuracy: 0.8700 - val_loss: 0.4872 - val_categorical_accuracy: 0.8200\n",
      "Epoch 95/100\n",
      "113/113 [==============================] - 52s 460ms/step - loss: 0.3270 - categorical_accuracy: 0.9056 - val_loss: 0.4560 - val_categorical_accuracy: 0.8700\n",
      "Epoch 96/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.3558 - categorical_accuracy: 0.8944 - val_loss: 0.3986 - val_categorical_accuracy: 0.8800\n",
      "Epoch 97/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.3054 - categorical_accuracy: 0.9033 - val_loss: 0.4988 - val_categorical_accuracy: 0.8500\n",
      "Epoch 98/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.3453 - categorical_accuracy: 0.8889 - val_loss: 0.5048 - val_categorical_accuracy: 0.8400\n",
      "Epoch 99/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.3556 - categorical_accuracy: 0.8967 - val_loss: 0.3912 - val_categorical_accuracy: 0.8700\n",
      "Epoch 100/100\n",
      "113/113 [==============================] - 52s 457ms/step - loss: 0.3030 - categorical_accuracy: 0.9111 - val_loss: 0.7505 - val_categorical_accuracy: 0.7400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁▁▁▂▃▃▃▃▃▄▅▆▇▆▇▇▇▇█▇▇█████████▇▇▇▇▇▇▇▇██</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>█████▇▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁█████▇▇▇▇▆</td></tr><tr><td>epoch/loss</td><td>██▇▇▆▆▅▅▅▅▅▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▁▁</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁▁▁▂▃▃▃▃▃▄▆▆▆▆▆▆▅▆▆▆▇▇▇███▇███▇█▇██▇█▇▇▆</td></tr><tr><td>epoch/val_loss</td><td>██▇▇▆▅▅▅▅▅▄▃▃▄▃▃▄▄▄▅▃▂▂▂▂▂▂▁▂▂▄▂▂▁▂▂▁▂▂▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.91111</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>8e-05</td></tr><tr><td>epoch/loss</td><td>0.30305</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.74</td></tr><tr><td>epoch/val_loss</td><td>0.75053</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM64-Dense64-allfeatures</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/tyh5w2ae' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/tyh5w2ae</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240408_124402-tyh5w2ae/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fda527cdad0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(64, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, 1629),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"LSTM64-Dense64-allfeatures.tf\"),\n",
    "            run_name=\"LSTM64-Dense64-allfeatures\", T_max=75, epochs=100, \n",
    "            max_lr = 1e-4, min_lr = 2.5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240408_141317-jhepc329</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/jhepc329' target=\"_blank\">LSTM64-Dense128-allfeatures</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/jhepc329' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/jhepc329</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.1320 - categorical_accuracy: 0.3978INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 67s 557ms/step - loss: 1.1320 - categorical_accuracy: 0.3978 - val_loss: 1.1157 - val_categorical_accuracy: 0.4100\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.1137 - categorical_accuracy: 0.4156INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 517ms/step - loss: 1.1137 - categorical_accuracy: 0.4156 - val_loss: 1.0914 - val_categorical_accuracy: 0.4600\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 1.0936 - categorical_accuracy: 0.4167 - val_loss: 1.0644 - val_categorical_accuracy: 0.4300\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0683 - categorical_accuracy: 0.3889INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 56s 501ms/step - loss: 1.0683 - categorical_accuracy: 0.3889 - val_loss: 1.0305 - val_categorical_accuracy: 0.4700\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - 52s 457ms/step - loss: 1.0345 - categorical_accuracy: 0.4256 - val_loss: 0.9855 - val_categorical_accuracy: 0.4300\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0035 - categorical_accuracy: 0.4456INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 59s 520ms/step - loss: 1.0035 - categorical_accuracy: 0.4456 - val_loss: 0.9437 - val_categorical_accuracy: 0.4900\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0103 - categorical_accuracy: 0.4544INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 513ms/step - loss: 1.0103 - categorical_accuracy: 0.4544 - val_loss: 0.9660 - val_categorical_accuracy: 0.5200\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.9675 - categorical_accuracy: 0.4767 - val_loss: 1.0216 - val_categorical_accuracy: 0.4900\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.9119 - categorical_accuracy: 0.5267INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 59s 517ms/step - loss: 0.9119 - categorical_accuracy: 0.5267 - val_loss: 0.8566 - val_categorical_accuracy: 0.5600\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.9029 - categorical_accuracy: 0.5400INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 59s 518ms/step - loss: 0.9029 - categorical_accuracy: 0.5400 - val_loss: 0.8066 - val_categorical_accuracy: 0.5800\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8586 - categorical_accuracy: 0.5511INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 511ms/step - loss: 0.8586 - categorical_accuracy: 0.5511 - val_loss: 0.8088 - val_categorical_accuracy: 0.5900\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - 52s 457ms/step - loss: 0.8127 - categorical_accuracy: 0.5533 - val_loss: 0.7929 - val_categorical_accuracy: 0.5800\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.8108 - categorical_accuracy: 0.5578 - val_loss: 0.7927 - val_categorical_accuracy: 0.5700\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.8790 - categorical_accuracy: 0.5344 - val_loss: 0.8307 - val_categorical_accuracy: 0.5500\n",
      "Epoch 15/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.7909 - categorical_accuracy: 0.5622 - val_loss: 0.8010 - val_categorical_accuracy: 0.5500\n",
      "Epoch 16/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.8140 - categorical_accuracy: 0.5711 - val_loss: 0.8049 - val_categorical_accuracy: 0.5600\n",
      "Epoch 17/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8103 - categorical_accuracy: 0.5778INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 508ms/step - loss: 0.8103 - categorical_accuracy: 0.5778 - val_loss: 0.8149 - val_categorical_accuracy: 0.6000\n",
      "Epoch 18/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.8039 - categorical_accuracy: 0.5756 - val_loss: 0.7945 - val_categorical_accuracy: 0.5800\n",
      "Epoch 19/100\n",
      "113/113 [==============================] - 52s 457ms/step - loss: 0.7993 - categorical_accuracy: 0.5856 - val_loss: 0.7896 - val_categorical_accuracy: 0.5500\n",
      "Epoch 20/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.7795 - categorical_accuracy: 0.5911 - val_loss: 0.8051 - val_categorical_accuracy: 0.5300\n",
      "Epoch 21/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.7706 - categorical_accuracy: 0.6022 - val_loss: 0.7732 - val_categorical_accuracy: 0.5700\n",
      "Epoch 22/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.7396 - categorical_accuracy: 0.6156 - val_loss: 0.8178 - val_categorical_accuracy: 0.5900\n",
      "Epoch 23/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.7452 - categorical_accuracy: 0.6011 - val_loss: 0.7757 - val_categorical_accuracy: 0.5900\n",
      "Epoch 24/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.7296 - categorical_accuracy: 0.6189 - val_loss: 0.7378 - val_categorical_accuracy: 0.6000\n",
      "Epoch 25/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.7152 - categorical_accuracy: 0.6333 - val_loss: 0.7172 - val_categorical_accuracy: 0.5900\n",
      "Epoch 26/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6897 - categorical_accuracy: 0.6811INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 515ms/step - loss: 0.6897 - categorical_accuracy: 0.6811 - val_loss: 0.6801 - val_categorical_accuracy: 0.6400\n",
      "Epoch 27/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6367 - categorical_accuracy: 0.7233INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 512ms/step - loss: 0.6367 - categorical_accuracy: 0.7233 - val_loss: 0.6453 - val_categorical_accuracy: 0.7200\n",
      "Epoch 28/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5524 - categorical_accuracy: 0.7744INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 515ms/step - loss: 0.5524 - categorical_accuracy: 0.7744 - val_loss: 0.5173 - val_categorical_accuracy: 0.8400\n",
      "Epoch 29/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.5915 - categorical_accuracy: 0.7633 - val_loss: 0.6240 - val_categorical_accuracy: 0.7300\n",
      "Epoch 30/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.5347 - categorical_accuracy: 0.7911 - val_loss: 0.4735 - val_categorical_accuracy: 0.8400\n",
      "Epoch 31/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.5165 - categorical_accuracy: 0.7944 - val_loss: 0.4232 - val_categorical_accuracy: 0.8100\n",
      "Epoch 32/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.4915 - categorical_accuracy: 0.8178 - val_loss: 0.4979 - val_categorical_accuracy: 0.8100\n",
      "Epoch 33/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4810 - categorical_accuracy: 0.8333INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 518ms/step - loss: 0.4810 - categorical_accuracy: 0.8333 - val_loss: 0.4634 - val_categorical_accuracy: 0.8500\n",
      "Epoch 34/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4596 - categorical_accuracy: 0.8411INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 515ms/step - loss: 0.4596 - categorical_accuracy: 0.8411 - val_loss: 0.4470 - val_categorical_accuracy: 0.8600\n",
      "Epoch 35/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4790 - categorical_accuracy: 0.8244INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 516ms/step - loss: 0.4790 - categorical_accuracy: 0.8244 - val_loss: 0.3842 - val_categorical_accuracy: 0.8700\n",
      "Epoch 36/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.4455 - categorical_accuracy: 0.8478 - val_loss: 0.4243 - val_categorical_accuracy: 0.8600\n",
      "Epoch 37/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.4786 - categorical_accuracy: 0.8322 - val_loss: 0.4498 - val_categorical_accuracy: 0.8400\n",
      "Epoch 38/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.4863 - categorical_accuracy: 0.8233 - val_loss: 0.4660 - val_categorical_accuracy: 0.8300\n",
      "Epoch 39/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.4091 - categorical_accuracy: 0.8544 - val_loss: 0.4194 - val_categorical_accuracy: 0.8700\n",
      "Epoch 40/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.4525 - categorical_accuracy: 0.8433 - val_loss: 0.5789 - val_categorical_accuracy: 0.8100\n",
      "Epoch 41/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.3981 - categorical_accuracy: 0.8767INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 59s 520ms/step - loss: 0.3981 - categorical_accuracy: 0.8767 - val_loss: 0.2976 - val_categorical_accuracy: 0.9100\n",
      "Epoch 42/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.4388 - categorical_accuracy: 0.8467 - val_loss: 0.6030 - val_categorical_accuracy: 0.7800\n",
      "Epoch 43/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.4349 - categorical_accuracy: 0.8567 - val_loss: 0.3512 - val_categorical_accuracy: 0.8700\n",
      "Epoch 44/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3927 - categorical_accuracy: 0.8689 - val_loss: 0.4066 - val_categorical_accuracy: 0.8600\n",
      "Epoch 45/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.3653 - categorical_accuracy: 0.8867 - val_loss: 0.3749 - val_categorical_accuracy: 0.8900\n",
      "Epoch 46/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.3591 - categorical_accuracy: 0.8867 - val_loss: 0.5302 - val_categorical_accuracy: 0.8400\n",
      "Epoch 47/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.4030 - categorical_accuracy: 0.8644 - val_loss: 0.4458 - val_categorical_accuracy: 0.8300\n",
      "Epoch 48/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.3572 - categorical_accuracy: 0.8889 - val_loss: 0.3102 - val_categorical_accuracy: 0.9100\n",
      "Epoch 49/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.3615 - categorical_accuracy: 0.8844 - val_loss: 0.3618 - val_categorical_accuracy: 0.8900\n",
      "Epoch 50/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.3644 - categorical_accuracy: 0.8856 - val_loss: 0.3790 - val_categorical_accuracy: 0.8900\n",
      "Epoch 51/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.3868 - categorical_accuracy: 0.8800 - val_loss: 0.3699 - val_categorical_accuracy: 0.8800\n",
      "Epoch 52/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.3769 - categorical_accuracy: 0.8833 - val_loss: 0.3959 - val_categorical_accuracy: 0.8700\n",
      "Epoch 53/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.3448 - categorical_accuracy: 0.8978 - val_loss: 0.4130 - val_categorical_accuracy: 0.8800\n",
      "Epoch 54/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.3351 - categorical_accuracy: 0.9000 - val_loss: 0.5915 - val_categorical_accuracy: 0.7800\n",
      "Epoch 55/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.3425 - categorical_accuracy: 0.8944 - val_loss: 0.3477 - val_categorical_accuracy: 0.9000\n",
      "Epoch 56/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.3734 - categorical_accuracy: 0.8778 - val_loss: 0.4047 - val_categorical_accuracy: 0.8800\n",
      "Epoch 57/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3462 - categorical_accuracy: 0.8978 - val_loss: 0.4394 - val_categorical_accuracy: 0.8500\n",
      "Epoch 58/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.3559 - categorical_accuracy: 0.8944 - val_loss: 0.4795 - val_categorical_accuracy: 0.8300\n",
      "Epoch 59/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.3530 - categorical_accuracy: 0.8922 - val_loss: 0.3734 - val_categorical_accuracy: 0.8900\n",
      "Epoch 60/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.3217 - categorical_accuracy: 0.9044 - val_loss: 0.3667 - val_categorical_accuracy: 0.8900\n",
      "Epoch 61/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3137 - categorical_accuracy: 0.9100 - val_loss: 0.3653 - val_categorical_accuracy: 0.9100\n",
      "Epoch 62/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.3203 - categorical_accuracy: 0.9033 - val_loss: 0.3331 - val_categorical_accuracy: 0.9000\n",
      "Epoch 63/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.3133 - categorical_accuracy: 0.9033 - val_loss: 0.3486 - val_categorical_accuracy: 0.9000\n",
      "Epoch 64/100\n",
      "113/113 [==============================] - 52s 460ms/step - loss: 0.3524 - categorical_accuracy: 0.9022 - val_loss: 0.3530 - val_categorical_accuracy: 0.8900\n",
      "Epoch 65/100\n",
      "113/113 [==============================] - 52s 452ms/step - loss: 0.2923 - categorical_accuracy: 0.9189 - val_loss: 0.3919 - val_categorical_accuracy: 0.8800\n",
      "Epoch 66/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.3018 - categorical_accuracy: 0.9178 - val_loss: 0.5833 - val_categorical_accuracy: 0.8100\n",
      "Epoch 67/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.3623 - categorical_accuracy: 0.8911 - val_loss: 0.3731 - val_categorical_accuracy: 0.8900\n",
      "Epoch 68/100\n",
      "113/113 [==============================] - 52s 457ms/step - loss: 0.3082 - categorical_accuracy: 0.9022 - val_loss: 0.4030 - val_categorical_accuracy: 0.8600\n",
      "Epoch 69/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.3097 - categorical_accuracy: 0.9122 - val_loss: 0.3692 - val_categorical_accuracy: 0.8800\n",
      "Epoch 70/100\n",
      "113/113 [==============================] - 52s 459ms/step - loss: 0.2826 - categorical_accuracy: 0.9233 - val_loss: 0.4324 - val_categorical_accuracy: 0.8600\n",
      "Epoch 71/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.2819 - categorical_accuracy: 0.9189 - val_loss: 0.5258 - val_categorical_accuracy: 0.8300\n",
      "Epoch 72/100\n",
      "113/113 [==============================] - 52s 459ms/step - loss: 0.3141 - categorical_accuracy: 0.9111 - val_loss: 0.4164 - val_categorical_accuracy: 0.8800\n",
      "Epoch 73/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.3128 - categorical_accuracy: 0.9156 - val_loss: 0.3882 - val_categorical_accuracy: 0.8800\n",
      "Epoch 74/100\n",
      "113/113 [==============================] - 51s 446ms/step - loss: 0.3042 - categorical_accuracy: 0.9144 - val_loss: 0.4380 - val_categorical_accuracy: 0.8700\n",
      "Epoch 75/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.4798 - categorical_accuracy: 0.8467 - val_loss: 0.5084 - val_categorical_accuracy: 0.8300\n",
      "Epoch 76/100\n",
      "113/113 [==============================] - 52s 457ms/step - loss: 0.5408 - categorical_accuracy: 0.8244 - val_loss: 0.4815 - val_categorical_accuracy: 0.8300\n",
      "Epoch 77/100\n",
      "113/113 [==============================] - 52s 457ms/step - loss: 0.4122 - categorical_accuracy: 0.8633 - val_loss: 0.6058 - val_categorical_accuracy: 0.8200\n",
      "Epoch 78/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.5175 - categorical_accuracy: 0.8189 - val_loss: 0.4488 - val_categorical_accuracy: 0.8500\n",
      "Epoch 79/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.4475 - categorical_accuracy: 0.8511 - val_loss: 0.3743 - val_categorical_accuracy: 0.9000\n",
      "Epoch 80/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.4989 - categorical_accuracy: 0.8378 - val_loss: 0.8412 - val_categorical_accuracy: 0.7000\n",
      "Epoch 81/100\n",
      "113/113 [==============================] - 52s 459ms/step - loss: 0.4617 - categorical_accuracy: 0.8389 - val_loss: 0.3929 - val_categorical_accuracy: 0.8900\n",
      "Epoch 82/100\n",
      "113/113 [==============================] - 52s 460ms/step - loss: 0.3690 - categorical_accuracy: 0.8878 - val_loss: 0.5399 - val_categorical_accuracy: 0.8200\n",
      "Epoch 83/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.5458 - categorical_accuracy: 0.8256 - val_loss: 0.6096 - val_categorical_accuracy: 0.7900\n",
      "Epoch 84/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.3830 - categorical_accuracy: 0.8844 - val_loss: 0.3760 - val_categorical_accuracy: 0.8900\n",
      "Epoch 85/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.4080 - categorical_accuracy: 0.8711 - val_loss: 0.4618 - val_categorical_accuracy: 0.8400\n",
      "Epoch 86/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.4076 - categorical_accuracy: 0.8622 - val_loss: 0.6547 - val_categorical_accuracy: 0.7900\n",
      "Epoch 87/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.3966 - categorical_accuracy: 0.8689 - val_loss: 0.6459 - val_categorical_accuracy: 0.7500\n",
      "Epoch 88/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3849 - categorical_accuracy: 0.8789 - val_loss: 0.3922 - val_categorical_accuracy: 0.8800\n",
      "Epoch 89/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.3967 - categorical_accuracy: 0.8722 - val_loss: 0.4036 - val_categorical_accuracy: 0.8500\n",
      "Epoch 90/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.4118 - categorical_accuracy: 0.8667 - val_loss: 0.3600 - val_categorical_accuracy: 0.8700\n",
      "Epoch 91/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.4056 - categorical_accuracy: 0.8733 - val_loss: 0.5976 - val_categorical_accuracy: 0.7800\n",
      "Epoch 92/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.4430 - categorical_accuracy: 0.8622 - val_loss: 0.3574 - val_categorical_accuracy: 0.8900\n",
      "Epoch 93/100\n",
      "113/113 [==============================] - 52s 455ms/step - loss: 0.3914 - categorical_accuracy: 0.8733 - val_loss: 0.4379 - val_categorical_accuracy: 0.8500\n",
      "Epoch 94/100\n",
      "113/113 [==============================] - 52s 461ms/step - loss: 0.3451 - categorical_accuracy: 0.8922 - val_loss: 0.4518 - val_categorical_accuracy: 0.8600\n",
      "Epoch 95/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.3373 - categorical_accuracy: 0.9000 - val_loss: 0.5741 - val_categorical_accuracy: 0.8000\n",
      "Epoch 96/100\n",
      "113/113 [==============================] - 51s 457ms/step - loss: 0.3771 - categorical_accuracy: 0.8856 - val_loss: 0.4136 - val_categorical_accuracy: 0.8600\n",
      "Epoch 97/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.3577 - categorical_accuracy: 0.8922 - val_loss: 0.4052 - val_categorical_accuracy: 0.8700\n",
      "Epoch 98/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.3453 - categorical_accuracy: 0.9000INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 516ms/step - loss: 0.3453 - categorical_accuracy: 0.9000 - val_loss: 0.2897 - val_categorical_accuracy: 0.9200\n",
      "Epoch 99/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.3037 - categorical_accuracy: 0.9133 - val_loss: 0.4313 - val_categorical_accuracy: 0.8800\n",
      "Epoch 100/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.3411 - categorical_accuracy: 0.8978 - val_loss: 0.4559 - val_categorical_accuracy: 0.8600\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁▁▂▂▃▃▃▃▄▄▅▆▆▇▇▇▇▇████████████▇▇██▇▇▇███</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>█████▇▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁█████▇▇▇▇▆</td></tr><tr><td>epoch/loss</td><td>██▇▇▆▅▅▅▅▅▄▃▃▂▂▂▂▂▁▁▂▁▂▁▁▁▁▁▁▁▂▂▂▂▂▂▂▁▁▁</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁▁▂▂▄▃▃▃▃▄▄▇▇▇▇▇█▇▇██▆███████▇▇█▇█▆▇█▇▇▇</td></tr><tr><td>epoch/val_loss</td><td>██▇▇▅▅▅▅▅▅▄▃▂▂▂▂▁▂▃▂▂▄▂▂▂▁▂▂▂▂▄▂▃▂▄▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.89778</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>8e-05</td></tr><tr><td>epoch/loss</td><td>0.3411</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.86</td></tr><tr><td>epoch/val_loss</td><td>0.45587</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM64-Dense128-allfeatures</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/jhepc329' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/jhepc329</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240408_141317-jhepc329/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fda52b45090>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(64, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, 1629),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"LSTM64-Dense128allfeatures.tf\"),\n",
    "            run_name=\"LSTM64-Dense128-allfeatures\", T_max=75, epochs=100, \n",
    "            max_lr = 1e-4, min_lr = 2.5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240408_154309-dtxcqcb5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/dtxcqcb5' target=\"_blank\">LSTM64-Dense257-allfeatures</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/dtxcqcb5' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/dtxcqcb5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.1294 - categorical_accuracy: 0.4156INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 62s 512ms/step - loss: 1.1294 - categorical_accuracy: 0.4156 - val_loss: 1.1076 - val_categorical_accuracy: 0.4400\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 1.1075 - categorical_accuracy: 0.4222 - val_loss: 1.0894 - val_categorical_accuracy: 0.4300\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 1.0856 - categorical_accuracy: 0.4189 - val_loss: 1.0600 - val_categorical_accuracy: 0.4300\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0583 - categorical_accuracy: 0.4211INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 500ms/step - loss: 1.0583 - categorical_accuracy: 0.4211 - val_loss: 1.0436 - val_categorical_accuracy: 0.4500\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - 53s 466ms/step - loss: 1.0402 - categorical_accuracy: 0.4322 - val_loss: 1.0101 - val_categorical_accuracy: 0.4200\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.9952 - categorical_accuracy: 0.4556INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 59s 520ms/step - loss: 0.9952 - categorical_accuracy: 0.4556 - val_loss: 0.9143 - val_categorical_accuracy: 0.5800\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.9315 - categorical_accuracy: 0.5089INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 518ms/step - loss: 0.9315 - categorical_accuracy: 0.5089 - val_loss: 0.8479 - val_categorical_accuracy: 0.5900\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.9245 - categorical_accuracy: 0.5300 - val_loss: 0.8585 - val_categorical_accuracy: 0.5500\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.8849 - categorical_accuracy: 0.5322 - val_loss: 0.8620 - val_categorical_accuracy: 0.5700\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.9289 - categorical_accuracy: 0.5156 - val_loss: 0.8300 - val_categorical_accuracy: 0.5600\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.8394 - categorical_accuracy: 0.5600 - val_loss: 0.8156 - val_categorical_accuracy: 0.5700\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.8493 - categorical_accuracy: 0.5567 - val_loss: 0.8283 - val_categorical_accuracy: 0.5800\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - 51s 446ms/step - loss: 0.8212 - categorical_accuracy: 0.5533 - val_loss: 0.8123 - val_categorical_accuracy: 0.5100\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.8168 - categorical_accuracy: 0.5822 - val_loss: 0.8011 - val_categorical_accuracy: 0.5700\n",
      "Epoch 15/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.8150 - categorical_accuracy: 0.5711 - val_loss: 0.7711 - val_categorical_accuracy: 0.5600\n",
      "Epoch 16/100\n",
      "113/113 [==============================] - 51s 447ms/step - loss: 0.7867 - categorical_accuracy: 0.5933 - val_loss: 0.7957 - val_categorical_accuracy: 0.5300\n",
      "Epoch 17/100\n",
      "113/113 [==============================] - 51s 447ms/step - loss: 0.7756 - categorical_accuracy: 0.5911 - val_loss: 0.7544 - val_categorical_accuracy: 0.5900\n",
      "Epoch 18/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7765 - categorical_accuracy: 0.5956INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 508ms/step - loss: 0.7765 - categorical_accuracy: 0.5956 - val_loss: 0.7695 - val_categorical_accuracy: 0.6000\n",
      "Epoch 19/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.7625 - categorical_accuracy: 0.6044 - val_loss: 0.8354 - val_categorical_accuracy: 0.5600\n",
      "Epoch 20/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.7641 - categorical_accuracy: 0.5956 - val_loss: 0.7856 - val_categorical_accuracy: 0.5800\n",
      "Epoch 21/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.7537 - categorical_accuracy: 0.6067 - val_loss: 0.8254 - val_categorical_accuracy: 0.5800\n",
      "Epoch 22/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.7807 - categorical_accuracy: 0.6044 - val_loss: 0.7774 - val_categorical_accuracy: 0.5600\n",
      "Epoch 23/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7218 - categorical_accuracy: 0.6544INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 515ms/step - loss: 0.7218 - categorical_accuracy: 0.6544 - val_loss: 0.7005 - val_categorical_accuracy: 0.6600\n",
      "Epoch 24/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6934 - categorical_accuracy: 0.6756INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 512ms/step - loss: 0.6934 - categorical_accuracy: 0.6756 - val_loss: 0.7214 - val_categorical_accuracy: 0.6900\n",
      "Epoch 25/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6997 - categorical_accuracy: 0.7067INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 515ms/step - loss: 0.6997 - categorical_accuracy: 0.7067 - val_loss: 0.6414 - val_categorical_accuracy: 0.7300\n",
      "Epoch 26/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6192 - categorical_accuracy: 0.7644INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 508ms/step - loss: 0.6192 - categorical_accuracy: 0.7644 - val_loss: 0.6036 - val_categorical_accuracy: 0.8000\n",
      "Epoch 27/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.5595 - categorical_accuracy: 0.7889 - val_loss: 0.6018 - val_categorical_accuracy: 0.7900\n",
      "Epoch 28/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.5250 - categorical_accuracy: 0.8167 - val_loss: 0.6052 - val_categorical_accuracy: 0.7800\n",
      "Epoch 29/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.5321 - categorical_accuracy: 0.7844 - val_loss: 0.5875 - val_categorical_accuracy: 0.7800\n",
      "Epoch 30/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5018 - categorical_accuracy: 0.8189INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 511ms/step - loss: 0.5018 - categorical_accuracy: 0.8189 - val_loss: 0.4842 - val_categorical_accuracy: 0.8100\n",
      "Epoch 31/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.5003 - categorical_accuracy: 0.8244 - val_loss: 0.7470 - val_categorical_accuracy: 0.7200\n",
      "Epoch 32/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5084 - categorical_accuracy: 0.8100INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 56s 494ms/step - loss: 0.5084 - categorical_accuracy: 0.8100 - val_loss: 0.4414 - val_categorical_accuracy: 0.8500\n",
      "Epoch 33/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.4533 - categorical_accuracy: 0.8344 - val_loss: 0.4862 - val_categorical_accuracy: 0.8400\n",
      "Epoch 34/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.4792 - categorical_accuracy: 0.8311 - val_loss: 0.5741 - val_categorical_accuracy: 0.7900\n",
      "Epoch 35/100\n",
      "113/113 [==============================] - 50s 441ms/step - loss: 0.4670 - categorical_accuracy: 0.8356 - val_loss: 0.5930 - val_categorical_accuracy: 0.7900\n",
      "Epoch 36/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.4843 - categorical_accuracy: 0.8300 - val_loss: 0.5214 - val_categorical_accuracy: 0.8300\n",
      "Epoch 37/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4420 - categorical_accuracy: 0.8489INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 56s 494ms/step - loss: 0.4420 - categorical_accuracy: 0.8489 - val_loss: 0.4315 - val_categorical_accuracy: 0.8700\n",
      "Epoch 38/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.4192 - categorical_accuracy: 0.8567 - val_loss: 0.4976 - val_categorical_accuracy: 0.8400\n",
      "Epoch 39/100\n",
      "113/113 [==============================] - 51s 456ms/step - loss: 0.4574 - categorical_accuracy: 0.8400 - val_loss: 0.4369 - val_categorical_accuracy: 0.8600\n",
      "Epoch 40/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.4144 - categorical_accuracy: 0.8656 - val_loss: 0.5767 - val_categorical_accuracy: 0.7900\n",
      "Epoch 41/100\n",
      "113/113 [==============================] - 51s 456ms/step - loss: 0.4299 - categorical_accuracy: 0.8578 - val_loss: 0.5178 - val_categorical_accuracy: 0.8000\n",
      "Epoch 42/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.4259 - categorical_accuracy: 0.8633 - val_loss: 0.6478 - val_categorical_accuracy: 0.7800\n",
      "Epoch 43/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.4309 - categorical_accuracy: 0.8544 - val_loss: 0.5246 - val_categorical_accuracy: 0.8400\n",
      "Epoch 44/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.4161 - categorical_accuracy: 0.8633 - val_loss: 0.5034 - val_categorical_accuracy: 0.8600\n",
      "Epoch 45/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.3872 - categorical_accuracy: 0.8789INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 507ms/step - loss: 0.3872 - categorical_accuracy: 0.8789 - val_loss: 0.4209 - val_categorical_accuracy: 0.8900\n",
      "Epoch 46/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3671 - categorical_accuracy: 0.8922 - val_loss: 0.5633 - val_categorical_accuracy: 0.7900\n",
      "Epoch 47/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.3706 - categorical_accuracy: 0.8822 - val_loss: 0.6383 - val_categorical_accuracy: 0.7500\n",
      "Epoch 48/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.3800 - categorical_accuracy: 0.8856 - val_loss: 0.4459 - val_categorical_accuracy: 0.8700\n",
      "Epoch 49/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.3679 - categorical_accuracy: 0.8911 - val_loss: 0.5862 - val_categorical_accuracy: 0.8500\n",
      "Epoch 50/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.3488 - categorical_accuracy: 0.8989 - val_loss: 0.4686 - val_categorical_accuracy: 0.8700\n",
      "Epoch 51/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.3243 - categorical_accuracy: 0.9089 - val_loss: 0.4609 - val_categorical_accuracy: 0.8500\n",
      "Epoch 52/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3512 - categorical_accuracy: 0.8944 - val_loss: 0.5035 - val_categorical_accuracy: 0.8200\n",
      "Epoch 53/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.3587 - categorical_accuracy: 0.8933 - val_loss: 0.4697 - val_categorical_accuracy: 0.8700\n",
      "Epoch 54/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.3023 - categorical_accuracy: 0.9178 - val_loss: 0.6036 - val_categorical_accuracy: 0.8400\n",
      "Epoch 55/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.3401 - categorical_accuracy: 0.9056 - val_loss: 0.4467 - val_categorical_accuracy: 0.8700\n",
      "Epoch 56/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.3360 - categorical_accuracy: 0.9000 - val_loss: 0.5005 - val_categorical_accuracy: 0.8200\n",
      "Epoch 57/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.3344 - categorical_accuracy: 0.8989 - val_loss: 0.4966 - val_categorical_accuracy: 0.8300\n",
      "Epoch 58/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3392 - categorical_accuracy: 0.9033 - val_loss: 0.4649 - val_categorical_accuracy: 0.8600\n",
      "Epoch 59/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.3297 - categorical_accuracy: 0.9033 - val_loss: 0.5340 - val_categorical_accuracy: 0.8500\n",
      "Epoch 60/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.3123 - categorical_accuracy: 0.9078 - val_loss: 0.4660 - val_categorical_accuracy: 0.8500\n",
      "Epoch 61/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.2969 - categorical_accuracy: 0.9167 - val_loss: 0.4637 - val_categorical_accuracy: 0.8800\n",
      "Epoch 62/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.3339 - categorical_accuracy: 0.9022 - val_loss: 0.5756 - val_categorical_accuracy: 0.8000\n",
      "Epoch 63/100\n",
      "113/113 [==============================] - 50s 448ms/step - loss: 0.2775 - categorical_accuracy: 0.9267 - val_loss: 0.4496 - val_categorical_accuracy: 0.8500\n",
      "Epoch 64/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.2522 - categorical_accuracy: 0.9322 - val_loss: 0.4852 - val_categorical_accuracy: 0.8500\n",
      "Epoch 65/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.2856 - categorical_accuracy: 0.9233 - val_loss: 0.4737 - val_categorical_accuracy: 0.8700\n",
      "Epoch 66/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.3039 - categorical_accuracy: 0.9111 - val_loss: 0.6630 - val_categorical_accuracy: 0.8300\n",
      "Epoch 67/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.2949 - categorical_accuracy: 0.9167 - val_loss: 0.4312 - val_categorical_accuracy: 0.8700\n",
      "Epoch 68/100\n",
      "113/113 [==============================] - 51s 447ms/step - loss: 0.3138 - categorical_accuracy: 0.9156 - val_loss: 0.5354 - val_categorical_accuracy: 0.8400\n",
      "Epoch 69/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.2769 - categorical_accuracy: 0.9189 - val_loss: 0.5554 - val_categorical_accuracy: 0.8400\n",
      "Epoch 70/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.2758 - categorical_accuracy: 0.9256 - val_loss: 0.4695 - val_categorical_accuracy: 0.8600\n",
      "Epoch 71/100\n",
      "113/113 [==============================] - 50s 448ms/step - loss: 0.2844 - categorical_accuracy: 0.9222 - val_loss: 0.4849 - val_categorical_accuracy: 0.8600\n",
      "Epoch 72/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.2574 - categorical_accuracy: 0.9322 - val_loss: 0.4980 - val_categorical_accuracy: 0.8400\n",
      "Epoch 73/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.2820 - categorical_accuracy: 0.9200 - val_loss: 0.4960 - val_categorical_accuracy: 0.8400\n",
      "Epoch 74/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.2596 - categorical_accuracy: 0.9278 - val_loss: 0.4992 - val_categorical_accuracy: 0.8500\n",
      "Epoch 75/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.5701 - categorical_accuracy: 0.8200 - val_loss: 0.4703 - val_categorical_accuracy: 0.8100\n",
      "Epoch 76/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4652 - categorical_accuracy: 0.8444INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 507ms/step - loss: 0.4652 - categorical_accuracy: 0.8444 - val_loss: 0.3677 - val_categorical_accuracy: 0.9100\n",
      "Epoch 77/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.4806 - categorical_accuracy: 0.8356 - val_loss: 0.4930 - val_categorical_accuracy: 0.8700\n",
      "Epoch 78/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.3906 - categorical_accuracy: 0.8889 - val_loss: 0.4066 - val_categorical_accuracy: 0.8600\n",
      "Epoch 79/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.4055 - categorical_accuracy: 0.8744 - val_loss: 0.8157 - val_categorical_accuracy: 0.7000\n",
      "Epoch 80/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.4152 - categorical_accuracy: 0.8711 - val_loss: 0.4717 - val_categorical_accuracy: 0.8200\n",
      "Epoch 81/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.3796 - categorical_accuracy: 0.8944 - val_loss: 0.5826 - val_categorical_accuracy: 0.8300\n",
      "Epoch 82/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.4741 - categorical_accuracy: 0.8289 - val_loss: 0.4194 - val_categorical_accuracy: 0.8500\n",
      "Epoch 83/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.4464 - categorical_accuracy: 0.8422 - val_loss: 0.5595 - val_categorical_accuracy: 0.8300\n",
      "Epoch 84/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.4245 - categorical_accuracy: 0.8600 - val_loss: 0.4758 - val_categorical_accuracy: 0.8200\n",
      "Epoch 85/100\n",
      "113/113 [==============================] - 51s 447ms/step - loss: 0.4011 - categorical_accuracy: 0.8667 - val_loss: 0.5513 - val_categorical_accuracy: 0.8500\n",
      "Epoch 86/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3663 - categorical_accuracy: 0.8900 - val_loss: 0.4687 - val_categorical_accuracy: 0.8500\n",
      "Epoch 87/100\n",
      "113/113 [==============================] - 51s 446ms/step - loss: 0.4279 - categorical_accuracy: 0.8689 - val_loss: 0.6647 - val_categorical_accuracy: 0.8100\n",
      "Epoch 88/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.3741 - categorical_accuracy: 0.8833 - val_loss: 0.5761 - val_categorical_accuracy: 0.8500\n",
      "Epoch 89/100\n",
      "113/113 [==============================] - 51s 446ms/step - loss: 0.3585 - categorical_accuracy: 0.8900 - val_loss: 0.9366 - val_categorical_accuracy: 0.6600\n",
      "Epoch 90/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3772 - categorical_accuracy: 0.8844 - val_loss: 0.4098 - val_categorical_accuracy: 0.8700\n",
      "Epoch 91/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.3593 - categorical_accuracy: 0.8933 - val_loss: 0.5831 - val_categorical_accuracy: 0.7700\n",
      "Epoch 92/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.4430 - categorical_accuracy: 0.8567 - val_loss: 0.5215 - val_categorical_accuracy: 0.7900\n",
      "Epoch 93/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.4518 - categorical_accuracy: 0.8578 - val_loss: 0.4675 - val_categorical_accuracy: 0.8400\n",
      "Epoch 94/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3978 - categorical_accuracy: 0.8744 - val_loss: 0.5808 - val_categorical_accuracy: 0.8300\n",
      "Epoch 95/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.4023 - categorical_accuracy: 0.8756 - val_loss: 0.6677 - val_categorical_accuracy: 0.7900\n",
      "Epoch 96/100\n",
      "113/113 [==============================] - 51s 447ms/step - loss: 0.3559 - categorical_accuracy: 0.8789 - val_loss: 0.3913 - val_categorical_accuracy: 0.8900\n",
      "Epoch 97/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3396 - categorical_accuracy: 0.8956 - val_loss: 0.4530 - val_categorical_accuracy: 0.8700\n",
      "Epoch 98/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.4212 - categorical_accuracy: 0.8733 - val_loss: 0.4849 - val_categorical_accuracy: 0.8300\n",
      "Epoch 99/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.3324 - categorical_accuracy: 0.8944 - val_loss: 0.5087 - val_categorical_accuracy: 0.8200\n",
      "Epoch 100/100\n",
      "113/113 [==============================] - 50s 444ms/step - loss: 0.3433 - categorical_accuracy: 0.8956 - val_loss: 0.3723 - val_categorical_accuracy: 0.8800\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁▁▂▃▃▃▃▃▄▄▆▆▇▇▇▇▇▇▇▇██████████▇▇▇▇▇▇▇▇██</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>█████▇▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁█████▇▇▇▇▆</td></tr><tr><td>epoch/loss</td><td>██▇▆▆▆▅▅▅▅▄▃▃▃▃▃▂▂▂▂▂▁▂▂▁▁▁▁▁▁▃▂▃▂▂▂▃▂▂▂</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁▁▃▃▃▂▃▄▃▅▇▆▆▇▇█▇█▇██▇▇████▇▇██▅█▇▇▅▇▇██</td></tr><tr><td>epoch/val_loss</td><td>██▆▆▅▅▅▅▅▄▃▃▅▃▂▂▂▂▃▃▂▃▂▃▂▂▂▃▂▂▂▅▁▂▄▆▂▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.89556</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>8e-05</td></tr><tr><td>epoch/loss</td><td>0.34329</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.88</td></tr><tr><td>epoch/val_loss</td><td>0.3723</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM64-Dense257-allfeatures</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/dtxcqcb5' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/dtxcqcb5</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240408_154309-dtxcqcb5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fda49b2fa50>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(64, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, 1629),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"LSTM64-Dense256allfeatures.tf\"),\n",
    "            run_name=\"LSTM64-Dense257-allfeatures\", T_max=75, epochs=100, \n",
    "            max_lr = 1e-4, min_lr = 2.5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_shape = (25, 1629), Y_shape = (13,)\n"
     ]
    }
   ],
   "source": [
    "X_shape = train_dataset_parquet[0][0].shape\n",
    "Y_shape = train_dataset_parquet[0][1].shape\n",
    "print(f\"X_shape = {X_shape}, Y_shape = {Y_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        return x + positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:8g8ds4yd) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM256-Dense128-Dense256-LipsEyesHandsPose_5k</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/8g8ds4yd' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/8g8ds4yd</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240416_144023-8g8ds4yd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:8g8ds4yd). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0bda949114c426181685c50087a2ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011114080829752817, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240416_144046-o25m0mz7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/o25m0mz7' target=\"_blank\">Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/o25m0mz7' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/o25m0mz7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "    497/Unknown - 9s 10ms/step - loss: 2.5642 - categorical_accuracy: 0.1066"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 14:41:03.907399: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8104596135730168633\n",
      "2024-04-16 14:41:03.907462: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7063810106286413230\n",
      "2024-04-16 14:41:04.886187: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 3094190343660208166\n",
      "2024-04-16 14:41:04.886254: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 17406417933654264596\n",
      "2024-04-16 14:41:04.886267: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 6462147652021642174\n",
      "2024-04-16 14:41:04.886293: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4153888742695321310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 15s 22ms/step - loss: 2.5637 - categorical_accuracy: 0.1063 - val_loss: 2.4410 - val_categorical_accuracy: 0.1770\n",
      "Epoch 2/100\n",
      "499/500 [============================>.] - ETA: 0s - loss: 2.4526 - categorical_accuracy: 0.1586INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 2.4525 - categorical_accuracy: 0.1583 - val_loss: 2.3826 - val_categorical_accuracy: 0.2170\n",
      "Epoch 3/100\n",
      "499/500 [============================>.] - ETA: 0s - loss: 2.4056 - categorical_accuracy: 0.1829INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 2.4053 - categorical_accuracy: 0.1832 - val_loss: 2.3468 - val_categorical_accuracy: 0.2340\n",
      "Epoch 4/100\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.3575 - categorical_accuracy: 0.2017INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 2.3568 - categorical_accuracy: 0.2027 - val_loss: 2.3265 - val_categorical_accuracy: 0.2370\n",
      "Epoch 5/100\n",
      "498/500 [============================>.] - ETA: 0s - loss: 2.3225 - categorical_accuracy: 0.2161INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.3223 - categorical_accuracy: 0.2160 - val_loss: 2.2558 - val_categorical_accuracy: 0.2840\n",
      "Epoch 6/100\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.2850 - categorical_accuracy: 0.2404INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 2.2841 - categorical_accuracy: 0.2415 - val_loss: 2.2454 - val_categorical_accuracy: 0.2850\n",
      "Epoch 7/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.2539 - categorical_accuracy: 0.2630INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 2.2539 - categorical_accuracy: 0.2630 - val_loss: 2.2394 - val_categorical_accuracy: 0.3010\n",
      "Epoch 8/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.2169 - categorical_accuracy: 0.2930INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 2.2169 - categorical_accuracy: 0.2930 - val_loss: 2.2031 - val_categorical_accuracy: 0.3120\n",
      "Epoch 9/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.1895 - categorical_accuracy: 0.3065INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 2.1895 - categorical_accuracy: 0.3065 - val_loss: 2.1413 - val_categorical_accuracy: 0.3480\n",
      "Epoch 10/100\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.1722 - categorical_accuracy: 0.3239INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 2.1706 - categorical_accuracy: 0.3250 - val_loss: 2.1352 - val_categorical_accuracy: 0.3570\n",
      "Epoch 11/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.1411 - categorical_accuracy: 0.3350INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 2.1411 - categorical_accuracy: 0.3350 - val_loss: 2.1161 - val_categorical_accuracy: 0.3780\n",
      "Epoch 12/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 2.1237 - categorical_accuracy: 0.3530 - val_loss: 2.1283 - val_categorical_accuracy: 0.3730\n",
      "Epoch 13/100\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.1007 - categorical_accuracy: 0.3649INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.0994 - categorical_accuracy: 0.3660 - val_loss: 2.0815 - val_categorical_accuracy: 0.4020\n",
      "Epoch 14/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 2.0837 - categorical_accuracy: 0.3747 - val_loss: 2.1352 - val_categorical_accuracy: 0.3750\n",
      "Epoch 15/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.0698 - categorical_accuracy: 0.3758INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 2.0698 - categorical_accuracy: 0.3758 - val_loss: 2.0697 - val_categorical_accuracy: 0.4090\n",
      "Epoch 16/100\n",
      "498/500 [============================>.] - ETA: 0s - loss: 2.0529 - categorical_accuracy: 0.3981INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 17ms/step - loss: 2.0521 - categorical_accuracy: 0.3983 - val_loss: 2.0441 - val_categorical_accuracy: 0.4320\n",
      "Epoch 17/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 2.0318 - categorical_accuracy: 0.4100 - val_loss: 2.0290 - val_categorical_accuracy: 0.4290\n",
      "Epoch 18/100\n",
      "499/500 [============================>.] - ETA: 0s - loss: 2.0151 - categorical_accuracy: 0.4181INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 2.0145 - categorical_accuracy: 0.4187 - val_loss: 2.0040 - val_categorical_accuracy: 0.4380\n",
      "Epoch 19/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.9912 - categorical_accuracy: 0.4363 - val_loss: 2.0059 - val_categorical_accuracy: 0.4360\n",
      "Epoch 20/100\n",
      "499/500 [============================>.] - ETA: 0s - loss: 1.9707 - categorical_accuracy: 0.4439INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 1.9703 - categorical_accuracy: 0.4440 - val_loss: 1.9635 - val_categorical_accuracy: 0.4650\n",
      "Epoch 21/100\n",
      "497/500 [============================>.] - ETA: 0s - loss: 1.9628 - categorical_accuracy: 0.4497INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 1.9622 - categorical_accuracy: 0.4502 - val_loss: 1.9552 - val_categorical_accuracy: 0.4660\n",
      "Epoch 22/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9492 - categorical_accuracy: 0.4610INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 1.9492 - categorical_accuracy: 0.4610 - val_loss: 1.9396 - val_categorical_accuracy: 0.4700\n",
      "Epoch 23/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9306 - categorical_accuracy: 0.4767INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 1.9306 - categorical_accuracy: 0.4767 - val_loss: 1.9275 - val_categorical_accuracy: 0.4880\n",
      "Epoch 24/100\n",
      "499/500 [============================>.] - ETA: 0s - loss: 1.9128 - categorical_accuracy: 0.4907INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 1.9124 - categorical_accuracy: 0.4908 - val_loss: 1.8978 - val_categorical_accuracy: 0.4980\n",
      "Epoch 25/100\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.9010 - categorical_accuracy: 0.4937INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 1.9001 - categorical_accuracy: 0.4942 - val_loss: 1.8863 - val_categorical_accuracy: 0.5130\n",
      "Epoch 26/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.8846 - categorical_accuracy: 0.5008 - val_loss: 1.8806 - val_categorical_accuracy: 0.5130\n",
      "Epoch 27/100\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.8740 - categorical_accuracy: 0.5113INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 1.8739 - categorical_accuracy: 0.5113 - val_loss: 1.8562 - val_categorical_accuracy: 0.5320\n",
      "Epoch 28/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.8537 - categorical_accuracy: 0.5295 - val_loss: 1.8730 - val_categorical_accuracy: 0.5160\n",
      "Epoch 29/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.8375 - categorical_accuracy: 0.5315 - val_loss: 1.8677 - val_categorical_accuracy: 0.5260\n",
      "Epoch 30/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.8323 - categorical_accuracy: 0.5347 - val_loss: 1.8621 - val_categorical_accuracy: 0.5290\n",
      "Epoch 31/100\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.8232 - categorical_accuracy: 0.5391INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 1.8222 - categorical_accuracy: 0.5400 - val_loss: 1.8554 - val_categorical_accuracy: 0.5370\n",
      "Epoch 32/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.8130 - categorical_accuracy: 0.5477INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 1.8130 - categorical_accuracy: 0.5477 - val_loss: 1.8451 - val_categorical_accuracy: 0.5410\n",
      "Epoch 33/100\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.8044 - categorical_accuracy: 0.5522INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 19ms/step - loss: 1.8042 - categorical_accuracy: 0.5520 - val_loss: 1.8561 - val_categorical_accuracy: 0.5420\n",
      "Epoch 34/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.7991 - categorical_accuracy: 0.5592INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 1.7991 - categorical_accuracy: 0.5592 - val_loss: 1.8426 - val_categorical_accuracy: 0.5500\n",
      "Epoch 35/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.7870 - categorical_accuracy: 0.5608INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 1.7870 - categorical_accuracy: 0.5608 - val_loss: 1.8362 - val_categorical_accuracy: 0.5560\n",
      "Epoch 36/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.7817 - categorical_accuracy: 0.5617INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 1.7817 - categorical_accuracy: 0.5617 - val_loss: 1.8141 - val_categorical_accuracy: 0.5670\n",
      "Epoch 37/100\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.7763 - categorical_accuracy: 0.5751INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 1.7753 - categorical_accuracy: 0.5755 - val_loss: 1.8230 - val_categorical_accuracy: 0.5700\n",
      "Epoch 38/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7616 - categorical_accuracy: 0.5803 - val_loss: 1.8388 - val_categorical_accuracy: 0.5530\n",
      "Epoch 39/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.7548 - categorical_accuracy: 0.5788INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 1.7548 - categorical_accuracy: 0.5788 - val_loss: 1.8051 - val_categorical_accuracy: 0.5730\n",
      "Epoch 40/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7525 - categorical_accuracy: 0.5828 - val_loss: 1.8235 - val_categorical_accuracy: 0.5640\n",
      "Epoch 41/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.7422 - categorical_accuracy: 0.5893 - val_loss: 1.8186 - val_categorical_accuracy: 0.5590\n",
      "Epoch 42/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.7401 - categorical_accuracy: 0.5903 - val_loss: 1.8079 - val_categorical_accuracy: 0.5710\n",
      "Epoch 43/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7330 - categorical_accuracy: 0.5950 - val_loss: 1.8170 - val_categorical_accuracy: 0.5600\n",
      "Epoch 44/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.7240 - categorical_accuracy: 0.5960 - val_loss: 1.8233 - val_categorical_accuracy: 0.5590\n",
      "Epoch 45/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7182 - categorical_accuracy: 0.6047 - val_loss: 1.8407 - val_categorical_accuracy: 0.5610\n",
      "Epoch 46/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7176 - categorical_accuracy: 0.6000 - val_loss: 1.8124 - val_categorical_accuracy: 0.5720\n",
      "Epoch 47/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7088 - categorical_accuracy: 0.6033 - val_loss: 1.8191 - val_categorical_accuracy: 0.5730\n",
      "Epoch 48/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.7030 - categorical_accuracy: 0.6200INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 1.7030 - categorical_accuracy: 0.6200 - val_loss: 1.8060 - val_categorical_accuracy: 0.5780\n",
      "Epoch 49/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7019 - categorical_accuracy: 0.6115 - val_loss: 1.8178 - val_categorical_accuracy: 0.5630\n",
      "Epoch 50/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.7680 - categorical_accuracy: 0.5765 - val_loss: 1.8046 - val_categorical_accuracy: 0.5600\n",
      "Epoch 51/100\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.7563 - categorical_accuracy: 0.5748INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 1.7557 - categorical_accuracy: 0.5753 - val_loss: 1.7978 - val_categorical_accuracy: 0.5820\n",
      "Epoch 52/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.7510 - categorical_accuracy: 0.5810 - val_loss: 1.8712 - val_categorical_accuracy: 0.5540\n",
      "Epoch 53/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7417 - categorical_accuracy: 0.5853 - val_loss: 1.8068 - val_categorical_accuracy: 0.5570\n",
      "Epoch 54/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7295 - categorical_accuracy: 0.5880 - val_loss: 1.8239 - val_categorical_accuracy: 0.5620\n",
      "Epoch 55/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7309 - categorical_accuracy: 0.5910 - val_loss: 1.8488 - val_categorical_accuracy: 0.5520\n",
      "Epoch 56/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7183 - categorical_accuracy: 0.6053 - val_loss: 1.8426 - val_categorical_accuracy: 0.5610\n",
      "Epoch 57/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.7104 - categorical_accuracy: 0.5990 - val_loss: 1.8623 - val_categorical_accuracy: 0.5350\n",
      "Epoch 58/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7138 - categorical_accuracy: 0.5930 - val_loss: 1.8272 - val_categorical_accuracy: 0.5620\n",
      "Epoch 59/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7024 - categorical_accuracy: 0.6083 - val_loss: 1.8635 - val_categorical_accuracy: 0.5460\n",
      "Epoch 60/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6863 - categorical_accuracy: 0.6080 - val_loss: 1.8260 - val_categorical_accuracy: 0.5730\n",
      "Epoch 61/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6903 - categorical_accuracy: 0.6183 - val_loss: 1.8453 - val_categorical_accuracy: 0.5680\n",
      "Epoch 62/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6764 - categorical_accuracy: 0.6292 - val_loss: 1.8362 - val_categorical_accuracy: 0.5680\n",
      "Epoch 63/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.6755 - categorical_accuracy: 0.6212 - val_loss: 1.8167 - val_categorical_accuracy: 0.5770\n",
      "Epoch 64/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.6608 - categorical_accuracy: 0.6365 - val_loss: 1.8232 - val_categorical_accuracy: 0.5630\n",
      "Epoch 65/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6648 - categorical_accuracy: 0.6283 - val_loss: 1.8192 - val_categorical_accuracy: 0.5680\n",
      "Epoch 66/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6520 - categorical_accuracy: 0.6435 - val_loss: 1.8179 - val_categorical_accuracy: 0.5650\n",
      "Epoch 67/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6513 - categorical_accuracy: 0.6385 - val_loss: 1.8330 - val_categorical_accuracy: 0.5770\n",
      "Epoch 68/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6366 - categorical_accuracy: 0.6485 - val_loss: 1.8068 - val_categorical_accuracy: 0.5710\n",
      "Epoch 69/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.6300 - categorical_accuracy: 0.6560 - val_loss: 1.8123 - val_categorical_accuracy: 0.5730\n",
      "Epoch 70/100\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.6224 - categorical_accuracy: 0.6637INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 1.6214 - categorical_accuracy: 0.6645 - val_loss: 1.8264 - val_categorical_accuracy: 0.5830\n",
      "Epoch 71/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6211 - categorical_accuracy: 0.6637 - val_loss: 1.7917 - val_categorical_accuracy: 0.5830\n",
      "Epoch 72/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6197 - categorical_accuracy: 0.6585 - val_loss: 1.8046 - val_categorical_accuracy: 0.5830\n",
      "Epoch 73/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6104 - categorical_accuracy: 0.6658 - val_loss: 1.8124 - val_categorical_accuracy: 0.5670\n",
      "Epoch 74/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.6037 - categorical_accuracy: 0.6687INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 1.6037 - categorical_accuracy: 0.6687 - val_loss: 1.7971 - val_categorical_accuracy: 0.5880\n",
      "Epoch 75/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.6046 - categorical_accuracy: 0.6710 - val_loss: 1.8234 - val_categorical_accuracy: 0.5830\n",
      "Epoch 76/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5908 - categorical_accuracy: 0.6810 - val_loss: 1.8274 - val_categorical_accuracy: 0.5650\n",
      "Epoch 77/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5845 - categorical_accuracy: 0.6765 - val_loss: 1.8203 - val_categorical_accuracy: 0.5800\n",
      "Epoch 78/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5857 - categorical_accuracy: 0.6835 - val_loss: 1.8220 - val_categorical_accuracy: 0.5800\n",
      "Epoch 79/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5817 - categorical_accuracy: 0.6817 - val_loss: 1.7942 - val_categorical_accuracy: 0.5860\n",
      "Epoch 80/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5765 - categorical_accuracy: 0.6905 - val_loss: 1.8197 - val_categorical_accuracy: 0.5790\n",
      "Epoch 81/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5637 - categorical_accuracy: 0.7048 - val_loss: 1.8201 - val_categorical_accuracy: 0.5770\n",
      "Epoch 82/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5620 - categorical_accuracy: 0.7038 - val_loss: 1.8203 - val_categorical_accuracy: 0.5860\n",
      "Epoch 83/100\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.5583 - categorical_accuracy: 0.7051INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 1.5573 - categorical_accuracy: 0.7057 - val_loss: 1.8164 - val_categorical_accuracy: 0.5930\n",
      "Epoch 84/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.5525 - categorical_accuracy: 0.7072 - val_loss: 1.8036 - val_categorical_accuracy: 0.5900\n",
      "Epoch 85/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5526 - categorical_accuracy: 0.7023 - val_loss: 1.8339 - val_categorical_accuracy: 0.5760\n",
      "Epoch 86/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5386 - categorical_accuracy: 0.7103 - val_loss: 1.8749 - val_categorical_accuracy: 0.5560\n",
      "Epoch 87/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5388 - categorical_accuracy: 0.7105 - val_loss: 1.8253 - val_categorical_accuracy: 0.5810\n",
      "Epoch 88/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5337 - categorical_accuracy: 0.7160 - val_loss: 1.8721 - val_categorical_accuracy: 0.5690\n",
      "Epoch 89/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5359 - categorical_accuracy: 0.7188 - val_loss: 1.8741 - val_categorical_accuracy: 0.5600\n",
      "Epoch 90/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5259 - categorical_accuracy: 0.7222 - val_loss: 1.8525 - val_categorical_accuracy: 0.5730\n",
      "Epoch 91/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5276 - categorical_accuracy: 0.7222 - val_loss: 1.8336 - val_categorical_accuracy: 0.5780\n",
      "Epoch 92/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.5213 - categorical_accuracy: 0.7232 - val_loss: 1.8477 - val_categorical_accuracy: 0.5700\n",
      "Epoch 93/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5263 - categorical_accuracy: 0.7220 - val_loss: 1.8536 - val_categorical_accuracy: 0.5790\n",
      "Epoch 94/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.5179 - categorical_accuracy: 0.7300 - val_loss: 1.8477 - val_categorical_accuracy: 0.5830\n",
      "Epoch 95/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5075 - categorical_accuracy: 0.7337 - val_loss: 1.8519 - val_categorical_accuracy: 0.5630\n",
      "Epoch 96/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5105 - categorical_accuracy: 0.7335 - val_loss: 1.8563 - val_categorical_accuracy: 0.5700\n",
      "Epoch 97/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.5074 - categorical_accuracy: 0.7452 - val_loss: 1.8447 - val_categorical_accuracy: 0.5710\n",
      "Epoch 98/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5020 - categorical_accuracy: 0.7380 - val_loss: 1.8290 - val_categorical_accuracy: 0.5840\n",
      "Epoch 99/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5026 - categorical_accuracy: 0.7418 - val_loss: 1.8382 - val_categorical_accuracy: 0.5810\n",
      "Epoch 100/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5383 - categorical_accuracy: 0.7180 - val_loss: 1.8332 - val_categorical_accuracy: 0.5780\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁▂▂▃▄▄▄▄▅▅▅▆▆▆▆▆▆▆▆▇▆▆▆▆▇▇▇▇▇▇▇▇████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁███▇▇▇▆▆▅▅▄▄▃▃▂▂▁▁▁█</td></tr><tr><td>epoch/loss</td><td>█▇▆▆▅▅▅▄▄▄▃▃▃▃▃▃▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁▂▃▃▄▅▅▅▆▆▇▇▇▇██▇▇█████▇███████████▇████</td></tr><tr><td>epoch/val_loss</td><td>█▇▆▅▄▄▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▂▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.718</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>1.53828</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.578</td></tr><tr><td>epoch/val_loss</td><td>1.83316</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/o25m0mz7' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/o25m0mz7</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240416_144046-o25m0mz7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f9500ab0710>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dim = 64  # Embedding size for each token\n",
    "num_heads = 4  # Number of attention heads\n",
    "ff_dim = 64  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = layers.Input(shape=X_shape)\n",
    "embedding_layer = TokenAndPositionEmbedding(X_shape[0], X_shape[1])\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(X_shape[1], num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = Dense(len(train_dataset_parquet.unique_labels), activation='softmax')(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf\"),\n",
    "            run_name=\"Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k\",\n",
    "            mediapipe_features=\"reduced-LipsEyesHandsPose\",\n",
    "            USE_WANDB=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:k5c6maot) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁</td></tr><tr><td>epoch/epoch</td><td>▁</td></tr><tr><td>epoch/learning_rate</td><td>▁</td></tr><tr><td>epoch/loss</td><td>▁</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁</td></tr><tr><td>epoch/val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.121</td></tr><tr><td>epoch/epoch</td><td>0</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>2.5331</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.206</td></tr><tr><td>epoch/val_loss</td><td>2.42635</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/k5c6maot' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/k5c6maot</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240416_150234-k5c6maot/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:k5c6maot). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a23036c3b44d3cbdea0da41ee22146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112549859616492, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240416_150354-tqe9uruf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/tqe9uruf' target=\"_blank\">Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/tqe9uruf' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/tqe9uruf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "    497/Unknown - 10s 11ms/step - loss: 2.5373 - categorical_accuracy: 0.1031"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 15:04:11.861998: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 5374993014047653291\n",
      "2024-04-16 15:04:11.862066: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8438867370503118669\n",
      "2024-04-16 15:04:11.862087: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7325566132977353672\n",
      "2024-04-16 15:04:12.853987: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 12812880993351260521\n",
      "2024-04-16 15:04:12.854041: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 534613766666025935\n",
      "2024-04-16 15:04:12.854053: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 516448505323512496\n",
      "2024-04-16 15:04:12.854060: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 13327268128286139334\n",
      "2024-04-16 15:04:12.854088: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7726172902863430400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 16s 23ms/step - loss: 2.5371 - categorical_accuracy: 0.1037 - val_loss: 2.4879 - val_categorical_accuracy: 0.1490\n",
      "Epoch 2/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.4198 - categorical_accuracy: 0.1652INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 2.4199 - categorical_accuracy: 0.1653 - val_loss: 2.3782 - val_categorical_accuracy: 0.1880\n",
      "Epoch 3/300\n",
      "496/500 [============================>.] - ETA: 0s - loss: 2.3815 - categorical_accuracy: 0.1875INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 22ms/step - loss: 2.3809 - categorical_accuracy: 0.1880 - val_loss: 2.3528 - val_categorical_accuracy: 0.2010\n",
      "Epoch 4/300\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.3464 - categorical_accuracy: 0.2015INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 19ms/step - loss: 2.3464 - categorical_accuracy: 0.2015 - val_loss: 2.3278 - val_categorical_accuracy: 0.2190\n",
      "Epoch 5/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.3176 - categorical_accuracy: 0.2188INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 2.3173 - categorical_accuracy: 0.2188 - val_loss: 2.2920 - val_categorical_accuracy: 0.2280\n",
      "Epoch 6/300\n",
      "498/500 [============================>.] - ETA: 0s - loss: 2.2915 - categorical_accuracy: 0.2324INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 2.2909 - categorical_accuracy: 0.2327 - val_loss: 2.2780 - val_categorical_accuracy: 0.2410\n",
      "Epoch 7/300\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.2636 - categorical_accuracy: 0.2525INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 22ms/step - loss: 2.2636 - categorical_accuracy: 0.2525 - val_loss: 2.2664 - val_categorical_accuracy: 0.2580\n",
      "Epoch 8/300\n",
      "498/500 [============================>.] - ETA: 0s - loss: 2.2438 - categorical_accuracy: 0.2676INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 22ms/step - loss: 2.2428 - categorical_accuracy: 0.2680 - val_loss: 2.2554 - val_categorical_accuracy: 0.2810\n",
      "Epoch 9/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.2222 - categorical_accuracy: 0.2782INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 19ms/step - loss: 2.2205 - categorical_accuracy: 0.2795 - val_loss: 2.2389 - val_categorical_accuracy: 0.2870\n",
      "Epoch 10/300\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.1982 - categorical_accuracy: 0.2955INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 22ms/step - loss: 2.1982 - categorical_accuracy: 0.2955 - val_loss: 2.2161 - val_categorical_accuracy: 0.2880\n",
      "Epoch 11/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.1808 - categorical_accuracy: 0.2995INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 19ms/step - loss: 2.1791 - categorical_accuracy: 0.3010 - val_loss: 2.1700 - val_categorical_accuracy: 0.3140\n",
      "Epoch 12/300\n",
      "496/500 [============================>.] - ETA: 0s - loss: 2.1617 - categorical_accuracy: 0.3304INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 22ms/step - loss: 2.1594 - categorical_accuracy: 0.3318 - val_loss: 2.1288 - val_categorical_accuracy: 0.3360\n",
      "Epoch 13/300\n",
      "498/500 [============================>.] - ETA: 0s - loss: 2.1319 - categorical_accuracy: 0.3386INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 22ms/step - loss: 2.1306 - categorical_accuracy: 0.3390 - val_loss: 2.1218 - val_categorical_accuracy: 0.3410\n",
      "Epoch 14/300\n",
      "498/500 [============================>.] - ETA: 0s - loss: 2.1130 - categorical_accuracy: 0.3509INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 19ms/step - loss: 2.1116 - categorical_accuracy: 0.3512 - val_loss: 2.1239 - val_categorical_accuracy: 0.3550\n",
      "Epoch 15/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.0892 - categorical_accuracy: 0.3607INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 22ms/step - loss: 2.0869 - categorical_accuracy: 0.3625 - val_loss: 2.1136 - val_categorical_accuracy: 0.3800\n",
      "Epoch 16/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.0593 - categorical_accuracy: 0.3919INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 19ms/step - loss: 2.0576 - categorical_accuracy: 0.3930 - val_loss: 2.0695 - val_categorical_accuracy: 0.4000\n",
      "Epoch 17/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 2.0388 - categorical_accuracy: 0.3880 - val_loss: 2.1097 - val_categorical_accuracy: 0.3890\n",
      "Epoch 18/300\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.0221 - categorical_accuracy: 0.4085INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 2.0221 - categorical_accuracy: 0.4085 - val_loss: 2.0939 - val_categorical_accuracy: 0.4100\n",
      "Epoch 19/300\n",
      "498/500 [============================>.] - ETA: 0s - loss: 2.0007 - categorical_accuracy: 0.4212INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 19ms/step - loss: 1.9995 - categorical_accuracy: 0.4215 - val_loss: 2.0792 - val_categorical_accuracy: 0.4150\n",
      "Epoch 20/300\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 1.9972 - categorical_accuracy: 0.4218 - val_loss: 2.0787 - val_categorical_accuracy: 0.4070\n",
      "Epoch 21/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.9722 - categorical_accuracy: 0.4372 - val_loss: 2.0706 - val_categorical_accuracy: 0.4070\n",
      "Epoch 22/300\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.9489 - categorical_accuracy: 0.4558INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 22ms/step - loss: 1.9482 - categorical_accuracy: 0.4557 - val_loss: 2.0420 - val_categorical_accuracy: 0.4280\n",
      "Epoch 23/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.9357 - categorical_accuracy: 0.4620 - val_loss: 2.0453 - val_categorical_accuracy: 0.4220\n",
      "Epoch 24/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.9158 - categorical_accuracy: 0.4700 - val_loss: 2.0506 - val_categorical_accuracy: 0.4220\n",
      "Epoch 25/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.9124 - categorical_accuracy: 0.4780 - val_loss: 2.0527 - val_categorical_accuracy: 0.4270\n",
      "Epoch 26/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.9023 - categorical_accuracy: 0.4733 - val_loss: 2.0364 - val_categorical_accuracy: 0.4270\n",
      "Epoch 27/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.8808 - categorical_accuracy: 0.4938 - val_loss: 2.0443 - val_categorical_accuracy: 0.4230\n",
      "Epoch 28/300\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.8679 - categorical_accuracy: 0.5013INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 1.8669 - categorical_accuracy: 0.5020 - val_loss: 2.0141 - val_categorical_accuracy: 0.4310\n",
      "Epoch 29/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.8450 - categorical_accuracy: 0.5125 - val_loss: 2.0509 - val_categorical_accuracy: 0.4230\n",
      "Epoch 30/300\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.8551 - categorical_accuracy: 0.5040INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 19ms/step - loss: 1.8535 - categorical_accuracy: 0.5058 - val_loss: 2.0087 - val_categorical_accuracy: 0.4380\n",
      "Epoch 31/300\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.8239 - categorical_accuracy: 0.5261INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 22ms/step - loss: 1.8227 - categorical_accuracy: 0.5272 - val_loss: 2.0286 - val_categorical_accuracy: 0.4430\n",
      "Epoch 32/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.8051 - categorical_accuracy: 0.5403 - val_loss: 2.0569 - val_categorical_accuracy: 0.4310\n",
      "Epoch 33/300\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.7920 - categorical_accuracy: 0.5508INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 19ms/step - loss: 1.7920 - categorical_accuracy: 0.5508 - val_loss: 2.0022 - val_categorical_accuracy: 0.4530\n",
      "Epoch 34/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7777 - categorical_accuracy: 0.5548 - val_loss: 2.0407 - val_categorical_accuracy: 0.4220\n",
      "Epoch 35/300\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.7624 - categorical_accuracy: 0.5693INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 22ms/step - loss: 1.7618 - categorical_accuracy: 0.5700 - val_loss: 2.0034 - val_categorical_accuracy: 0.4650\n",
      "Epoch 36/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.7542 - categorical_accuracy: 0.5773 - val_loss: 2.0069 - val_categorical_accuracy: 0.4480\n",
      "Epoch 37/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.7416 - categorical_accuracy: 0.5853 - val_loss: 1.9936 - val_categorical_accuracy: 0.4580\n",
      "Epoch 38/300\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.7222 - categorical_accuracy: 0.5956INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 23ms/step - loss: 1.7209 - categorical_accuracy: 0.5970 - val_loss: 1.9826 - val_categorical_accuracy: 0.4730\n",
      "Epoch 39/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7116 - categorical_accuracy: 0.5925 - val_loss: 1.9828 - val_categorical_accuracy: 0.4630\n",
      "Epoch 40/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6992 - categorical_accuracy: 0.6022 - val_loss: 1.9956 - val_categorical_accuracy: 0.4560\n",
      "Epoch 41/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.6924 - categorical_accuracy: 0.6183 - val_loss: 2.0027 - val_categorical_accuracy: 0.4730\n",
      "Epoch 42/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 1.6864 - categorical_accuracy: 0.6164INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 19ms/step - loss: 1.6852 - categorical_accuracy: 0.6177 - val_loss: 1.9843 - val_categorical_accuracy: 0.4760\n",
      "Epoch 43/300\n",
      "499/500 [============================>.] - ETA: 0s - loss: 1.6653 - categorical_accuracy: 0.6250INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 22ms/step - loss: 1.6645 - categorical_accuracy: 0.6255 - val_loss: 1.9784 - val_categorical_accuracy: 0.4860\n",
      "Epoch 44/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6696 - categorical_accuracy: 0.6263 - val_loss: 2.0002 - val_categorical_accuracy: 0.4710\n",
      "Epoch 45/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.6499 - categorical_accuracy: 0.6313 - val_loss: 1.9902 - val_categorical_accuracy: 0.4740\n",
      "Epoch 46/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6291 - categorical_accuracy: 0.6472 - val_loss: 1.9902 - val_categorical_accuracy: 0.4810\n",
      "Epoch 47/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6312 - categorical_accuracy: 0.6500 - val_loss: 1.9858 - val_categorical_accuracy: 0.4840\n",
      "Epoch 48/300\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.6203 - categorical_accuracy: 0.6532INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 19ms/step - loss: 1.6203 - categorical_accuracy: 0.6532 - val_loss: 2.0085 - val_categorical_accuracy: 0.4930\n",
      "Epoch 49/300\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 1.6146 - categorical_accuracy: 0.6605 - val_loss: 1.9846 - val_categorical_accuracy: 0.4920\n",
      "Epoch 50/300\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 1.5992 - categorical_accuracy: 0.6658 - val_loss: 2.0185 - val_categorical_accuracy: 0.4640\n",
      "Epoch 51/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.5935 - categorical_accuracy: 0.6720 - val_loss: 2.0112 - val_categorical_accuracy: 0.4710\n",
      "Epoch 52/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.5770 - categorical_accuracy: 0.6827 - val_loss: 2.0080 - val_categorical_accuracy: 0.4820\n",
      "Epoch 53/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.5738 - categorical_accuracy: 0.6855 - val_loss: 2.0038 - val_categorical_accuracy: 0.4900\n",
      "Epoch 54/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.5654 - categorical_accuracy: 0.6923 - val_loss: 2.0222 - val_categorical_accuracy: 0.4730\n",
      "Epoch 55/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.5621 - categorical_accuracy: 0.6982 - val_loss: 2.0401 - val_categorical_accuracy: 0.4740\n",
      "Epoch 56/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.5556 - categorical_accuracy: 0.6873 - val_loss: 2.0995 - val_categorical_accuracy: 0.4650\n",
      "Epoch 57/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.5395 - categorical_accuracy: 0.6980 - val_loss: 2.0595 - val_categorical_accuracy: 0.4770\n",
      "Epoch 58/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5349 - categorical_accuracy: 0.7020 - val_loss: 2.0905 - val_categorical_accuracy: 0.4530\n",
      "Epoch 59/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.5295 - categorical_accuracy: 0.7153 - val_loss: 2.0684 - val_categorical_accuracy: 0.4660\n",
      "Epoch 60/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5143 - categorical_accuracy: 0.7300 - val_loss: 2.0901 - val_categorical_accuracy: 0.4700\n",
      "Epoch 61/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5109 - categorical_accuracy: 0.7247 - val_loss: 2.0713 - val_categorical_accuracy: 0.4670\n",
      "Epoch 62/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.5064 - categorical_accuracy: 0.7220 - val_loss: 2.1147 - val_categorical_accuracy: 0.4390\n",
      "Epoch 63/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4923 - categorical_accuracy: 0.7335 - val_loss: 2.0633 - val_categorical_accuracy: 0.4840\n",
      "Epoch 64/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4817 - categorical_accuracy: 0.7435 - val_loss: 2.0833 - val_categorical_accuracy: 0.4790\n",
      "Epoch 65/300\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.4818 - categorical_accuracy: 0.7409INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 22ms/step - loss: 1.4811 - categorical_accuracy: 0.7415 - val_loss: 2.0877 - val_categorical_accuracy: 0.4950\n",
      "Epoch 66/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.4719 - categorical_accuracy: 0.7495 - val_loss: 2.0938 - val_categorical_accuracy: 0.4880\n",
      "Epoch 67/300\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 1.4577 - categorical_accuracy: 0.7535 - val_loss: 2.0786 - val_categorical_accuracy: 0.4880\n",
      "Epoch 68/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4655 - categorical_accuracy: 0.7495 - val_loss: 2.0968 - val_categorical_accuracy: 0.4770\n",
      "Epoch 69/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4440 - categorical_accuracy: 0.7655 - val_loss: 2.0839 - val_categorical_accuracy: 0.4820\n",
      "Epoch 70/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4599 - categorical_accuracy: 0.7580 - val_loss: 2.1008 - val_categorical_accuracy: 0.4810\n",
      "Epoch 71/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4412 - categorical_accuracy: 0.7667 - val_loss: 2.1198 - val_categorical_accuracy: 0.4820\n",
      "Epoch 72/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4322 - categorical_accuracy: 0.7700 - val_loss: 2.1085 - val_categorical_accuracy: 0.4900\n",
      "Epoch 73/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4294 - categorical_accuracy: 0.7728 - val_loss: 2.1103 - val_categorical_accuracy: 0.4800\n",
      "Epoch 74/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.4212 - categorical_accuracy: 0.7807 - val_loss: 2.0919 - val_categorical_accuracy: 0.4880\n",
      "Epoch 75/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4191 - categorical_accuracy: 0.7850 - val_loss: 2.1003 - val_categorical_accuracy: 0.4790\n",
      "Epoch 76/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4105 - categorical_accuracy: 0.7883 - val_loss: 2.1186 - val_categorical_accuracy: 0.4730\n",
      "Epoch 77/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.4168 - categorical_accuracy: 0.7790 - val_loss: 2.1199 - val_categorical_accuracy: 0.4750\n",
      "Epoch 78/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.4045 - categorical_accuracy: 0.7943 - val_loss: 2.1378 - val_categorical_accuracy: 0.4670\n",
      "Epoch 79/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.4093 - categorical_accuracy: 0.7845 - val_loss: 2.1418 - val_categorical_accuracy: 0.4650\n",
      "Epoch 80/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3902 - categorical_accuracy: 0.8015 - val_loss: 2.1049 - val_categorical_accuracy: 0.4780\n",
      "Epoch 81/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3825 - categorical_accuracy: 0.8020 - val_loss: 2.1366 - val_categorical_accuracy: 0.4690\n",
      "Epoch 82/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3948 - categorical_accuracy: 0.7983 - val_loss: 2.1153 - val_categorical_accuracy: 0.4780\n",
      "Epoch 83/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.3811 - categorical_accuracy: 0.8008 - val_loss: 2.1055 - val_categorical_accuracy: 0.4740\n",
      "Epoch 84/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3745 - categorical_accuracy: 0.8125 - val_loss: 2.0952 - val_categorical_accuracy: 0.4790\n",
      "Epoch 85/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.3676 - categorical_accuracy: 0.8150 - val_loss: 2.1104 - val_categorical_accuracy: 0.4760\n",
      "Epoch 86/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3654 - categorical_accuracy: 0.8183 - val_loss: 2.1094 - val_categorical_accuracy: 0.4880\n",
      "Epoch 87/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.3624 - categorical_accuracy: 0.8170 - val_loss: 2.1524 - val_categorical_accuracy: 0.4630\n",
      "Epoch 88/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.3522 - categorical_accuracy: 0.8232 - val_loss: 2.1183 - val_categorical_accuracy: 0.4920\n",
      "Epoch 89/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.3461 - categorical_accuracy: 0.8298 - val_loss: 2.1110 - val_categorical_accuracy: 0.4950\n",
      "Epoch 90/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3369 - categorical_accuracy: 0.8303 - val_loss: 2.1202 - val_categorical_accuracy: 0.4890\n",
      "Epoch 91/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3362 - categorical_accuracy: 0.8288 - val_loss: 2.1184 - val_categorical_accuracy: 0.4920\n",
      "Epoch 92/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3394 - categorical_accuracy: 0.8270 - val_loss: 2.1230 - val_categorical_accuracy: 0.4870\n",
      "Epoch 93/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.3364 - categorical_accuracy: 0.8325 - val_loss: 2.1120 - val_categorical_accuracy: 0.4920\n",
      "Epoch 94/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3330 - categorical_accuracy: 0.8353 - val_loss: 2.1151 - val_categorical_accuracy: 0.4800\n",
      "Epoch 95/300\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.3364 - categorical_accuracy: 0.8332INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 19ms/step - loss: 1.3359 - categorical_accuracy: 0.8335 - val_loss: 2.1024 - val_categorical_accuracy: 0.5000\n",
      "Epoch 96/300\n",
      "499/500 [============================>.] - ETA: 0s - loss: 1.3206 - categorical_accuracy: 0.8404INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 1.3202 - categorical_accuracy: 0.8407 - val_loss: 2.0999 - val_categorical_accuracy: 0.5110\n",
      "Epoch 97/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3237 - categorical_accuracy: 0.8410 - val_loss: 2.1189 - val_categorical_accuracy: 0.4990\n",
      "Epoch 98/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3229 - categorical_accuracy: 0.8465 - val_loss: 2.1259 - val_categorical_accuracy: 0.4970\n",
      "Epoch 99/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3091 - categorical_accuracy: 0.8485 - val_loss: 2.1368 - val_categorical_accuracy: 0.4970\n",
      "Epoch 100/300\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.3152 - categorical_accuracy: 0.8432INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 22ms/step - loss: 1.3143 - categorical_accuracy: 0.8440 - val_loss: 2.0867 - val_categorical_accuracy: 0.5150\n",
      "Epoch 101/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3104 - categorical_accuracy: 0.8493 - val_loss: 2.1164 - val_categorical_accuracy: 0.5080\n",
      "Epoch 102/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.3005 - categorical_accuracy: 0.8593 - val_loss: 2.0950 - val_categorical_accuracy: 0.5130\n",
      "Epoch 103/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3082 - categorical_accuracy: 0.8497 - val_loss: 2.0843 - val_categorical_accuracy: 0.5150\n",
      "Epoch 104/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2948 - categorical_accuracy: 0.8565 - val_loss: 2.0945 - val_categorical_accuracy: 0.5010\n",
      "Epoch 105/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2908 - categorical_accuracy: 0.8615 - val_loss: 2.0845 - val_categorical_accuracy: 0.5040\n",
      "Epoch 106/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2911 - categorical_accuracy: 0.8568 - val_loss: 2.0903 - val_categorical_accuracy: 0.5120\n",
      "Epoch 107/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2848 - categorical_accuracy: 0.8692 - val_loss: 2.1327 - val_categorical_accuracy: 0.4980\n",
      "Epoch 108/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2899 - categorical_accuracy: 0.8630 - val_loss: 2.1197 - val_categorical_accuracy: 0.5090\n",
      "Epoch 109/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2784 - categorical_accuracy: 0.8648 - val_loss: 2.1057 - val_categorical_accuracy: 0.5020\n",
      "Epoch 110/300\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.2874 - categorical_accuracy: 0.8650INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 1.2868 - categorical_accuracy: 0.8652 - val_loss: 2.1008 - val_categorical_accuracy: 0.5190\n",
      "Epoch 111/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2693 - categorical_accuracy: 0.8737 - val_loss: 2.0831 - val_categorical_accuracy: 0.5130\n",
      "Epoch 112/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2666 - categorical_accuracy: 0.8775 - val_loss: 2.1133 - val_categorical_accuracy: 0.4990\n",
      "Epoch 113/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2684 - categorical_accuracy: 0.8745 - val_loss: 2.0882 - val_categorical_accuracy: 0.5180\n",
      "Epoch 114/300\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 1.2698 - categorical_accuracy: 0.8715 - val_loss: 2.0992 - val_categorical_accuracy: 0.5070\n",
      "Epoch 115/300\n",
      "499/500 [============================>.] - ETA: 0s - loss: 1.2624 - categorical_accuracy: 0.8790INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 22ms/step - loss: 1.2621 - categorical_accuracy: 0.8792 - val_loss: 2.0957 - val_categorical_accuracy: 0.5220\n",
      "Epoch 116/300\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.2596 - categorical_accuracy: 0.8788INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 19ms/step - loss: 1.2594 - categorical_accuracy: 0.8790 - val_loss: 2.0835 - val_categorical_accuracy: 0.5270\n",
      "Epoch 117/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2587 - categorical_accuracy: 0.8805 - val_loss: 2.0938 - val_categorical_accuracy: 0.5240\n",
      "Epoch 118/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2558 - categorical_accuracy: 0.8823 - val_loss: 2.0946 - val_categorical_accuracy: 0.5200\n",
      "Epoch 119/300\n",
      "499/500 [============================>.] - ETA: 0s - loss: 1.2558 - categorical_accuracy: 0.8780INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 22ms/step - loss: 1.2556 - categorical_accuracy: 0.8783 - val_loss: 2.0817 - val_categorical_accuracy: 0.5280\n",
      "Epoch 120/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2511 - categorical_accuracy: 0.8790 - val_loss: 2.0972 - val_categorical_accuracy: 0.5200\n",
      "Epoch 121/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 1.2557 - categorical_accuracy: 0.8793INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 1.2550 - categorical_accuracy: 0.8798 - val_loss: 2.0880 - val_categorical_accuracy: 0.5360\n",
      "Epoch 122/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2465 - categorical_accuracy: 0.8867 - val_loss: 2.0852 - val_categorical_accuracy: 0.5210\n",
      "Epoch 123/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2418 - categorical_accuracy: 0.8917 - val_loss: 2.0874 - val_categorical_accuracy: 0.5170\n",
      "Epoch 124/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2500 - categorical_accuracy: 0.8820 - val_loss: 2.0782 - val_categorical_accuracy: 0.5130\n",
      "Epoch 125/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2350 - categorical_accuracy: 0.8957 - val_loss: 2.0869 - val_categorical_accuracy: 0.5280\n",
      "Epoch 126/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2437 - categorical_accuracy: 0.8870 - val_loss: 2.0725 - val_categorical_accuracy: 0.5230\n",
      "Epoch 127/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2399 - categorical_accuracy: 0.8955 - val_loss: 2.0771 - val_categorical_accuracy: 0.5280\n",
      "Epoch 128/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2317 - categorical_accuracy: 0.8932 - val_loss: 2.0736 - val_categorical_accuracy: 0.5210\n",
      "Epoch 129/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2279 - categorical_accuracy: 0.8970 - val_loss: 2.0895 - val_categorical_accuracy: 0.5210\n",
      "Epoch 130/300\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.2315 - categorical_accuracy: 0.8924INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 19ms/step - loss: 1.2315 - categorical_accuracy: 0.8925 - val_loss: 2.0840 - val_categorical_accuracy: 0.5390\n",
      "Epoch 131/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2291 - categorical_accuracy: 0.8907 - val_loss: 2.0585 - val_categorical_accuracy: 0.5320\n",
      "Epoch 132/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2237 - categorical_accuracy: 0.8975 - val_loss: 2.0736 - val_categorical_accuracy: 0.5310\n",
      "Epoch 133/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2241 - categorical_accuracy: 0.8992 - val_loss: 2.0737 - val_categorical_accuracy: 0.5230\n",
      "Epoch 134/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2258 - categorical_accuracy: 0.8970 - val_loss: 2.0748 - val_categorical_accuracy: 0.5250\n",
      "Epoch 135/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2151 - categorical_accuracy: 0.9070 - val_loss: 2.0834 - val_categorical_accuracy: 0.5250\n",
      "Epoch 136/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2171 - categorical_accuracy: 0.9078 - val_loss: 2.0779 - val_categorical_accuracy: 0.5270\n",
      "Epoch 137/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2126 - categorical_accuracy: 0.9078 - val_loss: 2.0565 - val_categorical_accuracy: 0.5300\n",
      "Epoch 138/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2070 - categorical_accuracy: 0.9135 - val_loss: 2.0681 - val_categorical_accuracy: 0.5240\n",
      "Epoch 139/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2159 - categorical_accuracy: 0.9060 - val_loss: 2.0707 - val_categorical_accuracy: 0.5300\n",
      "Epoch 140/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2123 - categorical_accuracy: 0.9080 - val_loss: 2.0650 - val_categorical_accuracy: 0.5230\n",
      "Epoch 141/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2113 - categorical_accuracy: 0.9135 - val_loss: 2.0682 - val_categorical_accuracy: 0.5380\n",
      "Epoch 142/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2055 - categorical_accuracy: 0.9093 - val_loss: 2.0842 - val_categorical_accuracy: 0.5280\n",
      "Epoch 143/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2002 - categorical_accuracy: 0.9183 - val_loss: 2.0708 - val_categorical_accuracy: 0.5260\n",
      "Epoch 144/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2063 - categorical_accuracy: 0.9145 - val_loss: 2.0711 - val_categorical_accuracy: 0.5310\n",
      "Epoch 145/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1832 - categorical_accuracy: 0.9283 - val_loss: 2.0665 - val_categorical_accuracy: 0.5330\n",
      "Epoch 146/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2004 - categorical_accuracy: 0.9107 - val_loss: 2.0896 - val_categorical_accuracy: 0.5260\n",
      "Epoch 147/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1984 - categorical_accuracy: 0.9135 - val_loss: 2.0673 - val_categorical_accuracy: 0.5370\n",
      "Epoch 148/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2018 - categorical_accuracy: 0.9115 - val_loss: 2.0653 - val_categorical_accuracy: 0.5280\n",
      "Epoch 149/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1884 - categorical_accuracy: 0.9243 - val_loss: 2.0773 - val_categorical_accuracy: 0.5270\n",
      "Epoch 150/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1957 - categorical_accuracy: 0.9185 - val_loss: 2.0549 - val_categorical_accuracy: 0.5390\n",
      "Epoch 151/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.4074 - categorical_accuracy: 0.7872 - val_loss: 2.1428 - val_categorical_accuracy: 0.5050\n",
      "Epoch 152/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.4069 - categorical_accuracy: 0.7900 - val_loss: 2.1898 - val_categorical_accuracy: 0.4870\n",
      "Epoch 153/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.3839 - categorical_accuracy: 0.8040 - val_loss: 2.1429 - val_categorical_accuracy: 0.5010\n",
      "Epoch 154/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3726 - categorical_accuracy: 0.8083 - val_loss: 2.1432 - val_categorical_accuracy: 0.4980\n",
      "Epoch 155/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3710 - categorical_accuracy: 0.8035 - val_loss: 2.1530 - val_categorical_accuracy: 0.4830\n",
      "Epoch 156/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3746 - categorical_accuracy: 0.8040 - val_loss: 2.1386 - val_categorical_accuracy: 0.5050\n",
      "Epoch 157/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3503 - categorical_accuracy: 0.8207 - val_loss: 2.1388 - val_categorical_accuracy: 0.5010\n",
      "Epoch 158/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3454 - categorical_accuracy: 0.8205 - val_loss: 2.1039 - val_categorical_accuracy: 0.5130\n",
      "Epoch 159/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3497 - categorical_accuracy: 0.8220 - val_loss: 2.1116 - val_categorical_accuracy: 0.4990\n",
      "Epoch 160/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3398 - categorical_accuracy: 0.8213 - val_loss: 2.1432 - val_categorical_accuracy: 0.4830\n",
      "Epoch 161/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3402 - categorical_accuracy: 0.8288 - val_loss: 2.1161 - val_categorical_accuracy: 0.5000\n",
      "Epoch 162/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3489 - categorical_accuracy: 0.8235 - val_loss: 2.1537 - val_categorical_accuracy: 0.4710\n",
      "Epoch 163/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3275 - categorical_accuracy: 0.8307 - val_loss: 2.1453 - val_categorical_accuracy: 0.4970\n",
      "Epoch 164/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3177 - categorical_accuracy: 0.8418 - val_loss: 2.1400 - val_categorical_accuracy: 0.4890\n",
      "Epoch 165/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3183 - categorical_accuracy: 0.8393 - val_loss: 2.1412 - val_categorical_accuracy: 0.4780\n",
      "Epoch 166/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3254 - categorical_accuracy: 0.8345 - val_loss: 2.1491 - val_categorical_accuracy: 0.4940\n",
      "Epoch 167/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3125 - categorical_accuracy: 0.8393 - val_loss: 2.1401 - val_categorical_accuracy: 0.5030\n",
      "Epoch 168/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3066 - categorical_accuracy: 0.8500 - val_loss: 2.1626 - val_categorical_accuracy: 0.4920\n",
      "Epoch 169/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3018 - categorical_accuracy: 0.8505 - val_loss: 2.1543 - val_categorical_accuracy: 0.5080\n",
      "Epoch 170/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2928 - categorical_accuracy: 0.8560 - val_loss: 2.1229 - val_categorical_accuracy: 0.5050\n",
      "Epoch 171/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2884 - categorical_accuracy: 0.8608 - val_loss: 2.1345 - val_categorical_accuracy: 0.5090\n",
      "Epoch 172/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2935 - categorical_accuracy: 0.8565 - val_loss: 2.1412 - val_categorical_accuracy: 0.4990\n",
      "Epoch 173/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2877 - categorical_accuracy: 0.8618 - val_loss: 2.1976 - val_categorical_accuracy: 0.4720\n",
      "Epoch 174/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2842 - categorical_accuracy: 0.8600 - val_loss: 2.1223 - val_categorical_accuracy: 0.5020\n",
      "Epoch 175/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2755 - categorical_accuracy: 0.8680 - val_loss: 2.0781 - val_categorical_accuracy: 0.4940\n",
      "Epoch 176/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2801 - categorical_accuracy: 0.8665 - val_loss: 2.1420 - val_categorical_accuracy: 0.4940\n",
      "Epoch 177/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2659 - categorical_accuracy: 0.8668 - val_loss: 2.1978 - val_categorical_accuracy: 0.4760\n",
      "Epoch 178/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2689 - categorical_accuracy: 0.8675 - val_loss: 2.1481 - val_categorical_accuracy: 0.4850\n",
      "Epoch 179/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2624 - categorical_accuracy: 0.8760 - val_loss: 2.1343 - val_categorical_accuracy: 0.5000\n",
      "Epoch 180/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2618 - categorical_accuracy: 0.8783 - val_loss: 2.1984 - val_categorical_accuracy: 0.4730\n",
      "Epoch 181/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2593 - categorical_accuracy: 0.8777 - val_loss: 2.1342 - val_categorical_accuracy: 0.4930\n",
      "Epoch 182/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2588 - categorical_accuracy: 0.8750 - val_loss: 2.1142 - val_categorical_accuracy: 0.4950\n",
      "Epoch 183/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2492 - categorical_accuracy: 0.8813 - val_loss: 2.1321 - val_categorical_accuracy: 0.4960\n",
      "Epoch 184/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2403 - categorical_accuracy: 0.8907 - val_loss: 2.0977 - val_categorical_accuracy: 0.5110\n",
      "Epoch 185/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2400 - categorical_accuracy: 0.8848 - val_loss: 2.1417 - val_categorical_accuracy: 0.4930\n",
      "Epoch 186/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2530 - categorical_accuracy: 0.8798 - val_loss: 2.1025 - val_categorical_accuracy: 0.5040\n",
      "Epoch 187/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2363 - categorical_accuracy: 0.8895 - val_loss: 2.1243 - val_categorical_accuracy: 0.5040\n",
      "Epoch 188/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2420 - categorical_accuracy: 0.8892 - val_loss: 2.1681 - val_categorical_accuracy: 0.4920\n",
      "Epoch 189/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2311 - categorical_accuracy: 0.8895 - val_loss: 2.1365 - val_categorical_accuracy: 0.4830\n",
      "Epoch 190/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2293 - categorical_accuracy: 0.8915 - val_loss: 2.1662 - val_categorical_accuracy: 0.4660\n",
      "Epoch 191/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2142 - categorical_accuracy: 0.9057 - val_loss: 2.1328 - val_categorical_accuracy: 0.5010\n",
      "Epoch 192/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2257 - categorical_accuracy: 0.8978 - val_loss: 2.1244 - val_categorical_accuracy: 0.5000\n",
      "Epoch 193/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2138 - categorical_accuracy: 0.9025 - val_loss: 2.1173 - val_categorical_accuracy: 0.5010\n",
      "Epoch 194/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2098 - categorical_accuracy: 0.9003 - val_loss: 2.1752 - val_categorical_accuracy: 0.4990\n",
      "Epoch 195/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2135 - categorical_accuracy: 0.9032 - val_loss: 2.1086 - val_categorical_accuracy: 0.5170\n",
      "Epoch 196/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2029 - categorical_accuracy: 0.9070 - val_loss: 2.1442 - val_categorical_accuracy: 0.5010\n",
      "Epoch 197/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2141 - categorical_accuracy: 0.9045 - val_loss: 2.1613 - val_categorical_accuracy: 0.4850\n",
      "Epoch 198/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2108 - categorical_accuracy: 0.9050 - val_loss: 2.1157 - val_categorical_accuracy: 0.5010\n",
      "Epoch 199/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2005 - categorical_accuracy: 0.9112 - val_loss: 2.1409 - val_categorical_accuracy: 0.4880\n",
      "Epoch 200/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2066 - categorical_accuracy: 0.9055 - val_loss: 2.1208 - val_categorical_accuracy: 0.5010\n",
      "Epoch 201/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1997 - categorical_accuracy: 0.9135 - val_loss: 2.1153 - val_categorical_accuracy: 0.4910\n",
      "Epoch 202/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1919 - categorical_accuracy: 0.9155 - val_loss: 2.1521 - val_categorical_accuracy: 0.4810\n",
      "Epoch 203/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1901 - categorical_accuracy: 0.9145 - val_loss: 2.1466 - val_categorical_accuracy: 0.4780\n",
      "Epoch 204/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1859 - categorical_accuracy: 0.9202 - val_loss: 2.1283 - val_categorical_accuracy: 0.4970\n",
      "Epoch 205/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1766 - categorical_accuracy: 0.9260 - val_loss: 2.1102 - val_categorical_accuracy: 0.5050\n",
      "Epoch 206/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1795 - categorical_accuracy: 0.9230 - val_loss: 2.1400 - val_categorical_accuracy: 0.4910\n",
      "Epoch 207/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1750 - categorical_accuracy: 0.9265 - val_loss: 2.1428 - val_categorical_accuracy: 0.4890\n",
      "Epoch 208/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1761 - categorical_accuracy: 0.9227 - val_loss: 2.1508 - val_categorical_accuracy: 0.5130\n",
      "Epoch 209/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1709 - categorical_accuracy: 0.9285 - val_loss: 2.1467 - val_categorical_accuracy: 0.4900\n",
      "Epoch 210/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1640 - categorical_accuracy: 0.9330 - val_loss: 2.1661 - val_categorical_accuracy: 0.4770\n",
      "Epoch 211/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1700 - categorical_accuracy: 0.9245 - val_loss: 2.1216 - val_categorical_accuracy: 0.5010\n",
      "Epoch 212/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1629 - categorical_accuracy: 0.9388 - val_loss: 2.1515 - val_categorical_accuracy: 0.4950\n",
      "Epoch 213/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1655 - categorical_accuracy: 0.9327 - val_loss: 2.0878 - val_categorical_accuracy: 0.5250\n",
      "Epoch 214/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1546 - categorical_accuracy: 0.9352 - val_loss: 2.0961 - val_categorical_accuracy: 0.5230\n",
      "Epoch 215/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1539 - categorical_accuracy: 0.9360 - val_loss: 2.1384 - val_categorical_accuracy: 0.4920\n",
      "Epoch 216/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1524 - categorical_accuracy: 0.9425 - val_loss: 2.1255 - val_categorical_accuracy: 0.4870\n",
      "Epoch 217/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1537 - categorical_accuracy: 0.9352 - val_loss: 2.1221 - val_categorical_accuracy: 0.5060\n",
      "Epoch 218/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1477 - categorical_accuracy: 0.9427 - val_loss: 2.1412 - val_categorical_accuracy: 0.4950\n",
      "Epoch 219/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1427 - categorical_accuracy: 0.9433 - val_loss: 2.1565 - val_categorical_accuracy: 0.4970\n",
      "Epoch 220/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1431 - categorical_accuracy: 0.9427 - val_loss: 2.1176 - val_categorical_accuracy: 0.4970\n",
      "Epoch 221/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1388 - categorical_accuracy: 0.9465 - val_loss: 2.1252 - val_categorical_accuracy: 0.4970\n",
      "Epoch 222/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1406 - categorical_accuracy: 0.9448 - val_loss: 2.1105 - val_categorical_accuracy: 0.5000\n",
      "Epoch 223/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1395 - categorical_accuracy: 0.9467 - val_loss: 2.0982 - val_categorical_accuracy: 0.5140\n",
      "Epoch 224/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1335 - categorical_accuracy: 0.9495 - val_loss: 2.1065 - val_categorical_accuracy: 0.5070\n",
      "Epoch 225/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1312 - categorical_accuracy: 0.9520 - val_loss: 2.1163 - val_categorical_accuracy: 0.5000\n",
      "Epoch 226/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1280 - categorical_accuracy: 0.9498 - val_loss: 2.1225 - val_categorical_accuracy: 0.4960\n",
      "Epoch 227/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1257 - categorical_accuracy: 0.9515 - val_loss: 2.1427 - val_categorical_accuracy: 0.4830\n",
      "Epoch 228/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1266 - categorical_accuracy: 0.9523 - val_loss: 2.1028 - val_categorical_accuracy: 0.4950\n",
      "Epoch 229/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1220 - categorical_accuracy: 0.9545 - val_loss: 2.0979 - val_categorical_accuracy: 0.5020\n",
      "Epoch 230/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1260 - categorical_accuracy: 0.9542 - val_loss: 2.0898 - val_categorical_accuracy: 0.5060\n",
      "Epoch 231/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1169 - categorical_accuracy: 0.9592 - val_loss: 2.0967 - val_categorical_accuracy: 0.5180\n",
      "Epoch 232/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1190 - categorical_accuracy: 0.9563 - val_loss: 2.1308 - val_categorical_accuracy: 0.4960\n",
      "Epoch 233/300\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 1.1161 - categorical_accuracy: 0.9555 - val_loss: 2.0706 - val_categorical_accuracy: 0.5220\n",
      "Epoch 234/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1131 - categorical_accuracy: 0.9607 - val_loss: 2.0693 - val_categorical_accuracy: 0.5160\n",
      "Epoch 235/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1152 - categorical_accuracy: 0.9578 - val_loss: 2.0684 - val_categorical_accuracy: 0.5280\n",
      "Epoch 236/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1081 - categorical_accuracy: 0.9610 - val_loss: 2.0701 - val_categorical_accuracy: 0.5310\n",
      "Epoch 237/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1112 - categorical_accuracy: 0.9575 - val_loss: 2.0781 - val_categorical_accuracy: 0.5240\n",
      "Epoch 238/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1093 - categorical_accuracy: 0.9572 - val_loss: 2.0812 - val_categorical_accuracy: 0.5180\n",
      "Epoch 239/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1058 - categorical_accuracy: 0.9650 - val_loss: 2.0540 - val_categorical_accuracy: 0.5200\n",
      "Epoch 240/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0983 - categorical_accuracy: 0.9678 - val_loss: 2.0687 - val_categorical_accuracy: 0.5220\n",
      "Epoch 241/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0976 - categorical_accuracy: 0.9650 - val_loss: 2.0646 - val_categorical_accuracy: 0.5130\n",
      "Epoch 242/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.0938 - categorical_accuracy: 0.9712 - val_loss: 2.0747 - val_categorical_accuracy: 0.5180\n",
      "Epoch 243/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0954 - categorical_accuracy: 0.9680 - val_loss: 2.0617 - val_categorical_accuracy: 0.5160\n",
      "Epoch 244/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0972 - categorical_accuracy: 0.9653 - val_loss: 2.0361 - val_categorical_accuracy: 0.5320\n",
      "Epoch 245/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0955 - categorical_accuracy: 0.9670 - val_loss: 2.0884 - val_categorical_accuracy: 0.5120\n",
      "Epoch 246/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0948 - categorical_accuracy: 0.9703 - val_loss: 2.0575 - val_categorical_accuracy: 0.5320\n",
      "Epoch 247/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0887 - categorical_accuracy: 0.9685 - val_loss: 2.0639 - val_categorical_accuracy: 0.5190\n",
      "Epoch 248/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0900 - categorical_accuracy: 0.9688 - val_loss: 2.0381 - val_categorical_accuracy: 0.5260\n",
      "Epoch 249/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0901 - categorical_accuracy: 0.9697 - val_loss: 2.0418 - val_categorical_accuracy: 0.5260\n",
      "Epoch 250/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0862 - categorical_accuracy: 0.9688 - val_loss: 2.0285 - val_categorical_accuracy: 0.5280\n",
      "Epoch 251/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0917 - categorical_accuracy: 0.9660 - val_loss: 2.0368 - val_categorical_accuracy: 0.5310\n",
      "Epoch 252/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0844 - categorical_accuracy: 0.9712 - val_loss: 2.0274 - val_categorical_accuracy: 0.5310\n",
      "Epoch 253/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0817 - categorical_accuracy: 0.9750 - val_loss: 2.0420 - val_categorical_accuracy: 0.5230\n",
      "Epoch 254/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0773 - categorical_accuracy: 0.9762 - val_loss: 2.0515 - val_categorical_accuracy: 0.5280\n",
      "Epoch 255/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.0836 - categorical_accuracy: 0.9695 - val_loss: 2.0420 - val_categorical_accuracy: 0.5280\n",
      "Epoch 256/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0772 - categorical_accuracy: 0.9728 - val_loss: 2.0315 - val_categorical_accuracy: 0.5230\n",
      "Epoch 257/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0770 - categorical_accuracy: 0.9770 - val_loss: 2.0451 - val_categorical_accuracy: 0.5290\n",
      "Epoch 258/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0820 - categorical_accuracy: 0.9745 - val_loss: 2.0372 - val_categorical_accuracy: 0.5250\n",
      "Epoch 259/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.0789 - categorical_accuracy: 0.9732 - val_loss: 2.0324 - val_categorical_accuracy: 0.5260\n",
      "Epoch 260/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0688 - categorical_accuracy: 0.9775 - val_loss: 2.0505 - val_categorical_accuracy: 0.5170\n",
      "Epoch 261/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0736 - categorical_accuracy: 0.9747 - val_loss: 2.0401 - val_categorical_accuracy: 0.5240\n",
      "Epoch 262/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.0709 - categorical_accuracy: 0.9812 - val_loss: 2.0309 - val_categorical_accuracy: 0.5300\n",
      "Epoch 263/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0739 - categorical_accuracy: 0.9718 - val_loss: 2.0204 - val_categorical_accuracy: 0.5300\n",
      "Epoch 264/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.0739 - categorical_accuracy: 0.9750 - val_loss: 2.0378 - val_categorical_accuracy: 0.5320\n",
      "Epoch 265/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0653 - categorical_accuracy: 0.9818 - val_loss: 2.0532 - val_categorical_accuracy: 0.5270\n",
      "Epoch 266/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0680 - categorical_accuracy: 0.9775 - val_loss: 2.0246 - val_categorical_accuracy: 0.5390\n",
      "Epoch 267/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0702 - categorical_accuracy: 0.9790 - val_loss: 2.0355 - val_categorical_accuracy: 0.5240\n",
      "Epoch 268/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.0660 - categorical_accuracy: 0.9793 - val_loss: 2.0352 - val_categorical_accuracy: 0.5280\n",
      "Epoch 269/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.0638 - categorical_accuracy: 0.9795 - val_loss: 2.0286 - val_categorical_accuracy: 0.5260\n",
      "Epoch 270/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0700 - categorical_accuracy: 0.9772 - val_loss: 2.0361 - val_categorical_accuracy: 0.5230\n",
      "Epoch 271/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0651 - categorical_accuracy: 0.9805 - val_loss: 2.0284 - val_categorical_accuracy: 0.5370\n",
      "Epoch 272/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0670 - categorical_accuracy: 0.9750 - val_loss: 2.0413 - val_categorical_accuracy: 0.5290\n",
      "Epoch 273/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.0629 - categorical_accuracy: 0.9793 - val_loss: 2.0324 - val_categorical_accuracy: 0.5290\n",
      "Epoch 274/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0631 - categorical_accuracy: 0.9803 - val_loss: 2.0275 - val_categorical_accuracy: 0.5280\n",
      "Epoch 275/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0623 - categorical_accuracy: 0.9820 - val_loss: 2.0303 - val_categorical_accuracy: 0.5330\n",
      "Epoch 276/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0620 - categorical_accuracy: 0.9812 - val_loss: 2.0379 - val_categorical_accuracy: 0.5230\n",
      "Epoch 277/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0571 - categorical_accuracy: 0.9833 - val_loss: 2.0293 - val_categorical_accuracy: 0.5240\n",
      "Epoch 278/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0594 - categorical_accuracy: 0.9820 - val_loss: 2.0151 - val_categorical_accuracy: 0.5350\n",
      "Epoch 279/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.0603 - categorical_accuracy: 0.9810 - val_loss: 2.0286 - val_categorical_accuracy: 0.5290\n",
      "Epoch 280/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0628 - categorical_accuracy: 0.9770 - val_loss: 2.0380 - val_categorical_accuracy: 0.5290\n",
      "Epoch 281/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.0575 - categorical_accuracy: 0.9810 - val_loss: 2.0269 - val_categorical_accuracy: 0.5330\n",
      "Epoch 282/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0561 - categorical_accuracy: 0.9843 - val_loss: 2.0206 - val_categorical_accuracy: 0.5300\n",
      "Epoch 283/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0567 - categorical_accuracy: 0.9818 - val_loss: 2.0248 - val_categorical_accuracy: 0.5350\n",
      "Epoch 284/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.0533 - categorical_accuracy: 0.9825 - val_loss: 2.0365 - val_categorical_accuracy: 0.5290\n",
      "Epoch 285/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.0554 - categorical_accuracy: 0.9827 - val_loss: 2.0439 - val_categorical_accuracy: 0.5260\n",
      "Epoch 286/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.0534 - categorical_accuracy: 0.9862 - val_loss: 2.0356 - val_categorical_accuracy: 0.5360\n",
      "Epoch 287/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 1.0578 - categorical_accuracy: 0.9814INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 1.0576 - categorical_accuracy: 0.9815 - val_loss: 2.0250 - val_categorical_accuracy: 0.5450\n",
      "Epoch 288/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0490 - categorical_accuracy: 0.9850 - val_loss: 2.0189 - val_categorical_accuracy: 0.5390\n",
      "Epoch 289/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0544 - categorical_accuracy: 0.9850 - val_loss: 2.0209 - val_categorical_accuracy: 0.5360\n",
      "Epoch 290/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.0533 - categorical_accuracy: 0.9820 - val_loss: 2.0082 - val_categorical_accuracy: 0.5380\n",
      "Epoch 291/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0540 - categorical_accuracy: 0.9827 - val_loss: 2.0284 - val_categorical_accuracy: 0.5390\n",
      "Epoch 292/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0528 - categorical_accuracy: 0.9833 - val_loss: 2.0162 - val_categorical_accuracy: 0.5330\n",
      "Epoch 293/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0517 - categorical_accuracy: 0.9835 - val_loss: 2.0257 - val_categorical_accuracy: 0.5290\n",
      "Epoch 294/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0507 - categorical_accuracy: 0.9843 - val_loss: 2.0274 - val_categorical_accuracy: 0.5300\n",
      "Epoch 295/300\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 1.0470 - categorical_accuracy: 0.9843 - val_loss: 2.0264 - val_categorical_accuracy: 0.5270\n",
      "Epoch 296/300\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 1.0524 - categorical_accuracy: 0.9843 - val_loss: 2.0246 - val_categorical_accuracy: 0.5260\n",
      "Epoch 297/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.0484 - categorical_accuracy: 0.9852 - val_loss: 2.0225 - val_categorical_accuracy: 0.5260\n",
      "Epoch 298/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0461 - categorical_accuracy: 0.9883 - val_loss: 2.0153 - val_categorical_accuracy: 0.5350\n",
      "Epoch 299/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.0477 - categorical_accuracy: 0.9820 - val_loss: 2.0162 - val_categorical_accuracy: 0.5320\n",
      "Epoch 300/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0491 - categorical_accuracy: 0.9852 - val_loss: 2.0170 - val_categorical_accuracy: 0.5390\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁▂▃▃▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁████▇▇▆▆▅▅▄▄▃▃▂▂▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▇▆▆▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁▂▅▅▆▆▇▆▇▇▇▇▇▇▇█▇███▇▇▇▇▇▇▇▇▇▇▇█████████</td></tr><tr><td>epoch/val_loss</td><td>█▆▃▂▂▁▁▃▃▃▄▃▄▃▄▃▃▃▂▂▅▄▅▄▃▄▄▄▄▃▃▃▂▂▂▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.98525</td></tr><tr><td>epoch/epoch</td><td>299</td></tr><tr><td>epoch/learning_rate</td><td>5e-05</td></tr><tr><td>epoch/loss</td><td>1.04907</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.539</td></tr><tr><td>epoch/val_loss</td><td>2.01703</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/tqe9uruf' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/tqe9uruf</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240416_150354-tqe9uruf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f954f739d50>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dim = 128  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 128  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = layers.Input(shape=X_shape)\n",
    "embedding_layer = TokenAndPositionEmbedding(X_shape[0], X_shape[1])\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(X_shape[1], num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(128, activation=\"gelu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(256, activation=\"gelu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = Dense(len(train_dataset_parquet.unique_labels), activation='softmax')(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", f\"Transformer-embed={embed_dim}-heads={num_heads}-ff={ff_dim}-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf\"),\n",
    "            run_name=\"Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k\",\n",
    "            mediapipe_features=\"reduced-LipsEyesHandsPose\",\n",
    "            T_max=151, epochs=300, \n",
    "            max_lr = 3e-4, min_lr = 5e-5,\n",
    "            USE_WANDB=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240416_154118-u544yx92</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/u544yx92' target=\"_blank\">Transformer-embed=128-heads=2-ff=128-D=0.2-Dense=256-LipsEyesHandsPose_5k</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/u544yx92' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/u544yx92</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "    498/Unknown - 10s 11ms/step - loss: 2.5964 - categorical_accuracy: 0.1082"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 15:41:28.686715: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 16023303133924989040\n",
      "2024-04-16 15:41:28.686813: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 6700206116132250662\n",
      "2024-04-16 15:41:28.686842: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7063810106286413230\n",
      "2024-04-16 15:41:28.686864: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2724364067725415920\n",
      "2024-04-16 15:41:28.686873: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 13169199823044535358\n",
      "2024-04-16 15:41:28.686904: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 8397698261506511473\n",
      "2024-04-16 15:41:28.686946: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11728785306099647447\n",
      "2024-04-16 15:41:28.686962: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 13714633262705169763\n",
      "2024-04-16 15:41:29.625282: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 5551213241208487965\n",
      "2024-04-16 15:41:29.625367: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 3094190343660208166\n",
      "2024-04-16 15:41:29.625385: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7767172026591208356\n",
      "2024-04-16 15:41:29.625407: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 6462147652021642174\n",
      "2024-04-16 15:41:29.625454: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4153888742695321310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 14s 19ms/step - loss: 2.5963 - categorical_accuracy: 0.1082 - val_loss: 2.4758 - val_categorical_accuracy: 0.1380\n",
      "Epoch 2/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.4528 - categorical_accuracy: 0.1564INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 2.4527 - categorical_accuracy: 0.1565 - val_loss: 2.3993 - val_categorical_accuracy: 0.1880\n",
      "Epoch 3/300\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.4036 - categorical_accuracy: 0.1723INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.4036 - categorical_accuracy: 0.1723 - val_loss: 2.3626 - val_categorical_accuracy: 0.1930\n",
      "Epoch 4/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.3696 - categorical_accuracy: 0.1937INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 2.3690 - categorical_accuracy: 0.1940 - val_loss: 2.3445 - val_categorical_accuracy: 0.2080\n",
      "Epoch 5/300\n",
      "496/500 [============================>.] - ETA: 0s - loss: 2.3356 - categorical_accuracy: 0.2117INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.3349 - categorical_accuracy: 0.2118 - val_loss: 2.2991 - val_categorical_accuracy: 0.2360\n",
      "Epoch 6/300\n",
      "498/500 [============================>.] - ETA: 0s - loss: 2.3101 - categorical_accuracy: 0.2374INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 2.3098 - categorical_accuracy: 0.2373 - val_loss: 2.3202 - val_categorical_accuracy: 0.2390\n",
      "Epoch 7/300\n",
      "499/500 [============================>.] - ETA: 0s - loss: 2.2881 - categorical_accuracy: 0.2490INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 2.2880 - categorical_accuracy: 0.2490 - val_loss: 2.3125 - val_categorical_accuracy: 0.2440\n",
      "Epoch 8/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.2672 - categorical_accuracy: 0.2540INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 2.2661 - categorical_accuracy: 0.2545 - val_loss: 2.2288 - val_categorical_accuracy: 0.2680\n",
      "Epoch 9/300\n",
      "496/500 [============================>.] - ETA: 0s - loss: 2.2416 - categorical_accuracy: 0.2752INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.2405 - categorical_accuracy: 0.2763 - val_loss: 2.2395 - val_categorical_accuracy: 0.2820\n",
      "Epoch 10/300\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.2199 - categorical_accuracy: 0.2870INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 2.2199 - categorical_accuracy: 0.2870 - val_loss: 2.2130 - val_categorical_accuracy: 0.2840\n",
      "Epoch 11/300\n",
      "499/500 [============================>.] - ETA: 0s - loss: 2.1954 - categorical_accuracy: 0.3071INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 2.1946 - categorical_accuracy: 0.3077 - val_loss: 2.1753 - val_categorical_accuracy: 0.3090\n",
      "Epoch 12/300\n",
      "495/500 [============================>.] - ETA: 0s - loss: 2.1768 - categorical_accuracy: 0.3139INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 2.1745 - categorical_accuracy: 0.3153 - val_loss: 2.1552 - val_categorical_accuracy: 0.3150\n",
      "Epoch 13/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.1623 - categorical_accuracy: 0.3302INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 2.1601 - categorical_accuracy: 0.3310 - val_loss: 2.1198 - val_categorical_accuracy: 0.3640\n",
      "Epoch 14/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 2.1316 - categorical_accuracy: 0.3512 - val_loss: 2.1175 - val_categorical_accuracy: 0.3450\n",
      "Epoch 15/300\n",
      "496/500 [============================>.] - ETA: 0s - loss: 2.1173 - categorical_accuracy: 0.3705INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 2.1152 - categorical_accuracy: 0.3715 - val_loss: 2.1046 - val_categorical_accuracy: 0.3670\n",
      "Epoch 16/300\n",
      "499/500 [============================>.] - ETA: 0s - loss: 2.0963 - categorical_accuracy: 0.3775INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.0953 - categorical_accuracy: 0.3780 - val_loss: 2.0665 - val_categorical_accuracy: 0.3880\n",
      "Epoch 17/300\n",
      "499/500 [============================>.] - ETA: 0s - loss: 2.0780 - categorical_accuracy: 0.3848INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 2.0774 - categorical_accuracy: 0.3853 - val_loss: 2.1007 - val_categorical_accuracy: 0.4030\n",
      "Epoch 18/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 2.0551 - categorical_accuracy: 0.4155 - val_loss: 2.0347 - val_categorical_accuracy: 0.3960\n",
      "Epoch 19/300\n",
      "498/500 [============================>.] - ETA: 0s - loss: 2.0356 - categorical_accuracy: 0.4081INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 2.0342 - categorical_accuracy: 0.4092 - val_loss: 2.0444 - val_categorical_accuracy: 0.4130\n",
      "Epoch 20/300\n",
      "499/500 [============================>.] - ETA: 0s - loss: 2.0143 - categorical_accuracy: 0.4243INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 2.0134 - categorical_accuracy: 0.4245 - val_loss: 2.0523 - val_categorical_accuracy: 0.4160\n",
      "Epoch 21/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.9961 - categorical_accuracy: 0.4383 - val_loss: 2.0486 - val_categorical_accuracy: 0.4160\n",
      "Epoch 22/300\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.9737 - categorical_accuracy: 0.4566INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 1.9724 - categorical_accuracy: 0.4570 - val_loss: 2.0074 - val_categorical_accuracy: 0.4450\n",
      "Epoch 23/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.9662 - categorical_accuracy: 0.4550 - val_loss: 2.0210 - val_categorical_accuracy: 0.4400\n",
      "Epoch 24/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.9454 - categorical_accuracy: 0.4708 - val_loss: 2.0270 - val_categorical_accuracy: 0.4260\n",
      "Epoch 25/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.9326 - categorical_accuracy: 0.4710 - val_loss: 2.0832 - val_categorical_accuracy: 0.4230\n",
      "Epoch 26/300\n",
      "499/500 [============================>.] - ETA: 0s - loss: 1.9131 - categorical_accuracy: 0.4975INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 1.9126 - categorical_accuracy: 0.4978 - val_loss: 1.9986 - val_categorical_accuracy: 0.4640\n",
      "Epoch 27/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 1.8975 - categorical_accuracy: 0.5005INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 1.8962 - categorical_accuracy: 0.5010 - val_loss: 1.9515 - val_categorical_accuracy: 0.4810\n",
      "Epoch 28/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.8794 - categorical_accuracy: 0.5075 - val_loss: 1.9579 - val_categorical_accuracy: 0.4650\n",
      "Epoch 29/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.8784 - categorical_accuracy: 0.5170 - val_loss: 1.9601 - val_categorical_accuracy: 0.4780\n",
      "Epoch 30/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.8600 - categorical_accuracy: 0.5235 - val_loss: 2.0044 - val_categorical_accuracy: 0.4620\n",
      "Epoch 31/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.8478 - categorical_accuracy: 0.5245 - val_loss: 2.0177 - val_categorical_accuracy: 0.4620\n",
      "Epoch 32/300\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.8412 - categorical_accuracy: 0.5325INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 19ms/step - loss: 1.8412 - categorical_accuracy: 0.5325 - val_loss: 1.9413 - val_categorical_accuracy: 0.4880\n",
      "Epoch 33/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.8198 - categorical_accuracy: 0.5525 - val_loss: 2.0235 - val_categorical_accuracy: 0.4590\n",
      "Epoch 34/300\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.7980 - categorical_accuracy: 0.5638INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 1.7976 - categorical_accuracy: 0.5635 - val_loss: 1.9754 - val_categorical_accuracy: 0.4910\n",
      "Epoch 35/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7974 - categorical_accuracy: 0.5560 - val_loss: 2.0355 - val_categorical_accuracy: 0.4760\n",
      "Epoch 36/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7982 - categorical_accuracy: 0.5565 - val_loss: 2.0018 - val_categorical_accuracy: 0.4740\n",
      "Epoch 37/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7745 - categorical_accuracy: 0.5673 - val_loss: 1.9823 - val_categorical_accuracy: 0.4820\n",
      "Epoch 38/300\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.7642 - categorical_accuracy: 0.5773INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 1.7639 - categorical_accuracy: 0.5773 - val_loss: 1.9670 - val_categorical_accuracy: 0.5030\n",
      "Epoch 39/300\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.7620 - categorical_accuracy: 0.5803INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 1.7620 - categorical_accuracy: 0.5803 - val_loss: 1.9884 - val_categorical_accuracy: 0.5040\n",
      "Epoch 40/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7434 - categorical_accuracy: 0.5878 - val_loss: 1.9618 - val_categorical_accuracy: 0.4970\n",
      "Epoch 41/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7401 - categorical_accuracy: 0.5975 - val_loss: 1.9971 - val_categorical_accuracy: 0.4750\n",
      "Epoch 42/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7367 - categorical_accuracy: 0.6008 - val_loss: 1.9997 - val_categorical_accuracy: 0.4610\n",
      "Epoch 43/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7360 - categorical_accuracy: 0.5950 - val_loss: 1.9817 - val_categorical_accuracy: 0.4970\n",
      "Epoch 44/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7068 - categorical_accuracy: 0.6162 - val_loss: 1.9821 - val_categorical_accuracy: 0.5020\n",
      "Epoch 45/300\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.7052 - categorical_accuracy: 0.6145INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 1.7043 - categorical_accuracy: 0.6148 - val_loss: 1.9819 - val_categorical_accuracy: 0.5060\n",
      "Epoch 46/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 1.6951 - categorical_accuracy: 0.6258INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 1.6942 - categorical_accuracy: 0.6255 - val_loss: 1.9664 - val_categorical_accuracy: 0.5120\n",
      "Epoch 47/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6912 - categorical_accuracy: 0.6298 - val_loss: 1.9862 - val_categorical_accuracy: 0.4920\n",
      "Epoch 48/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6791 - categorical_accuracy: 0.6265 - val_loss: 1.9644 - val_categorical_accuracy: 0.5060\n",
      "Epoch 49/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6778 - categorical_accuracy: 0.6367 - val_loss: 1.9659 - val_categorical_accuracy: 0.5090\n",
      "Epoch 50/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.6645 - categorical_accuracy: 0.6407 - val_loss: 1.9601 - val_categorical_accuracy: 0.5090\n",
      "Epoch 51/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 1.6527 - categorical_accuracy: 0.6476INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 17ms/step - loss: 1.6516 - categorical_accuracy: 0.6488 - val_loss: 1.9749 - val_categorical_accuracy: 0.5140\n",
      "Epoch 52/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6380 - categorical_accuracy: 0.6620 - val_loss: 1.9809 - val_categorical_accuracy: 0.5060\n",
      "Epoch 53/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6398 - categorical_accuracy: 0.6538 - val_loss: 1.9458 - val_categorical_accuracy: 0.5060\n",
      "Epoch 54/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6234 - categorical_accuracy: 0.6637 - val_loss: 1.9547 - val_categorical_accuracy: 0.5100\n",
      "Epoch 55/300\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.6265 - categorical_accuracy: 0.6724INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 1.6260 - categorical_accuracy: 0.6733 - val_loss: 1.9731 - val_categorical_accuracy: 0.5150\n",
      "Epoch 56/300\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.6118 - categorical_accuracy: 0.6691INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 1.6109 - categorical_accuracy: 0.6700 - val_loss: 1.9866 - val_categorical_accuracy: 0.5230\n",
      "Epoch 57/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 1.6064 - categorical_accuracy: 0.6728INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 1.6056 - categorical_accuracy: 0.6735 - val_loss: 1.9542 - val_categorical_accuracy: 0.5260\n",
      "Epoch 58/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.5992 - categorical_accuracy: 0.6865 - val_loss: 1.9618 - val_categorical_accuracy: 0.5190\n",
      "Epoch 59/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5840 - categorical_accuracy: 0.6917 - val_loss: 1.9698 - val_categorical_accuracy: 0.5230\n",
      "Epoch 60/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5936 - categorical_accuracy: 0.6852 - val_loss: 1.9946 - val_categorical_accuracy: 0.4970\n",
      "Epoch 61/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5692 - categorical_accuracy: 0.7038 - val_loss: 1.9578 - val_categorical_accuracy: 0.5140\n",
      "Epoch 62/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.5693 - categorical_accuracy: 0.7035 - val_loss: 1.9759 - val_categorical_accuracy: 0.5070\n",
      "Epoch 63/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5578 - categorical_accuracy: 0.7120 - val_loss: 2.0007 - val_categorical_accuracy: 0.4960\n",
      "Epoch 64/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5613 - categorical_accuracy: 0.7000 - val_loss: 2.0088 - val_categorical_accuracy: 0.4940\n",
      "Epoch 65/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 1.5603 - categorical_accuracy: 0.7070INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 1.5590 - categorical_accuracy: 0.7082 - val_loss: 1.9659 - val_categorical_accuracy: 0.5310\n",
      "Epoch 66/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 1.5511 - categorical_accuracy: 0.7155INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 1.5496 - categorical_accuracy: 0.7168 - val_loss: 1.9475 - val_categorical_accuracy: 0.5360\n",
      "Epoch 67/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5458 - categorical_accuracy: 0.7220 - val_loss: 1.9624 - val_categorical_accuracy: 0.5240\n",
      "Epoch 68/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5417 - categorical_accuracy: 0.7245 - val_loss: 2.0123 - val_categorical_accuracy: 0.5180\n",
      "Epoch 69/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5264 - categorical_accuracy: 0.7295 - val_loss: 1.9954 - val_categorical_accuracy: 0.5270\n",
      "Epoch 70/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5217 - categorical_accuracy: 0.7290 - val_loss: 1.9860 - val_categorical_accuracy: 0.5270\n",
      "Epoch 71/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.5206 - categorical_accuracy: 0.7322 - val_loss: 2.0054 - val_categorical_accuracy: 0.5120\n",
      "Epoch 72/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.5176 - categorical_accuracy: 0.7327 - val_loss: 2.0168 - val_categorical_accuracy: 0.5110\n",
      "Epoch 73/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5238 - categorical_accuracy: 0.7287 - val_loss: 2.0035 - val_categorical_accuracy: 0.5180\n",
      "Epoch 74/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.5014 - categorical_accuracy: 0.7470 - val_loss: 2.0106 - val_categorical_accuracy: 0.5250\n",
      "Epoch 75/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.4966 - categorical_accuracy: 0.7462 - val_loss: 2.0251 - val_categorical_accuracy: 0.5270\n",
      "Epoch 76/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4972 - categorical_accuracy: 0.7492 - val_loss: 2.0353 - val_categorical_accuracy: 0.5150\n",
      "Epoch 77/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4813 - categorical_accuracy: 0.7563 - val_loss: 2.0237 - val_categorical_accuracy: 0.5060\n",
      "Epoch 78/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4813 - categorical_accuracy: 0.7525 - val_loss: 2.0396 - val_categorical_accuracy: 0.5170\n",
      "Epoch 79/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4728 - categorical_accuracy: 0.7663 - val_loss: 2.0432 - val_categorical_accuracy: 0.5050\n",
      "Epoch 80/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4786 - categorical_accuracy: 0.7538 - val_loss: 2.0381 - val_categorical_accuracy: 0.5230\n",
      "Epoch 81/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4803 - categorical_accuracy: 0.7550 - val_loss: 2.0968 - val_categorical_accuracy: 0.4780\n",
      "Epoch 82/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.4695 - categorical_accuracy: 0.7628 - val_loss: 2.0496 - val_categorical_accuracy: 0.5120\n",
      "Epoch 83/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.4716 - categorical_accuracy: 0.7605 - val_loss: 2.0445 - val_categorical_accuracy: 0.5030\n",
      "Epoch 84/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.4636 - categorical_accuracy: 0.7715 - val_loss: 2.0265 - val_categorical_accuracy: 0.5110\n",
      "Epoch 85/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.4565 - categorical_accuracy: 0.7753 - val_loss: 2.0637 - val_categorical_accuracy: 0.4970\n",
      "Epoch 86/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4488 - categorical_accuracy: 0.7735 - val_loss: 2.0673 - val_categorical_accuracy: 0.4830\n",
      "Epoch 87/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.4483 - categorical_accuracy: 0.7800 - val_loss: 2.0251 - val_categorical_accuracy: 0.5170\n",
      "Epoch 88/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4519 - categorical_accuracy: 0.7768 - val_loss: 2.0442 - val_categorical_accuracy: 0.5050\n",
      "Epoch 89/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4399 - categorical_accuracy: 0.7853 - val_loss: 2.0370 - val_categorical_accuracy: 0.5040\n",
      "Epoch 90/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4452 - categorical_accuracy: 0.7722 - val_loss: 2.0413 - val_categorical_accuracy: 0.5110\n",
      "Epoch 91/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4362 - categorical_accuracy: 0.7970 - val_loss: 2.0812 - val_categorical_accuracy: 0.4840\n",
      "Epoch 92/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4416 - categorical_accuracy: 0.7812 - val_loss: 2.0321 - val_categorical_accuracy: 0.5110\n",
      "Epoch 93/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.4245 - categorical_accuracy: 0.7908 - val_loss: 2.0195 - val_categorical_accuracy: 0.4970\n",
      "Epoch 94/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.4293 - categorical_accuracy: 0.7872 - val_loss: 2.0322 - val_categorical_accuracy: 0.5190\n",
      "Epoch 95/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4207 - categorical_accuracy: 0.7935 - val_loss: 2.0347 - val_categorical_accuracy: 0.5030\n",
      "Epoch 96/300\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 1.4154 - categorical_accuracy: 0.8075 - val_loss: 2.0480 - val_categorical_accuracy: 0.5210\n",
      "Epoch 97/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4132 - categorical_accuracy: 0.7987 - val_loss: 2.0238 - val_categorical_accuracy: 0.5160\n",
      "Epoch 98/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4125 - categorical_accuracy: 0.8018 - val_loss: 2.0364 - val_categorical_accuracy: 0.5230\n",
      "Epoch 99/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.4110 - categorical_accuracy: 0.8037 - val_loss: 2.0409 - val_categorical_accuracy: 0.5070\n",
      "Epoch 100/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4062 - categorical_accuracy: 0.8055 - val_loss: 2.0438 - val_categorical_accuracy: 0.5190\n",
      "Epoch 101/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4077 - categorical_accuracy: 0.8027 - val_loss: 2.0179 - val_categorical_accuracy: 0.5190\n",
      "Epoch 102/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3983 - categorical_accuracy: 0.8158 - val_loss: 2.0457 - val_categorical_accuracy: 0.5120\n",
      "Epoch 103/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3922 - categorical_accuracy: 0.8180 - val_loss: 2.0285 - val_categorical_accuracy: 0.5160\n",
      "Epoch 104/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3986 - categorical_accuracy: 0.8105 - val_loss: 2.0248 - val_categorical_accuracy: 0.5340\n",
      "Epoch 105/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3850 - categorical_accuracy: 0.8213 - val_loss: 2.0350 - val_categorical_accuracy: 0.5280\n",
      "Epoch 106/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3816 - categorical_accuracy: 0.8192 - val_loss: 2.0313 - val_categorical_accuracy: 0.5350\n",
      "Epoch 107/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3793 - categorical_accuracy: 0.8245 - val_loss: 2.0491 - val_categorical_accuracy: 0.5260\n",
      "Epoch 108/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.3673 - categorical_accuracy: 0.8235 - val_loss: 2.0581 - val_categorical_accuracy: 0.5310\n",
      "Epoch 109/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3680 - categorical_accuracy: 0.8273 - val_loss: 2.0367 - val_categorical_accuracy: 0.5270\n",
      "Epoch 110/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3742 - categorical_accuracy: 0.8192 - val_loss: 2.0492 - val_categorical_accuracy: 0.5330\n",
      "Epoch 111/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3693 - categorical_accuracy: 0.8357 - val_loss: 2.0237 - val_categorical_accuracy: 0.5300\n",
      "Epoch 112/300\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.3648 - categorical_accuracy: 0.8324INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 17ms/step - loss: 1.3646 - categorical_accuracy: 0.8328 - val_loss: 2.0260 - val_categorical_accuracy: 0.5370\n",
      "Epoch 113/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3629 - categorical_accuracy: 0.8300 - val_loss: 2.0548 - val_categorical_accuracy: 0.5190\n",
      "Epoch 114/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.3543 - categorical_accuracy: 0.8350 - val_loss: 2.0647 - val_categorical_accuracy: 0.5270\n",
      "Epoch 115/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3620 - categorical_accuracy: 0.8295 - val_loss: 2.0435 - val_categorical_accuracy: 0.5260\n",
      "Epoch 116/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3517 - categorical_accuracy: 0.8335 - val_loss: 2.0584 - val_categorical_accuracy: 0.5240\n",
      "Epoch 117/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3420 - categorical_accuracy: 0.8485 - val_loss: 2.0789 - val_categorical_accuracy: 0.5190\n",
      "Epoch 118/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3422 - categorical_accuracy: 0.8465 - val_loss: 2.0691 - val_categorical_accuracy: 0.5230\n",
      "Epoch 119/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.3411 - categorical_accuracy: 0.8510 - val_loss: 2.0784 - val_categorical_accuracy: 0.5170\n",
      "Epoch 120/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3416 - categorical_accuracy: 0.8455 - val_loss: 2.0774 - val_categorical_accuracy: 0.5310\n",
      "Epoch 121/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3413 - categorical_accuracy: 0.8487 - val_loss: 2.0753 - val_categorical_accuracy: 0.5210\n",
      "Epoch 122/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.3287 - categorical_accuracy: 0.8572 - val_loss: 2.0902 - val_categorical_accuracy: 0.5170\n",
      "Epoch 123/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3337 - categorical_accuracy: 0.8475 - val_loss: 2.0764 - val_categorical_accuracy: 0.5310\n",
      "Epoch 124/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.3247 - categorical_accuracy: 0.8537 - val_loss: 2.0840 - val_categorical_accuracy: 0.5280\n",
      "Epoch 125/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.3293 - categorical_accuracy: 0.8485 - val_loss: 2.0794 - val_categorical_accuracy: 0.5260\n",
      "Epoch 126/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3220 - categorical_accuracy: 0.8570 - val_loss: 2.0788 - val_categorical_accuracy: 0.5190\n",
      "Epoch 127/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3259 - categorical_accuracy: 0.8543 - val_loss: 2.0680 - val_categorical_accuracy: 0.5230\n",
      "Epoch 128/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3211 - categorical_accuracy: 0.8505 - val_loss: 2.0618 - val_categorical_accuracy: 0.5270\n",
      "Epoch 129/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3201 - categorical_accuracy: 0.8547 - val_loss: 2.0716 - val_categorical_accuracy: 0.5310\n",
      "Epoch 130/300\n",
      "499/500 [============================>.] - ETA: 0s - loss: 1.3095 - categorical_accuracy: 0.8652INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 1.3090 - categorical_accuracy: 0.8655 - val_loss: 2.0945 - val_categorical_accuracy: 0.5450\n",
      "Epoch 131/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3175 - categorical_accuracy: 0.8655 - val_loss: 2.0812 - val_categorical_accuracy: 0.5320\n",
      "Epoch 132/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3119 - categorical_accuracy: 0.8665 - val_loss: 2.0780 - val_categorical_accuracy: 0.5320\n",
      "Epoch 133/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3048 - categorical_accuracy: 0.8675 - val_loss: 2.0773 - val_categorical_accuracy: 0.5280\n",
      "Epoch 134/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3042 - categorical_accuracy: 0.8670 - val_loss: 2.0836 - val_categorical_accuracy: 0.5330\n",
      "Epoch 135/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3009 - categorical_accuracy: 0.8695 - val_loss: 2.0890 - val_categorical_accuracy: 0.5260\n",
      "Epoch 136/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2991 - categorical_accuracy: 0.8710 - val_loss: 2.0877 - val_categorical_accuracy: 0.5240\n",
      "Epoch 137/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.3013 - categorical_accuracy: 0.8673 - val_loss: 2.0822 - val_categorical_accuracy: 0.5270\n",
      "Epoch 138/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2999 - categorical_accuracy: 0.8745 - val_loss: 2.0868 - val_categorical_accuracy: 0.5180\n",
      "Epoch 139/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3009 - categorical_accuracy: 0.8677 - val_loss: 2.0850 - val_categorical_accuracy: 0.5210\n",
      "Epoch 140/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2960 - categorical_accuracy: 0.8765 - val_loss: 2.0911 - val_categorical_accuracy: 0.5350\n",
      "Epoch 141/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2945 - categorical_accuracy: 0.8677 - val_loss: 2.0955 - val_categorical_accuracy: 0.5240\n",
      "Epoch 142/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2956 - categorical_accuracy: 0.8767 - val_loss: 2.0953 - val_categorical_accuracy: 0.5330\n",
      "Epoch 143/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2902 - categorical_accuracy: 0.8760 - val_loss: 2.0912 - val_categorical_accuracy: 0.5350\n",
      "Epoch 144/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2807 - categorical_accuracy: 0.8830 - val_loss: 2.0931 - val_categorical_accuracy: 0.5280\n",
      "Epoch 145/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2814 - categorical_accuracy: 0.8775 - val_loss: 2.0912 - val_categorical_accuracy: 0.5340\n",
      "Epoch 146/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2948 - categorical_accuracy: 0.8710 - val_loss: 2.0850 - val_categorical_accuracy: 0.5350\n",
      "Epoch 147/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.2872 - categorical_accuracy: 0.8790 - val_loss: 2.0982 - val_categorical_accuracy: 0.5340\n",
      "Epoch 148/300\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 1.2832 - categorical_accuracy: 0.8823 - val_loss: 2.0948 - val_categorical_accuracy: 0.5220\n",
      "Epoch 149/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2836 - categorical_accuracy: 0.8808 - val_loss: 2.0984 - val_categorical_accuracy: 0.5390\n",
      "Epoch 150/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2827 - categorical_accuracy: 0.8838 - val_loss: 2.0904 - val_categorical_accuracy: 0.5330\n",
      "Epoch 151/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4547 - categorical_accuracy: 0.7755 - val_loss: 2.1176 - val_categorical_accuracy: 0.5200\n",
      "Epoch 152/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4519 - categorical_accuracy: 0.7753 - val_loss: 2.1721 - val_categorical_accuracy: 0.5050\n",
      "Epoch 153/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.4543 - categorical_accuracy: 0.7703 - val_loss: 2.0552 - val_categorical_accuracy: 0.5370\n",
      "Epoch 154/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4462 - categorical_accuracy: 0.7753 - val_loss: 2.0677 - val_categorical_accuracy: 0.5350\n",
      "Epoch 155/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4294 - categorical_accuracy: 0.7860 - val_loss: 2.0604 - val_categorical_accuracy: 0.5260\n",
      "Epoch 156/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4417 - categorical_accuracy: 0.7797 - val_loss: 2.0641 - val_categorical_accuracy: 0.5310\n",
      "Epoch 157/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4323 - categorical_accuracy: 0.7845 - val_loss: 2.0730 - val_categorical_accuracy: 0.5210\n",
      "Epoch 158/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4130 - categorical_accuracy: 0.7970 - val_loss: 2.1505 - val_categorical_accuracy: 0.5210\n",
      "Epoch 159/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4168 - categorical_accuracy: 0.7897 - val_loss: 2.1062 - val_categorical_accuracy: 0.4970\n",
      "Epoch 160/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4141 - categorical_accuracy: 0.8005 - val_loss: 2.0638 - val_categorical_accuracy: 0.5260\n",
      "Epoch 161/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4107 - categorical_accuracy: 0.8027 - val_loss: 2.0931 - val_categorical_accuracy: 0.5250\n",
      "Epoch 162/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4047 - categorical_accuracy: 0.8000 - val_loss: 2.1204 - val_categorical_accuracy: 0.5070\n",
      "Epoch 163/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4073 - categorical_accuracy: 0.8060 - val_loss: 2.0882 - val_categorical_accuracy: 0.5290\n",
      "Epoch 164/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3771 - categorical_accuracy: 0.8230 - val_loss: 2.0902 - val_categorical_accuracy: 0.5240\n",
      "Epoch 165/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3945 - categorical_accuracy: 0.8133 - val_loss: 2.1130 - val_categorical_accuracy: 0.5100\n",
      "Epoch 166/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3952 - categorical_accuracy: 0.8060 - val_loss: 2.0820 - val_categorical_accuracy: 0.5350\n",
      "Epoch 167/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.3828 - categorical_accuracy: 0.8190 - val_loss: 2.0762 - val_categorical_accuracy: 0.5230\n",
      "Epoch 168/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3773 - categorical_accuracy: 0.8220 - val_loss: 2.1104 - val_categorical_accuracy: 0.5230\n",
      "Epoch 169/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3724 - categorical_accuracy: 0.8220 - val_loss: 2.1583 - val_categorical_accuracy: 0.5100\n",
      "Epoch 170/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3596 - categorical_accuracy: 0.8310 - val_loss: 2.1548 - val_categorical_accuracy: 0.4990\n",
      "Epoch 171/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.3610 - categorical_accuracy: 0.8292 - val_loss: 2.0776 - val_categorical_accuracy: 0.5250\n",
      "Epoch 172/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3611 - categorical_accuracy: 0.8310 - val_loss: 2.1007 - val_categorical_accuracy: 0.5170\n",
      "Epoch 173/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3448 - categorical_accuracy: 0.8428 - val_loss: 2.1123 - val_categorical_accuracy: 0.5310\n",
      "Epoch 174/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3495 - categorical_accuracy: 0.8390 - val_loss: 2.0708 - val_categorical_accuracy: 0.5140\n",
      "Epoch 175/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3493 - categorical_accuracy: 0.8328 - val_loss: 2.1464 - val_categorical_accuracy: 0.5000\n",
      "Epoch 176/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3374 - categorical_accuracy: 0.8503 - val_loss: 2.1232 - val_categorical_accuracy: 0.5220\n",
      "Epoch 177/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.3383 - categorical_accuracy: 0.8425 - val_loss: 2.1308 - val_categorical_accuracy: 0.5160\n",
      "Epoch 178/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3369 - categorical_accuracy: 0.8435 - val_loss: 2.1403 - val_categorical_accuracy: 0.5210\n",
      "Epoch 179/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3373 - categorical_accuracy: 0.8480 - val_loss: 2.1277 - val_categorical_accuracy: 0.5170\n",
      "Epoch 180/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.3327 - categorical_accuracy: 0.8505 - val_loss: 2.1229 - val_categorical_accuracy: 0.5260\n",
      "Epoch 181/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3308 - categorical_accuracy: 0.8462 - val_loss: 2.1105 - val_categorical_accuracy: 0.5130\n",
      "Epoch 182/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3132 - categorical_accuracy: 0.8602 - val_loss: 2.1068 - val_categorical_accuracy: 0.5140\n",
      "Epoch 183/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3267 - categorical_accuracy: 0.8562 - val_loss: 2.0764 - val_categorical_accuracy: 0.5440\n",
      "Epoch 184/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3090 - categorical_accuracy: 0.8605 - val_loss: 2.1299 - val_categorical_accuracy: 0.5230\n",
      "Epoch 185/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3157 - categorical_accuracy: 0.8690 - val_loss: 2.1201 - val_categorical_accuracy: 0.5190\n",
      "Epoch 186/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3069 - categorical_accuracy: 0.8710 - val_loss: 2.1339 - val_categorical_accuracy: 0.5300\n",
      "Epoch 187/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.3041 - categorical_accuracy: 0.8622 - val_loss: 2.1029 - val_categorical_accuracy: 0.5230\n",
      "Epoch 188/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2899 - categorical_accuracy: 0.8752 - val_loss: 2.1816 - val_categorical_accuracy: 0.4940\n",
      "Epoch 189/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2974 - categorical_accuracy: 0.8717 - val_loss: 2.1286 - val_categorical_accuracy: 0.5200\n",
      "Epoch 190/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2879 - categorical_accuracy: 0.8770 - val_loss: 2.1506 - val_categorical_accuracy: 0.5360\n",
      "Epoch 191/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2797 - categorical_accuracy: 0.8888 - val_loss: 2.1599 - val_categorical_accuracy: 0.5150\n",
      "Epoch 192/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2857 - categorical_accuracy: 0.8800 - val_loss: 2.1549 - val_categorical_accuracy: 0.4980\n",
      "Epoch 193/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2824 - categorical_accuracy: 0.8788 - val_loss: 2.1226 - val_categorical_accuracy: 0.5280\n",
      "Epoch 194/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2736 - categorical_accuracy: 0.8898 - val_loss: 2.1761 - val_categorical_accuracy: 0.5240\n",
      "Epoch 195/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2761 - categorical_accuracy: 0.8840 - val_loss: 2.1407 - val_categorical_accuracy: 0.5260\n",
      "Epoch 196/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2768 - categorical_accuracy: 0.8863 - val_loss: 2.1105 - val_categorical_accuracy: 0.5310\n",
      "Epoch 197/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2700 - categorical_accuracy: 0.8873 - val_loss: 2.1174 - val_categorical_accuracy: 0.5280\n",
      "Epoch 198/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2593 - categorical_accuracy: 0.8955 - val_loss: 2.1869 - val_categorical_accuracy: 0.5250\n",
      "Epoch 199/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2655 - categorical_accuracy: 0.8928 - val_loss: 2.1294 - val_categorical_accuracy: 0.5320\n",
      "Epoch 200/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2623 - categorical_accuracy: 0.8988 - val_loss: 2.1038 - val_categorical_accuracy: 0.5210\n",
      "Epoch 201/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2599 - categorical_accuracy: 0.8955 - val_loss: 2.1175 - val_categorical_accuracy: 0.5220\n",
      "Epoch 202/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2604 - categorical_accuracy: 0.8947 - val_loss: 2.1561 - val_categorical_accuracy: 0.5150\n",
      "Epoch 203/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2567 - categorical_accuracy: 0.8935 - val_loss: 2.1415 - val_categorical_accuracy: 0.5170\n",
      "Epoch 204/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2424 - categorical_accuracy: 0.9095 - val_loss: 2.1638 - val_categorical_accuracy: 0.5310\n",
      "Epoch 205/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.2507 - categorical_accuracy: 0.9053 - val_loss: 2.1228 - val_categorical_accuracy: 0.5260\n",
      "Epoch 206/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2474 - categorical_accuracy: 0.9035 - val_loss: 2.1226 - val_categorical_accuracy: 0.5330\n",
      "Epoch 207/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2360 - categorical_accuracy: 0.9075 - val_loss: 2.1770 - val_categorical_accuracy: 0.5180\n",
      "Epoch 208/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2347 - categorical_accuracy: 0.9135 - val_loss: 2.1625 - val_categorical_accuracy: 0.5250\n",
      "Epoch 209/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2353 - categorical_accuracy: 0.9080 - val_loss: 2.1529 - val_categorical_accuracy: 0.5210\n",
      "Epoch 210/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2263 - categorical_accuracy: 0.9178 - val_loss: 2.1870 - val_categorical_accuracy: 0.5240\n",
      "Epoch 211/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2336 - categorical_accuracy: 0.9125 - val_loss: 2.1586 - val_categorical_accuracy: 0.5150\n",
      "Epoch 212/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2179 - categorical_accuracy: 0.9208 - val_loss: 2.1529 - val_categorical_accuracy: 0.5220\n",
      "Epoch 213/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2265 - categorical_accuracy: 0.9165 - val_loss: 2.1467 - val_categorical_accuracy: 0.5210\n",
      "Epoch 214/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.2142 - categorical_accuracy: 0.9255 - val_loss: 2.1791 - val_categorical_accuracy: 0.5290\n",
      "Epoch 215/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2140 - categorical_accuracy: 0.9245 - val_loss: 2.1673 - val_categorical_accuracy: 0.5180\n",
      "Epoch 216/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2174 - categorical_accuracy: 0.9235 - val_loss: 2.1487 - val_categorical_accuracy: 0.5290\n",
      "Epoch 217/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2115 - categorical_accuracy: 0.9275 - val_loss: 2.1220 - val_categorical_accuracy: 0.5210\n",
      "Epoch 218/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2151 - categorical_accuracy: 0.9222 - val_loss: 2.1216 - val_categorical_accuracy: 0.5290\n",
      "Epoch 219/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2078 - categorical_accuracy: 0.9260 - val_loss: 2.1381 - val_categorical_accuracy: 0.5340\n",
      "Epoch 220/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2040 - categorical_accuracy: 0.9333 - val_loss: 2.1552 - val_categorical_accuracy: 0.5180\n",
      "Epoch 221/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1987 - categorical_accuracy: 0.9348 - val_loss: 2.1723 - val_categorical_accuracy: 0.5100\n",
      "Epoch 222/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1933 - categorical_accuracy: 0.9337 - val_loss: 2.1272 - val_categorical_accuracy: 0.5190\n",
      "Epoch 223/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1970 - categorical_accuracy: 0.9330 - val_loss: 2.1189 - val_categorical_accuracy: 0.5190\n",
      "Epoch 224/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2010 - categorical_accuracy: 0.9305 - val_loss: 2.1190 - val_categorical_accuracy: 0.5310\n",
      "Epoch 225/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1911 - categorical_accuracy: 0.9358 - val_loss: 2.1559 - val_categorical_accuracy: 0.5150\n",
      "Epoch 226/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.1842 - categorical_accuracy: 0.9375 - val_loss: 2.1303 - val_categorical_accuracy: 0.5180\n",
      "Epoch 227/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1937 - categorical_accuracy: 0.9360 - val_loss: 2.1514 - val_categorical_accuracy: 0.5260\n",
      "Epoch 228/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1836 - categorical_accuracy: 0.9430 - val_loss: 2.1696 - val_categorical_accuracy: 0.5260\n",
      "Epoch 229/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1871 - categorical_accuracy: 0.9400 - val_loss: 2.1843 - val_categorical_accuracy: 0.5110\n",
      "Epoch 230/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1822 - categorical_accuracy: 0.9433 - val_loss: 2.1735 - val_categorical_accuracy: 0.5180\n",
      "Epoch 231/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1804 - categorical_accuracy: 0.9433 - val_loss: 2.1477 - val_categorical_accuracy: 0.5220\n",
      "Epoch 232/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1851 - categorical_accuracy: 0.9380 - val_loss: 2.1317 - val_categorical_accuracy: 0.5160\n",
      "Epoch 233/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1723 - categorical_accuracy: 0.9442 - val_loss: 2.1635 - val_categorical_accuracy: 0.5250\n",
      "Epoch 234/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1694 - categorical_accuracy: 0.9505 - val_loss: 2.1687 - val_categorical_accuracy: 0.5150\n",
      "Epoch 235/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1735 - categorical_accuracy: 0.9503 - val_loss: 2.1384 - val_categorical_accuracy: 0.5270\n",
      "Epoch 236/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1736 - categorical_accuracy: 0.9465 - val_loss: 2.1615 - val_categorical_accuracy: 0.5150\n",
      "Epoch 237/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1723 - categorical_accuracy: 0.9445 - val_loss: 2.1483 - val_categorical_accuracy: 0.5200\n",
      "Epoch 238/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1613 - categorical_accuracy: 0.9572 - val_loss: 2.1336 - val_categorical_accuracy: 0.5190\n",
      "Epoch 239/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1632 - categorical_accuracy: 0.9480 - val_loss: 2.1039 - val_categorical_accuracy: 0.5300\n",
      "Epoch 240/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1636 - categorical_accuracy: 0.9503 - val_loss: 2.1261 - val_categorical_accuracy: 0.5350\n",
      "Epoch 241/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1631 - categorical_accuracy: 0.9513 - val_loss: 2.1811 - val_categorical_accuracy: 0.5270\n",
      "Epoch 242/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1570 - categorical_accuracy: 0.9572 - val_loss: 2.1470 - val_categorical_accuracy: 0.5270\n",
      "Epoch 243/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1567 - categorical_accuracy: 0.9560 - val_loss: 2.1555 - val_categorical_accuracy: 0.5140\n",
      "Epoch 244/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1590 - categorical_accuracy: 0.9548 - val_loss: 2.1503 - val_categorical_accuracy: 0.5320\n",
      "Epoch 245/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1559 - categorical_accuracy: 0.9538 - val_loss: 2.1838 - val_categorical_accuracy: 0.5110\n",
      "Epoch 246/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1552 - categorical_accuracy: 0.9570 - val_loss: 2.1210 - val_categorical_accuracy: 0.5230\n",
      "Epoch 247/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.1566 - categorical_accuracy: 0.9532 - val_loss: 2.1555 - val_categorical_accuracy: 0.5170\n",
      "Epoch 248/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1522 - categorical_accuracy: 0.9550 - val_loss: 2.1302 - val_categorical_accuracy: 0.5180\n",
      "Epoch 249/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1476 - categorical_accuracy: 0.9632 - val_loss: 2.1277 - val_categorical_accuracy: 0.5280\n",
      "Epoch 250/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1442 - categorical_accuracy: 0.9655 - val_loss: 2.1612 - val_categorical_accuracy: 0.5210\n",
      "Epoch 251/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1474 - categorical_accuracy: 0.9582 - val_loss: 2.1253 - val_categorical_accuracy: 0.5250\n",
      "Epoch 252/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1447 - categorical_accuracy: 0.9640 - val_loss: 2.1562 - val_categorical_accuracy: 0.5220\n",
      "Epoch 253/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1431 - categorical_accuracy: 0.9635 - val_loss: 2.1667 - val_categorical_accuracy: 0.5240\n",
      "Epoch 254/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1425 - categorical_accuracy: 0.9613 - val_loss: 2.1403 - val_categorical_accuracy: 0.5330\n",
      "Epoch 255/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1416 - categorical_accuracy: 0.9640 - val_loss: 2.1295 - val_categorical_accuracy: 0.5250\n",
      "Epoch 256/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1403 - categorical_accuracy: 0.9628 - val_loss: 2.1186 - val_categorical_accuracy: 0.5320\n",
      "Epoch 257/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1363 - categorical_accuracy: 0.9655 - val_loss: 2.1466 - val_categorical_accuracy: 0.5270\n",
      "Epoch 258/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1363 - categorical_accuracy: 0.9655 - val_loss: 2.1165 - val_categorical_accuracy: 0.5230\n",
      "Epoch 259/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1352 - categorical_accuracy: 0.9653 - val_loss: 2.0942 - val_categorical_accuracy: 0.5310\n",
      "Epoch 260/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1432 - categorical_accuracy: 0.9650 - val_loss: 2.1423 - val_categorical_accuracy: 0.5300\n",
      "Epoch 261/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1358 - categorical_accuracy: 0.9693 - val_loss: 2.1230 - val_categorical_accuracy: 0.5220\n",
      "Epoch 262/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1287 - categorical_accuracy: 0.9703 - val_loss: 2.0972 - val_categorical_accuracy: 0.5290\n",
      "Epoch 263/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1316 - categorical_accuracy: 0.9688 - val_loss: 2.1176 - val_categorical_accuracy: 0.5360\n",
      "Epoch 264/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1299 - categorical_accuracy: 0.9680 - val_loss: 2.1132 - val_categorical_accuracy: 0.5400\n",
      "Epoch 265/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1302 - categorical_accuracy: 0.9678 - val_loss: 2.1455 - val_categorical_accuracy: 0.5290\n",
      "Epoch 266/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1264 - categorical_accuracy: 0.9710 - val_loss: 2.0933 - val_categorical_accuracy: 0.5380\n",
      "Epoch 267/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1310 - categorical_accuracy: 0.9707 - val_loss: 2.1198 - val_categorical_accuracy: 0.5260\n",
      "Epoch 268/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1303 - categorical_accuracy: 0.9682 - val_loss: 2.1205 - val_categorical_accuracy: 0.5300\n",
      "Epoch 269/300\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.1267 - categorical_accuracy: 0.9705INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 1.1266 - categorical_accuracy: 0.9703 - val_loss: 2.0949 - val_categorical_accuracy: 0.5460\n",
      "Epoch 270/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1212 - categorical_accuracy: 0.9740 - val_loss: 2.0985 - val_categorical_accuracy: 0.5460\n",
      "Epoch 271/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1196 - categorical_accuracy: 0.9715 - val_loss: 2.1031 - val_categorical_accuracy: 0.5400\n",
      "Epoch 272/300\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.1324 - categorical_accuracy: 0.9705INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 1.1326 - categorical_accuracy: 0.9700 - val_loss: 2.0920 - val_categorical_accuracy: 0.5470\n",
      "Epoch 273/300\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.1207 - categorical_accuracy: 0.9733INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 1.1205 - categorical_accuracy: 0.9732 - val_loss: 2.0848 - val_categorical_accuracy: 0.5580\n",
      "Epoch 274/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1217 - categorical_accuracy: 0.9718 - val_loss: 2.1025 - val_categorical_accuracy: 0.5470\n",
      "Epoch 275/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1203 - categorical_accuracy: 0.9740 - val_loss: 2.1123 - val_categorical_accuracy: 0.5350\n",
      "Epoch 276/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1215 - categorical_accuracy: 0.9707 - val_loss: 2.0885 - val_categorical_accuracy: 0.5430\n",
      "Epoch 277/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1188 - categorical_accuracy: 0.9753 - val_loss: 2.1138 - val_categorical_accuracy: 0.5370\n",
      "Epoch 278/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1203 - categorical_accuracy: 0.9707 - val_loss: 2.1219 - val_categorical_accuracy: 0.5440\n",
      "Epoch 279/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.1223 - categorical_accuracy: 0.9722 - val_loss: 2.1040 - val_categorical_accuracy: 0.5400\n",
      "Epoch 280/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1119 - categorical_accuracy: 0.9728 - val_loss: 2.1092 - val_categorical_accuracy: 0.5360\n",
      "Epoch 281/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1172 - categorical_accuracy: 0.9762 - val_loss: 2.1117 - val_categorical_accuracy: 0.5500\n",
      "Epoch 282/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1176 - categorical_accuracy: 0.9718 - val_loss: 2.1202 - val_categorical_accuracy: 0.5460\n",
      "Epoch 283/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1196 - categorical_accuracy: 0.9735 - val_loss: 2.1118 - val_categorical_accuracy: 0.5370\n",
      "Epoch 284/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1119 - categorical_accuracy: 0.9758 - val_loss: 2.0920 - val_categorical_accuracy: 0.5490\n",
      "Epoch 285/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1095 - categorical_accuracy: 0.9790 - val_loss: 2.0827 - val_categorical_accuracy: 0.5440\n",
      "Epoch 286/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1167 - categorical_accuracy: 0.9732 - val_loss: 2.1018 - val_categorical_accuracy: 0.5400\n",
      "Epoch 287/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1080 - categorical_accuracy: 0.9812 - val_loss: 2.1054 - val_categorical_accuracy: 0.5470\n",
      "Epoch 288/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1097 - categorical_accuracy: 0.9765 - val_loss: 2.0804 - val_categorical_accuracy: 0.5450\n",
      "Epoch 289/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1111 - categorical_accuracy: 0.9772 - val_loss: 2.0867 - val_categorical_accuracy: 0.5440\n",
      "Epoch 290/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1114 - categorical_accuracy: 0.9758 - val_loss: 2.0951 - val_categorical_accuracy: 0.5510\n",
      "Epoch 291/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1082 - categorical_accuracy: 0.9780 - val_loss: 2.1041 - val_categorical_accuracy: 0.5420\n",
      "Epoch 292/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1115 - categorical_accuracy: 0.9770 - val_loss: 2.1099 - val_categorical_accuracy: 0.5490\n",
      "Epoch 293/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1078 - categorical_accuracy: 0.9793 - val_loss: 2.1042 - val_categorical_accuracy: 0.5440\n",
      "Epoch 294/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1136 - categorical_accuracy: 0.9740 - val_loss: 2.0764 - val_categorical_accuracy: 0.5450\n",
      "Epoch 295/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1077 - categorical_accuracy: 0.9780 - val_loss: 2.0918 - val_categorical_accuracy: 0.5500\n",
      "Epoch 296/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1004 - categorical_accuracy: 0.9803 - val_loss: 2.0800 - val_categorical_accuracy: 0.5460\n",
      "Epoch 297/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1067 - categorical_accuracy: 0.9790 - val_loss: 2.0751 - val_categorical_accuracy: 0.5530\n",
      "Epoch 298/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1052 - categorical_accuracy: 0.9810 - val_loss: 2.0948 - val_categorical_accuracy: 0.5560\n",
      "Epoch 299/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1058 - categorical_accuracy: 0.9760 - val_loss: 2.0948 - val_categorical_accuracy: 0.5490\n",
      "Epoch 300/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1058 - categorical_accuracy: 0.9758 - val_loss: 2.0758 - val_categorical_accuracy: 0.5530\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁▂▃▃▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▆▆▇▇▇▇▇▇████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁████▇▇▆▆▅▅▄▄▃▃▂▂▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▇▆▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁▂▅▅▇▇▇▇▇▇▇▇▇▇█▇▇██▇▇▇▇▇▇▇▇▇██▇▇█▇██████</td></tr><tr><td>epoch/val_loss</td><td>█▆▃▂▁▁▁▂▂▂▃▂▃▃▃▃▃▃▄▄▅▃▄▄▄▅▄▅▅▄▅▅▅▅▄▄▄▄▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.97575</td></tr><tr><td>epoch/epoch</td><td>299</td></tr><tr><td>epoch/learning_rate</td><td>5e-05</td></tr><tr><td>epoch/loss</td><td>1.10578</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.553</td></tr><tr><td>epoch/val_loss</td><td>2.07578</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Transformer-embed=128-heads=2-ff=128-D=0.2-Dense=256-LipsEyesHandsPose_5k</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/u544yx92' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/u544yx92</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240416_154118-u544yx92/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f94783909d0>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dim = 64  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 128  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = layers.Input(shape=X_shape)\n",
    "embedding_layer = TokenAndPositionEmbedding(X_shape[0], X_shape[1])\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(X_shape[1], num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(256, activation=\"gelu\")(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "outputs = Dense(len(train_dataset_parquet.unique_labels), activation='softmax')(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", f\"Transformer-embed={embed_dim}-heads={num_heads}-ff={ff_dim}-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf\"),\n",
    "            run_name=\"Transformer-embed=128-heads=2-ff=128-D=0.2-Dense=256-LipsEyesHandsPose_5k\",\n",
    "            mediapipe_features=\"reduced-LipsEyesHandsPose\",\n",
    "            T_max=151, epochs=300, \n",
    "            max_lr = 3e-4, min_lr = 5e-5,\n",
    "            USE_WANDB=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 5087314,
     "sourceId": 46105,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
