{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.0)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.14.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.16.6)\n",
      "Collecting tdqm\n",
      "  Downloading tdqm-0.0.1.tar.gz (1.4 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting wurlitzer\n",
      "  Obtaining dependency information for wurlitzer from https://files.pythonhosted.org/packages/28/69/9d913bc2f85305c4eaf078fb22fd4828182b33189c1e78f2256ae27eaacb/wurlitzer-3.0.3-py3-none-any.whl.metadata\n",
      "  Downloading wurlitzer-3.0.3-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.21.0)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (16.0.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.8.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.58.0)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.14.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.14.0)\n",
      "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.14.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (1.45.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.11/dist-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from tdqm) (4.66.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (8.2.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.23.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.3.7)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.0)\n",
      "Downloading wurlitzer-3.0.3-py3-none-any.whl (7.3 kB)\n",
      "Building wheels for collected packages: tdqm\n",
      "  Building wheel for tdqm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tdqm: filename=tdqm-0.0.1-py3-none-any.whl size=1322 sha256=a64e676cf68103df5bd260c1933cb57c409a9e85893f2b4ed630a623b2f71b11\n",
      "  Stored in directory: /root/.cache/pip/wheels/c8/c7/30/e5935be2cfa6883be72462333edc414d8928f3c78eaabec38a\n",
      "Successfully built tdqm\n",
      "Installing collected packages: wurlitzer, tdqm\n",
      "Successfully installed tdqm-0.0.1 wurlitzer-3.0.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python numpy tensorflow scikit-learn matplotlib wandb tdqm wurlitzer pandas plotly pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T22:50:12.410001Z",
     "start_time": "2024-04-21T22:50:12.399760Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 21:38:58.605415: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-22 21:38:58.605468: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-22 21:38:58.605490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-22 21:38:58.612715: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#Load parquet data into dataset_parquet for training.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.regularizers import l2\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import random\n",
    "import time\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T22:50:12.681574Z",
     "start_time": "2024-04-21T22:50:12.676228Z"
    }
   },
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    # root = os.path.join(\"/\", \"kaggle\", \"input\", \"asl-signs\") \n",
    "    # root = os.path.join(\".\")\n",
    "    root = os.path.join(\".\")\n",
    "    DATA_LIMIT = 1000\n",
    "    BATCH_SIZE = 32\n",
    "    VIDEO_LENGTH = 25\n",
    "    TRAIN_VAL_SPLIT = 0.8\n",
    "    WANDB_RUN = \"mediapipe-asl-dataset\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T22:50:13.034956Z",
     "start_time": "2024-04-21T22:50:13.018927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPS :  40\n",
      "EYE_LEFT :  20\n",
      "EYE_RIGHT :  20\n",
      "LEFT_HAND :  21\n",
      "RIGHT_HAND :  21\n",
      "LEFT_POSE :  5\n",
      "RIGHT_POSE :  5\n",
      "132\n"
     ]
    }
   ],
   "source": [
    "LIPS_IDXS0 = np.array([\n",
    "        61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "        291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "        78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "        95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "    ])\n",
    "\n",
    "\n",
    "EYE_LEFT = np.array([33, 7, 246, 161, 160, 159, 158, 157, 173, 133, 155, 154, 153, 145, 144, 163, 471, 470, 469, 472])\n",
    "\n",
    "\n",
    "EYE_RIGHT = np.array([362, 398, 384, 385, 386, 387, 388, 466, 263, 249, 390, 373, 374, 380, 381, 382, 476, 475, 474, 477])\n",
    "# Landmark indices in original data\n",
    "LEFT_HAND_IDXS0 = np.arange(468,489)\n",
    "RIGHT_HAND_IDXS0 = np.arange(522,543)\n",
    "LEFT_POSE_IDXS0 = np.array([502, 504, 506, 508, 510])\n",
    "RIGHT_POSE_IDXS0 = np.array([503, 505, 507, 509, 511])\n",
    "\n",
    "\n",
    "LIPS_END = len(LIPS_IDXS0)\n",
    "EYE_LEFT_END = LIPS_END + len(EYE_LEFT)\n",
    "EYE_RIGHT_END = EYE_LEFT_END + len(EYE_RIGHT)\n",
    "LEFT_HAND_END = EYE_RIGHT_END + len(LEFT_HAND_IDXS0)\n",
    "RIGHT_HAND_END = LEFT_HAND_END + len(RIGHT_HAND_IDXS0)\n",
    "LEFT_POSE_END = RIGHT_HAND_END + len(LEFT_POSE_IDXS0)\n",
    "RIGHT_POSE_END = LEFT_POSE_END + len(RIGHT_POSE_IDXS0)\n",
    "\n",
    "print(\"LIPS : \",len(LIPS_IDXS0))\n",
    "print(\"EYE_LEFT : \",len(EYE_LEFT))\n",
    "print(\"EYE_RIGHT : \",len(EYE_RIGHT))\n",
    "print(\"LEFT_HAND : \",len(LEFT_HAND_IDXS0))\n",
    "print(\"RIGHT_HAND : \",len(RIGHT_HAND_IDXS0))\n",
    "print(\"LEFT_POSE : \",len(LEFT_POSE_IDXS0))\n",
    "print(\"RIGHT_POSE : \",len(RIGHT_POSE_IDXS0))\n",
    "\n",
    "all_selection = np.concatenate([LIPS_IDXS0, EYE_LEFT, EYE_RIGHT, LEFT_HAND_IDXS0, RIGHT_HAND_IDXS0, LEFT_POSE_IDXS0, RIGHT_POSE_IDXS0])\n",
    "print(len(all_selection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T22:50:13.367202Z",
     "start_time": "2024-04-21T22:50:13.361465Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_mean (values, mean, cnt):\n",
    "    if np.any(np.isnan(values)):\n",
    "        return mean, cnt\n",
    "    \n",
    "    mean += np.sum(values, axis=0)\n",
    "    cnt += values.shape[0]\n",
    "    return mean, cnt\n",
    "\n",
    "def update_std(values, mean, std):\n",
    "    if np.any(np.isnan(values)):\n",
    "        return std\n",
    "    \n",
    "    std += np.sum((values - mean)**2, axis=0)\n",
    "    return std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T22:50:13.838712Z",
     "start_time": "2024-04-21T22:50:13.830027Z"
    }
   },
   "outputs": [],
   "source": [
    "#this code sorts out a parquet files and rearrange the order to pose,face, left-hand, right-hand\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import plotly.graph_objs as go\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.io as pio\n",
    "\n",
    "def visualize_keypoints(frames : np.ndarray, point_size : int):\n",
    "    if len(frames.shape) == 1:\n",
    "        frames = np.array([frames])\n",
    "    \n",
    "    for frame in frames:\n",
    "        frame = frame.reshape(-1, 3)\n",
    "        sizes = point_size * np.ones(frame.shape[0])\n",
    "\n",
    "        fig = go.Figure(data=go.Scatter(x=frame[:,0], y=2.5 - frame[:,1], mode='markers',\n",
    "                                        marker=dict(\n",
    "                                            size=sizes\n",
    "                                            )))\n",
    "\n",
    "    # Customize the layout\n",
    "    fig.update_layout(title='visualization of human keypoints',\n",
    "                        xaxis_title='',\n",
    "                        yaxis_title='',\n",
    "                        width=1000,\n",
    "                        height=1600)\n",
    "\n",
    "    fig.update_xaxes(range=[-0.2, 1.4])  # Set x-axis range from 0 to 6\n",
    "    fig.update_yaxes(range=[0, 2.5])  # Set y-axis range from 10 to 20\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "\n",
    "def process_parquet2(ds, idxes = None):\n",
    "    ret = []    \n",
    "    frame_size = 543\n",
    "    it = len(ds) // frame_size\n",
    "    assert it == len(ds) / frame_size\n",
    "    \n",
    "    for i in range(it):\n",
    "        vals = ds.iloc[ i * frame_size : (i + 1 ) * frame_size ]        \n",
    "        \n",
    "        if idxes is not None:          \n",
    "            vals = ds.iloc[idxes]\n",
    "                        \n",
    "        ret.append(np.array(vals[[\"x\",\"y\", \"z\"]]))\n",
    "        \n",
    "    return np.array(np.array(ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T22:51:16.333562Z",
     "start_time": "2024-04-21T22:50:14.345658Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94477/94477 [00:00<00:00, 303714.14it/s]\n",
      "Cacheing: 100%|██████████| 800/800 [00:17<00:00, 45.60it/s]\n",
      "calculating means: 100%|██████████| 800/800 [00:01<00:00, 550.71it/s]\n",
      "calculating std: 100%|██████████| 800/800 [00:01<00:00, 470.88it/s]\n",
      "100%|██████████| 94477/94477 [00:00<00:00, 302286.50it/s]\n",
      "Cacheing: 100%|██████████| 200/200 [00:04<00:00, 48.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cardinality of train : 800, cardinality of validation : 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class Groups:\n",
    "    def __init__(self):\n",
    "        self.starts = []\n",
    "        self.id_2_label = {}\n",
    "        self.means = []\n",
    "        self.cnts = []\n",
    "        self.stds = []\n",
    "\n",
    "    def add(self, start, name):\n",
    "        self.id_2_label[len(self.starts)] = name\n",
    "        self.starts.append(start)\n",
    "        self.means.append(np.array([0.0,0.0,0.0]))\n",
    "        self.cnts.append(0)\n",
    "        self.stds.append(np.array([0.0,0.0,0.0]))\n",
    "        \n",
    "    def reset(self):\n",
    "        for i in range(len(self.means)):\n",
    "            self.means[i] = np.array([0,0,0])\n",
    "            self.cnts[i] = 0\n",
    "            self.stds[i] = np.array([0,0,0])\n",
    "            \n",
    "    def __str__(self):\n",
    "        s = \"\"\n",
    "        for i in range(len(self.starts)):\n",
    "            s += f\"name={self.id_2_label[i]} idxes = ({self.starts[i][0]}, {self.starts[i][1]}), mu={self.means[i]}, std = {self.stds[i]}\\n\"\n",
    "        \n",
    "        return s\n",
    "    \n",
    "#custom class to load data from Parquet files for training ML models.\n",
    "class ParquetDatasetCached(keras.utils.Sequence):\n",
    "    def __init__(self, dataset_folder, csv_file : str, groups : Groups=None, batch_size=CONFIG.BATCH_SIZE, \n",
    "                 data_limit :int= CONFIG.DATA_LIMIT, check_if_file_exists = True, \n",
    "                 frame_length :int = CONFIG.VIDEO_LENGTH, split : str = \"train\", \n",
    "                 train_val_split : float = CONFIG.TRAIN_VAL_SPLIT, sort_by_counts : bool = True, \n",
    "                 idxes=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        #taking keras sequence for .fit(), .evaluate(), .predict() methods\n",
    "        #load csv - it has the path to parquet file, and another to store label\n",
    "        self.cached_Y = None\n",
    "        self.cached_X = None\n",
    "        self.csv_path = csv_file\n",
    "        self.root_folder = dataset_folder\n",
    "        self.batch_size = batch_size\n",
    "        #optional pre-processing function to the parquet files.\n",
    "        self.group = groups\n",
    "        \n",
    "        self.csv_data = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        self.all_files = []\n",
    "        self.not_exists = []\n",
    "        self.frame_length = frame_length\n",
    "\n",
    "        for path, label in tqdm(list(zip(self.csv_data[\"path\"], self.csv_data[\"sign\"]))):\n",
    "            prop_path = os.path.join(self.root_folder, path)\n",
    "            \n",
    "            if check_if_file_exists:\n",
    "                if os.path.exists(prop_path):\n",
    "                    self.all_files.append((prop_path, label))\n",
    "                else:\n",
    "                    self.not_exists.append(prop_path)\n",
    "            else:\n",
    "                self.all_files.append((prop_path, label))\n",
    "                \n",
    "                    \n",
    "        self.all_files = np.array(self.all_files)\n",
    "        self.unique_labels = np.unique(self.all_files[:, 1])\n",
    "        self.label_2_id = { key : i for i, key in enumerate(self.unique_labels)}\n",
    "    \n",
    "        # sort the values by popularity\n",
    "        if sort_by_counts:\n",
    "            cnt = Counter(self.all_files[:, 1])\n",
    "            vals = []\n",
    "            \n",
    "            for i,row in enumerate(self.all_files):\n",
    "                vals.append((int(1e6 * cnt[row[1]] + self.label_2_id [row[1]]),i))\n",
    "            \n",
    "            vals = np.array(sorted(vals)[::-1])\n",
    "            self.all_files = self.all_files[vals[:,1]]\n",
    "\n",
    "        \n",
    "        if data_limit < 0:\n",
    "            train_ds, val_ds = train_test_split(self.all_files, train_size=train_val_split, random_state=42)\n",
    "        else:\n",
    "            train_ds, val_ds = train_test_split(self.all_files[:data_limit], train_size=train_val_split, random_state=42)\n",
    "            self.unique_labels = np.unique(self.all_files[:data_limit, 1])\n",
    "            self.label_2_id = { key : i for i, key in enumerate(self.unique_labels)}\n",
    "            \n",
    "        if split.lower() == \"train\":\n",
    "            self.dataset = train_ds\n",
    "            \n",
    "        elif split.lower() == \"val\":\n",
    "            self.dataset = val_ds \n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"please specify split to be either train or val\")\n",
    "            \n",
    "        self.cache_data(idxes)\n",
    "        self.calculate_props_for_groups()\n",
    "                   \n",
    "    def getGroups(self):\n",
    "        return self.group\n",
    "    \n",
    "    def select(self, frames):\n",
    "        current_length, num_features, _ = frames.shape\n",
    "    \n",
    "        if current_length >= CONFIG.VIDEO_LENGTH:\n",
    "                # TODO: a better than uniform value ? Could place gaussian in the middle\n",
    "                random_start = random.randint(0, current_length - CONFIG.VIDEO_LENGTH)\n",
    "                return frames[random_start : (random_start + CONFIG.VIDEO_LENGTH)]\n",
    "            \n",
    "        # padd the video to contain zeros \n",
    "        return np.concatenate([frames, np.zeros((CONFIG.VIDEO_LENGTH - current_length, num_features,3))], axis=0)\n",
    "\n",
    "    def preprocess(self, raw_frames):\n",
    "        select_frames = self.select(raw_frames)\n",
    "        t = select_frames.shape[0]\n",
    "        \n",
    "        for i in range(t):   \n",
    "            for j in range(len(self.group.starts)):\n",
    "                start = self.group.starts[j] \n",
    "                mean, std = self.group.means[j], self.group.stds[j]\n",
    "                region = select_frames[i, start[0] : start[1], :]                \n",
    "                region = (region - mean) / std\n",
    "                region = np.nan_to_num(region)                \n",
    "                select_frames[i, start[0] : start[1]] = region\n",
    "                \n",
    "                \n",
    "        return select_frames.reshape(t, -1)\n",
    "        \n",
    "            \n",
    "    def cache_data(self, idxes):\n",
    "        self.cached_X, self.cached_Y = [], []\n",
    "        \n",
    "        pb = tqdm(range(len(self.dataset)), desc=\"Cacheing\")\n",
    "\n",
    "        for i in pb:\n",
    "            \n",
    "            path, label = self.dataset[i]\n",
    "            df = pd.read_parquet(path)\n",
    "                        \n",
    "            one_hot_encoded_label = np.zeros(len(self.unique_labels))\n",
    "            one_hot_encoded_label[self.label_2_id[label]] = 1\n",
    "\n",
    "            self.cached_X.append(process_parquet2(df, idxes=idxes)) \n",
    "            self.cached_Y.append(one_hot_encoded_label)     \n",
    "            \n",
    "        \n",
    "    def calculate_props_for_groups(self):\n",
    "        \n",
    "        if self.group is None:\n",
    "            return \n",
    "                            \n",
    "        for frames in tqdm(self.cached_X, desc=\"calculating means\"):\n",
    "            for frame in frames:\n",
    "                for i in range(len(self.group.starts)):\n",
    "                    start = self.group.starts[i]\n",
    "                    mean = self.group.means[i]\n",
    "                    cnts = self.group.cnts[i]\n",
    "                    \n",
    "                    feature = frame[start[0]:start[1]]\n",
    "                    \n",
    "                    self.group.means[i], self.group.cnts[i] = update_mean(feature, mean, cnts)                   \n",
    " \n",
    "        \n",
    "        # set the means \n",
    "        for i in range(len(self.group.starts)):            \n",
    "            self.group.means[i] = self.group.means[i] / self.group.cnts[i]\n",
    "            \n",
    "        # calculate the standard deviations\n",
    "        for frames in tqdm(self.cached_X, desc=\"calculating std\"):\n",
    "            for frame in frames:\n",
    "                for i in range(len(self.group.starts)):\n",
    "                    start = self.group.starts[i]\n",
    "                    std = self.group.stds[i]\n",
    "                    mean = self.group.means[i]                    \n",
    "                    feature = frame[start[0]:start[1]]\n",
    "                    \n",
    "                    self.group.stds[i] = update_std(feature, mean=mean, std=std)                   \n",
    " \n",
    "        # set the standard deviations \n",
    "        for i in range(len(self.group.starts)):            \n",
    "            self.group.stds[i] = np.sqrt(self.group.stds[i] / self.group.cnts[i])\n",
    "  \n",
    "    def set_group(self, group):\n",
    "        self.group = group\n",
    "\n",
    "    def __len__(self):\n",
    "        # Assuming each Parquet file should be one batch; adjust if necessary\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.preprocess(self.cached_X[idx]), self.cached_Y[idx]                \n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        # Shuffle files for the next epoch\n",
    "        np.random.shuffle(self.dataset)\n",
    "\n",
    "\n",
    "    \n",
    "groups = Groups()\n",
    "groups.add((0, LIPS_END), \"lips\")\n",
    "groups.add((LIPS_END, EYE_LEFT_END), \"left eye\")\n",
    "groups.add((EYE_LEFT_END, EYE_RIGHT_END), \"right eye\")\n",
    "groups.add((EYE_RIGHT_END, LEFT_HAND_END), \"left hand\")\n",
    "groups.add((LEFT_HAND_END, RIGHT_HAND_END), \"right hand\")\n",
    "groups.add((RIGHT_HAND_END, LEFT_POSE_END), \"left pose\")\n",
    "groups.add((LEFT_POSE_END, RIGHT_POSE_END), \"right pose\")\n",
    "\n",
    "# Usage example\n",
    "parquet_folder_path = CONFIG.root\n",
    "train_dataset_parquet = ParquetDatasetCached(parquet_folder_path, csv_file = os.path.join(CONFIG.root, \"train.csv\"), \n",
    "                                 batch_size=CONFIG.BATCH_SIZE, data_limit=CONFIG.DATA_LIMIT, groups=groups,\n",
    "                                             check_if_file_exists = True,\n",
    "                                             split=\"train\", idxes= all_selection)\n",
    "\n",
    "val_dataset_parquet = ParquetDatasetCached(parquet_folder_path, csv_file = os.path.join(CONFIG.root, \"train.csv\"), \n",
    "                                 batch_size=CONFIG.BATCH_SIZE, data_limit=CONFIG.DATA_LIMIT,\n",
    "                                 check_if_file_exists= True,\n",
    "                                 split=\"val\",idxes= all_selection)\n",
    "val_dataset_parquet.set_group(train_dataset_parquet.group)\n",
    "\n",
    "print(f\"cardinality of train : {len(train_dataset_parquet)}, cardinality of validation : {len(val_dataset_parquet)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T23:05:38.256734Z",
     "start_time": "2024-04-21T23:05:35.299481Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:02<00:00, 355.71it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 398.13it/s]\n"
     ]
    }
   ],
   "source": [
    "nans = False\n",
    "\n",
    "for x, y in tqdm(train_dataset_parquet):\n",
    "    nans |= np.any(np.isnan(x))\n",
    "    if nans:\n",
    "        print(\"SAD : \")\n",
    "        break\n",
    "\n",
    "for x, y in tqdm(val_dataset_parquet):\n",
    "    nans |= np.any(np.isnan(x))\n",
    "    if nans:    \n",
    "        print(\"SAD : \")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T19:24:45.534076Z",
     "start_time": "2024-04-21T19:24:45.527158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name=lips idxes = (0, 40), mu=[ 0.47021672  0.4764824  -0.02343413], std = [0.07918408 0.07547071 0.01623904]\n",
      "name=left eye idxes = (40, 60), mu=[0.3986628  0.43055143 0.01069527], std = [0.14245859 0.14136499 0.03225427]\n",
      "name=right eye idxes = (60, 80), mu=[ 0.51451116  0.40910735 -0.00128009], std = [0.10255321 0.12820712 0.04453863]\n",
      "name=left hand idxes = (80, 101), mu=[ 0.6498931   0.62196375 -0.05204559], std = [0.15224076 0.16998623 0.06532311]\n",
      "name=right hand idxes = (101, 122), mu=[ 0.30557308  0.58527081 -0.05768258], std = [0.1381897  0.15819383 0.05942085]\n",
      "name=left pose idxes = (122, 127), mu=[ 0.87496043  0.95066813 -1.35376196], std = [0.17851542 0.27971058 0.70780739]\n",
      "name=right pose idxes = (127, 132), mu=[ 0.11753856  0.93380588 -1.59372785], std = [0.1639152  0.31182161 0.97429655]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T19:23:14.816736Z",
     "start_time": "2024-04-21T19:23:14.563937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 122 127\n",
      "len(frames) = 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "i = 1\n",
    "LIPS_END = len(LIPS_IDXS0)\n",
    "EYE_LEFT_END = LIPS_END + len(EYE_LEFT)\n",
    "EYE_RIGHT_END = EYE_LEFT_END + len(EYE_RIGHT)\n",
    "LEFT_HAND_END = EYE_RIGHT_END + len(LEFT_HAND_IDXS0)\n",
    "RIGHT_HAND_END = LEFT_HAND_END + len(RIGHT_HAND_IDXS0)\n",
    "LEFT_POSE_END = RIGHT_HAND_END + len(LEFT_POSE_IDXS0)\n",
    "RIGHT_POSE_END = LEFT_POSE_END + len(RIGHT_POSE_IDXS0)\n",
    "\n",
    "print(LEFT_HAND_END, RIGHT_HAND_END,LEFT_POSE_END)\n",
    "\n",
    "frames = train_dataset_parquet.cached_X[i]\n",
    "print(f\"len(frames) = {len(frames)}\")\n",
    "\n",
    "frame = train_dataset_parquet.cached_X[0][0]\n",
    "\n",
    "plt.scatter(frame[:LIPS_END, 0], frame[:LIPS_END, 1], label=\"lips\")\n",
    "plt.scatter(frame[LIPS_END:EYE_LEFT_END, 0], frame[LIPS_END:EYE_LEFT_END, 1], label=\"eye left\")\n",
    "plt.scatter(frame[EYE_LEFT_END:EYE_RIGHT_END,0], frame[EYE_LEFT_END:EYE_RIGHT_END, 1], label=\"eye right\")\n",
    "plt.scatter(frame[EYE_RIGHT_END:LEFT_HAND_END,0], frame[EYE_RIGHT_END:LEFT_HAND_END, 1], label=\"left hand\")\n",
    "plt.scatter(frame[LEFT_HAND_END:RIGHT_HAND_END, 0], frame[LEFT_HAND_END:RIGHT_HAND_END, 1], label=\"right hand\")\n",
    "plt.scatter(frame[RIGHT_HAND_END:LEFT_POSE_END, 0], frame[RIGHT_HAND_END:LEFT_POSE_END, 1], label=\"left pose\")\n",
    "plt.scatter(frame[LEFT_POSE_END:RIGHT_POSE_END, 0], frame[LEFT_POSE_END:RIGHT_POSE_END, 1], label=\"right pose\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dataset_parquet))\n",
    "X_shape, Y_shape = x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 21:39:31.407700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 180 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:04:00.0, compute capability: 8.6\n",
      "2024-04-22 21:39:31.423633: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 180.94MiB (189726720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n"
     ]
    }
   ],
   "source": [
    "def dataset_train_generator():\n",
    "    # Instantiate your existing dataset loader\n",
    "\n",
    "    for i in range(len(train_dataset_parquet)):\n",
    "        X_batch, Y_batch = train_dataset_parquet[i]\n",
    "        yield X_batch, Y_batch\n",
    "        \n",
    "def dataset_val_generator():\n",
    "    # Instantiate your existing dataset loader\n",
    "\n",
    "    for i in range(len(val_dataset_parquet)):\n",
    "        X_batch, Y_batch = val_dataset_parquet[i]\n",
    "        yield X_batch, Y_batch        \n",
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: dataset_train_generator(),\n",
    "    output_types=(tf.float32, tf.float32),  # Adjust types based on your actual data\n",
    "    output_shapes=(X_shape, Y_shape)\n",
    ").prefetch(tf.data.AUTOTUNE).batch(CONFIG.BATCH_SIZE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: dataset_val_generator(),\n",
    "    output_types=(tf.float32, tf.float32),  # Adjust types based on your actual data\n",
    "    output_shapes=(X_shape, Y_shape)\n",
    ").prefetch(tf.data.AUTOTUNE).batch(CONFIG.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-1.23355842e+00 -1.24636483e+00  4.58735704e+00 ... -3.36595982e-01\n",
      "    9.28819716e-01  6.78189099e-01]\n",
      "  [-1.23355842e+00 -1.24636483e+00  4.58735704e+00 ... -3.36595982e-01\n",
      "    9.28819716e-01  6.78189099e-01]\n",
      "  [-1.23355842e+00 -1.24636483e+00  4.58735704e+00 ... -3.36595982e-01\n",
      "    9.28819716e-01  6.78189099e-01]\n",
      "  ...\n",
      "  [-5.93827391e+00 -6.31347466e+00  1.44307327e+00 ... -7.17069268e-01\n",
      "   -2.99467969e+00  1.63577282e+00]\n",
      "  [-5.93827391e+00 -6.31347466e+00  1.44307327e+00 ... -7.17069268e-01\n",
      "   -2.99467969e+00  1.63577282e+00]\n",
      "  [-5.93827391e+00 -6.31347466e+00  1.44307327e+00 ... -7.17069268e-01\n",
      "   -2.99467969e+00  1.63577282e+00]]\n",
      "\n",
      " [[-3.15966034e+00 -1.26942301e+00  9.81657624e-01 ... -7.41855353e-02\n",
      "    4.83052999e-01  5.50546765e-01]\n",
      "  [-3.15966034e+00 -1.26942301e+00  9.81657624e-01 ... -7.41855353e-02\n",
      "    4.83052999e-01  5.50546765e-01]\n",
      "  [-3.15966034e+00 -1.26942301e+00  9.81657624e-01 ... -7.41855353e-02\n",
      "    4.83052999e-01  5.50546765e-01]\n",
      "  ...\n",
      "  [-5.93827391e+00 -6.31347466e+00  1.44307327e+00 ... -7.17069268e-01\n",
      "   -2.99467969e+00  1.63577282e+00]\n",
      "  [-5.93827391e+00 -6.31347466e+00  1.44307327e+00 ... -7.17069268e-01\n",
      "   -2.99467969e+00  1.63577282e+00]\n",
      "  [-5.93827391e+00 -6.31347466e+00  1.44307327e+00 ... -7.17069268e-01\n",
      "   -2.99467969e+00  1.63577282e+00]]\n",
      "\n",
      " [[-1.25689852e+00 -7.33272493e-01  9.52616036e-01 ... -4.28875595e-01\n",
      "    9.70337868e-01  1.02314067e+00]\n",
      "  [-1.25689852e+00 -7.33272493e-01  9.52616036e-01 ... -4.28875595e-01\n",
      "    9.70337868e-01  1.02314067e+00]\n",
      "  [-1.25689852e+00 -7.33272493e-01  9.52616036e-01 ... -4.28875595e-01\n",
      "    9.70337868e-01  1.02314067e+00]\n",
      "  ...\n",
      "  [-5.93827391e+00 -6.31347466e+00  1.44307327e+00 ... -7.17069268e-01\n",
      "   -2.99467969e+00  1.63577282e+00]\n",
      "  [-5.93827391e+00 -6.31347466e+00  1.44307327e+00 ... -7.17069268e-01\n",
      "   -2.99467969e+00  1.63577282e+00]\n",
      "  [-5.93827391e+00 -6.31347466e+00  1.44307327e+00 ... -7.17069268e-01\n",
      "   -2.99467969e+00  1.63577282e+00]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-7.42046461e-02 -3.06541383e-01  1.83280718e+00 ... -6.71736479e-01\n",
      "    5.53284109e-01 -1.11003947e+00]\n",
      "  [-7.42046461e-02 -3.06541383e-01  1.83280718e+00 ... -6.71736479e-01\n",
      "    5.53284109e-01 -1.11003947e+00]\n",
      "  [-7.42046461e-02 -3.06541383e-01  1.83280718e+00 ... -6.71736479e-01\n",
      "    5.53284109e-01 -1.11003947e+00]\n",
      "  ...\n",
      "  [-5.93827391e+00 -6.31347466e+00  1.44307327e+00 ... -7.17069268e-01\n",
      "   -2.99467969e+00  1.63577282e+00]\n",
      "  [-5.93827391e+00 -6.31347466e+00  1.44307327e+00 ... -7.17069268e-01\n",
      "   -2.99467969e+00  1.63577282e+00]\n",
      "  [-5.93827391e+00 -6.31347466e+00  1.44307327e+00 ... -7.17069268e-01\n",
      "   -2.99467969e+00  1.63577282e+00]]\n",
      "\n",
      " [[-3.95514488e+00 -3.12503862e+00  8.06330795e+01 ...  5.18421030e+00\n",
      "   -6.05567551e+00  7.63751090e-01]\n",
      "  [-3.95514488e+00 -3.12503862e+00  8.06330795e+01 ...  5.18421030e+00\n",
      "   -6.05567551e+00  7.63751090e-01]\n",
      "  [-3.95514488e+00 -3.12503862e+00  8.06330795e+01 ...  5.18421030e+00\n",
      "   -6.05567551e+00  7.63751090e-01]\n",
      "  ...\n",
      "  [ 1.57032236e-01  2.40633518e-01  1.28596997e+00 ...  9.67309415e-01\n",
      "   -9.54484642e-01 -8.49607825e-01]\n",
      "  [ 1.57032236e-01  2.40633518e-01  1.28596997e+00 ...  9.67309415e-01\n",
      "   -9.54484642e-01 -8.49607825e-01]\n",
      "  [ 1.57032236e-01  2.40633518e-01  1.28596997e+00 ...  9.67309415e-01\n",
      "   -9.54484642e-01 -8.49607825e-01]]\n",
      "\n",
      " [[ 9.16534141e-02 -5.63441515e-01  1.63508618e+00 ...  2.12966132e+00\n",
      "   -1.02243626e+00 -8.69952619e-01]\n",
      "  [ 9.16534141e-02 -5.63441515e-01  1.63508618e+00 ...  2.12966132e+00\n",
      "   -1.02243626e+00 -8.69952619e-01]\n",
      "  [ 9.16534141e-02 -5.63441515e-01  1.63508618e+00 ...  2.12966132e+00\n",
      "   -1.02243626e+00 -8.69952619e-01]\n",
      "  ...\n",
      "  [-4.78080082e+00 -1.37791729e+01  1.02131645e+02 ...  1.22753887e+01\n",
      "   -6.27359390e+00  7.42869496e-01]\n",
      "  [-4.78080082e+00 -1.37791729e+01  1.02131645e+02 ...  1.22753887e+01\n",
      "   -6.27359390e+00  7.42869496e-01]\n",
      "  [-4.78080082e+00 -1.37791729e+01  1.02131645e+02 ...  1.22753887e+01\n",
      "   -6.27359390e+00  7.42869496e-01]]], shape=(32, 25, 396), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_dataset:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  8.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 25, 396) (32, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02,  9.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating through dataset took : 2.5526s\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time.time()\n",
    "isnans =False\n",
    "\n",
    "id2label = {v : k for k,v in train_dataset_parquet.label_2_id.items()}\n",
    "\n",
    "f = True\n",
    "labels_batches = []\n",
    "for el in tqdm(train_dataset):\n",
    "    if f:\n",
    "        print(el[0].shape, el[1].shape)\n",
    "        f = False\n",
    "\n",
    "    ls = np.argmax(el[1], axis=1)\n",
    "    for l in ls:\n",
    "        labels_batches.append(id2label[l])\n",
    "    \n",
    "    isnans |= np.any(np.isnan(el[0]))\n",
    "    if isnans:\n",
    "        print(\"FOUND NAN!\")\n",
    "        break\n",
    "\n",
    "print(f\"Iterating through dataset took : {round( time.time() - start , 4)}s\")\n",
    "plt.hist(labels_batches, bins=len(np.unique(labels_batches)), edgecolor=\"black\")\n",
    "plt.xlabel('labels')\n",
    "plt.ylabel('counts')\n",
    "plt.title('')\n",
    "\n",
    "# Rotate x-axis ticks\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig(\"histogram.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, LeakyReLU\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras import backend as K\n",
    "from wandb.keras import WandbMetricsLogger\n",
    "from keras.callbacks import Callback\n",
    "import tensorflow as tf\n",
    "import wandb\n",
    "\n",
    "\n",
    "class CosineAnnealingLearningRateScheduler(Callback):\n",
    "    def __init__(self, max_lr, min_lr, T_max):\n",
    "        super(CosineAnnealingLearningRateScheduler, self).__init__()\n",
    "        self.max_lr = max_lr  # Maximum learning rate (i.e., start learning rate)\n",
    "        self.min_lr = min_lr  # Minimum learning rate\n",
    "        self.T_max = T_max    # Specifies the number of epochs per cycle\n",
    "        self.t = 0            # Current epoch\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.t += 1\n",
    "        cos = np.cos(np.pi * (self.t % self.T_max) / self.T_max)\n",
    "        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + cos)\n",
    "\n",
    "        keras.backend.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "def keras_train(model, filepath : str, max_lr = 1e-4, min_lr = 5e-5, T_max=51, epochs=100, run_name=\"\",\n",
    "                mediapipe_features = \"all\", USE_WANDB=True): \n",
    "    \n",
    "    \n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
    "                                                 monitor=\"val_categorical_accuracy\",\n",
    "                                                 verbose=0,\n",
    "                                                 save_best_only=True,\n",
    "                                                 mode=\"max\",\n",
    "                                                 save_freq=\"epoch\")\n",
    "    \n",
    "    cosine_annealer = CosineAnnealingLearningRateScheduler(max_lr=max_lr,\n",
    "                                                           min_lr=min_lr,\n",
    "                                                           T_max=T_max)\n",
    "    \n",
    "    #Adam Optimizer - fixed learning rate.\n",
    "    adam_optimizer = tf.keras.optimizers.Adam(learning_rate=max_lr, weight_decay=1e-5, clipnorm=1.)\n",
    "\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False,label_smoothing=0.2)\n",
    "    model.compile(optimizer=adam_optimizer, loss=loss, metrics=['categorical_accuracy'])\n",
    "    \n",
    "    \n",
    "    callbacks  = [checkpoint, cosine_annealer]\n",
    "    \n",
    "    if USE_WANDB:\n",
    "        wandb.init(project=CONFIG.WANDB_RUN,\n",
    "                        name=run_name,\n",
    "                        notes=\"Model summary : \\n\" + str(model),\n",
    "                        config={\"max_lr\" : max_lr, \n",
    "                                \"min_lr\" : 5e-5, \n",
    "                                \"scheduler\" : \"cosineAnnealer\", \n",
    "                                \"epochs\" : epochs, \n",
    "                                \"T_max\" : T_max, \n",
    "                                \"train_size\" : len(train_dataset_parquet.dataset),\n",
    "                                \"val_size\" : len(val_dataset_parquet.dataset),\n",
    "                                \"unique_classes\" : len(train_dataset_parquet.unique_labels), \n",
    "                                \"video_length\" : CONFIG.VIDEO_LENGTH,\n",
    "                                \"features\" : mediapipe_features\n",
    "                                })\n",
    "        callbacks.append(WandbMetricsLogger())\n",
    "\n",
    "\n",
    "    history = model.fit(train_dataset, epochs=epochs, validation_data = val_dataset, callbacks=callbacks)\n",
    "    \n",
    "    if USE_WANDB:      \n",
    "        wandb.finish()\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 21:39:34.395343: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmlewand\u001b[0m (\u001b[33mmlewand7\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240422_213936-ai29lone</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/ai29lone' target=\"_blank\">LSTM164-LR-D256-GELU-reduced-reduced-1k</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/ai29lone' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/ai29lone</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 21:39:39.881199: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:447] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\n",
      "2024-04-22 21:39:39.881272: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:451] Memory usage: 7274496 bytes free, 25438126080 bytes total.\n",
      "2024-04-22 21:39:39.881310: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:461] Possibly insufficient driver version: 525.105.17\n",
      "2024-04-22 21:39:39.881326: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at cudnn_rnn_ops.cc:1764 : UNKNOWN: Fail to find the dnn implementation.\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\nDetected at node CudnnRNN defined at (most recent call last):\n<stack traces unavailable>\nFail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential/lstm/PartitionedCall]] [Op:__inference_train_function_3983]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m256\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgelu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;28mlen\u001b[39m(train_dataset_parquet\u001b[38;5;241m.\u001b[39munique_labels), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 11\u001b[0m \u001b[43mkeras_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLSTM164-LR-D256-GELU-reduced-reduced-1k.tf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLSTM164-LR-D256-GELU-reduced-reduced-1k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmediapipe_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreduced-LipsEyesHandsPose-scaled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mUSE_WANDB\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 68\u001b[0m, in \u001b[0;36mkeras_train\u001b[0;34m(model, filepath, max_lr, min_lr, T_max, epochs, run_name, mediapipe_features, USE_WANDB)\u001b[0m\n\u001b[1;32m     51\u001b[0m     wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39mCONFIG\u001b[38;5;241m.\u001b[39mWANDB_RUN,\n\u001b[1;32m     52\u001b[0m                     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m     53\u001b[0m                     notes\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel summary : \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(model),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m : mediapipe_features\n\u001b[1;32m     64\u001b[0m                             })\n\u001b[1;32m     65\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(WandbMetricsLogger())\n\u001b[0;32m---> 68\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m USE_WANDB:      \n\u001b[1;32m     71\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nDetected at node CudnnRNN defined at (most recent call last):\n<stack traces unavailable>\nFail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential/lstm/PartitionedCall]] [Op:__inference_train_function_3983]"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(64, return_sequences=False, input_shape=(X_shape[0], X_shape[1]),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(256, activation='gelu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"LSTM164-LR-D256-GELU-reduced-reduced-1k.tf\"),\n",
    "            run_name=\"LSTM164-LR-D256-GELU-reduced-reduced-1k\",\n",
    "            mediapipe_features=\"reduced-LipsEyesHandsPose-scaled\",\n",
    "            USE_WANDB=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "     50/Unknown - 9s 132ms/step - loss: nan - categorical_accuracy: 0.0950"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m256\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;28mlen\u001b[39m(train_dataset_parquet\u001b[38;5;241m.\u001b[39munique_labels), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 12\u001b[0m \u001b[43mkeras_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLSTM1-2.tf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLSTM128-Dense128-Dense256-LipsEyesHandsPose_5k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmediapipe_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreduced-LipsEyesHandsPose\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43mUSE_WANDB\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[57], line 68\u001b[0m, in \u001b[0;36mkeras_train\u001b[0;34m(model, filepath, max_lr, min_lr, T_max, epochs, run_name, mediapipe_features, USE_WANDB)\u001b[0m\n\u001b[1;32m     51\u001b[0m     wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39mCONFIG\u001b[38;5;241m.\u001b[39mWANDB_RUN,\n\u001b[1;32m     52\u001b[0m                     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m     53\u001b[0m                     notes\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel summary : \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(model),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m : mediapipe_features\n\u001b[1;32m     64\u001b[0m                             })\n\u001b[1;32m     65\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(WandbMetricsLogger())\n\u001b[0;32m---> 68\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m USE_WANDB:      \n\u001b[1;32m     71\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1776\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1777\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1781\u001b[0m ):\n\u001b[1;32m   1782\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1783\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1785\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    828\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 831\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    834\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    864\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1262\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m     args,\n\u001b[1;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1268\u001b[0m     executing_eagerly)\n\u001b[1;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    216\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    251\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 252\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    262\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1479\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1480\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1481\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1482\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1487\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1488\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1489\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1493\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1494\u001b[0m   )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(256, return_sequences=False, input_shape=(X_shape[0], X_shape[1]),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"LSTM1-2.tf\"),\n",
    "            run_name=\"LSTM128-Dense128-Dense256-LipsEyesHandsPose_5k\",\n",
    "            mediapipe_features=\"reduced-LipsEyesHandsPose\",\n",
    "            USE_WANDB=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240416_142143-iu5byyhv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/iu5byyhv' target=\"_blank\">LSTM64-Dense128-Dense256-LipsEyesHandsPose_5k</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/iu5byyhv' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/iu5byyhv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "    494/Unknown - 6s 6ms/step - loss: 2.5751 - categorical_accuracy: 0.1043"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 14:21:49.586712: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7580302709240232395\n",
      "2024-04-16 14:21:49.586753: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 1486800163596208403\n",
      "2024-04-16 14:21:49.586767: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 1011830033887524483\n",
      "2024-04-16 14:21:49.586779: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4153888742695321310\n",
      "2024-04-16 14:21:50.744220: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 3771462535022358857\n",
      "2024-04-16 14:21:50.744267: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 3371227081520258209\n",
      "2024-04-16 14:21:50.744283: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12156306670906514424\n",
      "2024-04-16 14:21:50.744310: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7717513745239281288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 14s 22ms/step - loss: 2.5744 - categorical_accuracy: 0.1063 - val_loss: 2.5371 - val_categorical_accuracy: 0.1400\n",
      "Epoch 2/100\n",
      "498/500 [============================>.] - ETA: 0s - loss: 2.4465 - categorical_accuracy: 0.1574INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 2.4459 - categorical_accuracy: 0.1567 - val_loss: 2.3761 - val_categorical_accuracy: 0.1940\n",
      "Epoch 3/100\n",
      "493/500 [============================>.] - ETA: 0s - loss: 2.3787 - categorical_accuracy: 0.1833INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.3786 - categorical_accuracy: 0.1835 - val_loss: 2.3476 - val_categorical_accuracy: 0.2160\n",
      "Epoch 4/100\n",
      "496/500 [============================>.] - ETA: 0s - loss: 2.3495 - categorical_accuracy: 0.2036INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.3490 - categorical_accuracy: 0.2030 - val_loss: 2.3199 - val_categorical_accuracy: 0.2310\n",
      "Epoch 5/100\n",
      "494/500 [============================>.] - ETA: 0s - loss: 2.3254 - categorical_accuracy: 0.2234INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.3252 - categorical_accuracy: 0.2230 - val_loss: 2.2948 - val_categorical_accuracy: 0.2550\n",
      "Epoch 6/100\n",
      "498/500 [============================>.] - ETA: 0s - loss: 2.3054 - categorical_accuracy: 0.2415INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 2.3053 - categorical_accuracy: 0.2412 - val_loss: 2.2704 - val_categorical_accuracy: 0.2700\n",
      "Epoch 7/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 2.2840 - categorical_accuracy: 0.2447 - val_loss: 2.2611 - val_categorical_accuracy: 0.2690\n",
      "Epoch 8/100\n",
      "494/500 [============================>.] - ETA: 0s - loss: 2.2694 - categorical_accuracy: 0.2424INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.2693 - categorical_accuracy: 0.2428 - val_loss: 2.2539 - val_categorical_accuracy: 0.2750\n",
      "Epoch 9/100\n",
      "494/500 [============================>.] - ETA: 0s - loss: 2.2539 - categorical_accuracy: 0.2551INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 23ms/step - loss: 2.2537 - categorical_accuracy: 0.2548 - val_loss: 2.2384 - val_categorical_accuracy: 0.2790\n",
      "Epoch 10/100\n",
      "491/500 [============================>.] - ETA: 0s - loss: 2.2448 - categorical_accuracy: 0.2747INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.2444 - categorical_accuracy: 0.2755 - val_loss: 2.2252 - val_categorical_accuracy: 0.2910\n",
      "Epoch 11/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.2272 - categorical_accuracy: 0.2775INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 2.2272 - categorical_accuracy: 0.2775 - val_loss: 2.2113 - val_categorical_accuracy: 0.3020\n",
      "Epoch 12/100\n",
      "492/500 [============================>.] - ETA: 0s - loss: 2.2144 - categorical_accuracy: 0.2950INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.2131 - categorical_accuracy: 0.2962 - val_loss: 2.1871 - val_categorical_accuracy: 0.3120\n",
      "Epoch 13/100\n",
      "499/500 [============================>.] - ETA: 0s - loss: 2.1986 - categorical_accuracy: 0.3056INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.1982 - categorical_accuracy: 0.3060 - val_loss: 2.1749 - val_categorical_accuracy: 0.3290\n",
      "Epoch 14/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 2.1849 - categorical_accuracy: 0.3122 - val_loss: 2.1619 - val_categorical_accuracy: 0.3180\n",
      "Epoch 15/100\n",
      "492/500 [============================>.] - ETA: 0s - loss: 2.1735 - categorical_accuracy: 0.3211INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.1721 - categorical_accuracy: 0.3220 - val_loss: 2.1553 - val_categorical_accuracy: 0.3330\n",
      "Epoch 16/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 2.1567 - categorical_accuracy: 0.3368 - val_loss: 2.1553 - val_categorical_accuracy: 0.3200\n",
      "Epoch 17/100\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.1440 - categorical_accuracy: 0.3400INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.1425 - categorical_accuracy: 0.3408 - val_loss: 2.1429 - val_categorical_accuracy: 0.3410\n",
      "Epoch 18/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 2.1320 - categorical_accuracy: 0.3483 - val_loss: 2.1302 - val_categorical_accuracy: 0.3340\n",
      "Epoch 19/100\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.1225 - categorical_accuracy: 0.3521INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.1211 - categorical_accuracy: 0.3530 - val_loss: 2.1167 - val_categorical_accuracy: 0.3450\n",
      "Epoch 20/100\n",
      "498/500 [============================>.] - ETA: 0s - loss: 2.1092 - categorical_accuracy: 0.3577INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 2.1085 - categorical_accuracy: 0.3580 - val_loss: 2.1173 - val_categorical_accuracy: 0.3520\n",
      "Epoch 21/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 2.0984 - categorical_accuracy: 0.3655 - val_loss: 2.1079 - val_categorical_accuracy: 0.3430\n",
      "Epoch 22/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 2.0867 - categorical_accuracy: 0.3758 - val_loss: 2.1076 - val_categorical_accuracy: 0.3480\n",
      "Epoch 23/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 2.0776 - categorical_accuracy: 0.3823 - val_loss: 2.1120 - val_categorical_accuracy: 0.3500\n",
      "Epoch 24/100\n",
      "494/500 [============================>.] - ETA: 0s - loss: 2.0670 - categorical_accuracy: 0.3813INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.0656 - categorical_accuracy: 0.3825 - val_loss: 2.0905 - val_categorical_accuracy: 0.3650\n",
      "Epoch 25/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 2.0545 - categorical_accuracy: 0.3955 - val_loss: 2.0955 - val_categorical_accuracy: 0.3460\n",
      "Epoch 26/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 2.0429 - categorical_accuracy: 0.4060 - val_loss: 2.0900 - val_categorical_accuracy: 0.3590\n",
      "Epoch 27/100\n",
      "494/500 [============================>.] - ETA: 0s - loss: 2.0377 - categorical_accuracy: 0.4016INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 2.0369 - categorical_accuracy: 0.4022 - val_loss: 2.0765 - val_categorical_accuracy: 0.3730\n",
      "Epoch 28/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 2.0270 - categorical_accuracy: 0.4160 - val_loss: 2.0819 - val_categorical_accuracy: 0.3620\n",
      "Epoch 29/100\n",
      "498/500 [============================>.] - ETA: 0s - loss: 2.0123 - categorical_accuracy: 0.4275INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.0115 - categorical_accuracy: 0.4280 - val_loss: 2.0657 - val_categorical_accuracy: 0.3960\n",
      "Epoch 30/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 2.0050 - categorical_accuracy: 0.4297 - val_loss: 2.0684 - val_categorical_accuracy: 0.3890\n",
      "Epoch 31/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.9892 - categorical_accuracy: 0.4445 - val_loss: 2.0638 - val_categorical_accuracy: 0.3910\n",
      "Epoch 32/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.9824 - categorical_accuracy: 0.4502 - val_loss: 2.0563 - val_categorical_accuracy: 0.3950\n",
      "Epoch 33/100\n",
      "492/500 [============================>.] - ETA: 0s - loss: 1.9735 - categorical_accuracy: 0.4484INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 1.9726 - categorical_accuracy: 0.4485 - val_loss: 2.0426 - val_categorical_accuracy: 0.3990\n",
      "Epoch 34/100\n",
      "495/500 [============================>.] - ETA: 0s - loss: 1.9642 - categorical_accuracy: 0.4624INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 1.9631 - categorical_accuracy: 0.4625 - val_loss: 2.0376 - val_categorical_accuracy: 0.4090\n",
      "Epoch 35/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.9554 - categorical_accuracy: 0.4640 - val_loss: 2.0410 - val_categorical_accuracy: 0.3990\n",
      "Epoch 36/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.9462 - categorical_accuracy: 0.4737 - val_loss: 2.0567 - val_categorical_accuracy: 0.3940\n",
      "Epoch 37/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.9403 - categorical_accuracy: 0.4748 - val_loss: 2.0506 - val_categorical_accuracy: 0.3920\n",
      "Epoch 38/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.9330 - categorical_accuracy: 0.4782 - val_loss: 2.0266 - val_categorical_accuracy: 0.4050\n",
      "Epoch 39/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.9225 - categorical_accuracy: 0.4870 - val_loss: 2.0357 - val_categorical_accuracy: 0.3970\n",
      "Epoch 40/100\n",
      "495/500 [============================>.] - ETA: 0s - loss: 1.9169 - categorical_accuracy: 0.4854INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 1.9159 - categorical_accuracy: 0.4863 - val_loss: 2.0321 - val_categorical_accuracy: 0.4130\n",
      "Epoch 41/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.9114 - categorical_accuracy: 0.4877 - val_loss: 2.0368 - val_categorical_accuracy: 0.4110\n",
      "Epoch 42/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.9067 - categorical_accuracy: 0.4988 - val_loss: 2.0395 - val_categorical_accuracy: 0.4040\n",
      "Epoch 43/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.8968 - categorical_accuracy: 0.5005 - val_loss: 2.0267 - val_categorical_accuracy: 0.4100\n",
      "Epoch 44/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.8952 - categorical_accuracy: 0.4978 - val_loss: 2.0218 - val_categorical_accuracy: 0.4130\n",
      "Epoch 45/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.8929 - categorical_accuracy: 0.5015 - val_loss: 2.0344 - val_categorical_accuracy: 0.4060\n",
      "Epoch 46/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.8855 - categorical_accuracy: 0.5013 - val_loss: 2.0309 - val_categorical_accuracy: 0.4100\n",
      "Epoch 47/100\n",
      "499/500 [============================>.] - ETA: 0s - loss: 1.8845 - categorical_accuracy: 0.4997INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 1.8837 - categorical_accuracy: 0.5005 - val_loss: 2.0073 - val_categorical_accuracy: 0.4280\n",
      "Epoch 48/100\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.8736 - categorical_accuracy: 0.5154INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 1.8725 - categorical_accuracy: 0.5155 - val_loss: 2.0097 - val_categorical_accuracy: 0.4340\n",
      "Epoch 49/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.8667 - categorical_accuracy: 0.5135 - val_loss: 2.0133 - val_categorical_accuracy: 0.4250\n",
      "Epoch 50/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.9156 - categorical_accuracy: 0.4857 - val_loss: 2.0037 - val_categorical_accuracy: 0.4340\n",
      "Epoch 51/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.9073 - categorical_accuracy: 0.4910 - val_loss: 2.0392 - val_categorical_accuracy: 0.3960\n",
      "Epoch 52/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.9080 - categorical_accuracy: 0.4970 - val_loss: 2.0256 - val_categorical_accuracy: 0.4180\n",
      "Epoch 53/100\n",
      "494/500 [============================>.] - ETA: 0s - loss: 1.9019 - categorical_accuracy: 0.4909INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 1.9018 - categorical_accuracy: 0.4913 - val_loss: 1.9863 - val_categorical_accuracy: 0.4440\n",
      "Epoch 54/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.8826 - categorical_accuracy: 0.5033 - val_loss: 2.0288 - val_categorical_accuracy: 0.4320\n",
      "Epoch 55/100\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.8826 - categorical_accuracy: 0.5028INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 1.8816 - categorical_accuracy: 0.5040 - val_loss: 1.9954 - val_categorical_accuracy: 0.4530\n",
      "Epoch 56/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.8721 - categorical_accuracy: 0.5080 - val_loss: 2.0111 - val_categorical_accuracy: 0.4470\n",
      "Epoch 57/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.8705 - categorical_accuracy: 0.5182 - val_loss: 1.9979 - val_categorical_accuracy: 0.4500\n",
      "Epoch 58/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.8750 - categorical_accuracy: 0.5080 - val_loss: 1.9921 - val_categorical_accuracy: 0.4470\n",
      "Epoch 59/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.8504 - categorical_accuracy: 0.5255 - val_loss: 1.9855 - val_categorical_accuracy: 0.4470\n",
      "Epoch 60/100\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.8474 - categorical_accuracy: 0.5292INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 1.8461 - categorical_accuracy: 0.5300 - val_loss: 1.9910 - val_categorical_accuracy: 0.4550\n",
      "Epoch 61/100\n",
      "492/500 [============================>.] - ETA: 0s - loss: 1.8574 - categorical_accuracy: 0.5272INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 1.8573 - categorical_accuracy: 0.5270 - val_loss: 2.0022 - val_categorical_accuracy: 0.4610\n",
      "Epoch 62/100\n",
      "497/500 [============================>.] - ETA: 0s - loss: 1.8352 - categorical_accuracy: 0.5337INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 1.8335 - categorical_accuracy: 0.5345 - val_loss: 1.9828 - val_categorical_accuracy: 0.4670\n",
      "Epoch 63/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.8352 - categorical_accuracy: 0.5385 - val_loss: 1.9890 - val_categorical_accuracy: 0.4660\n",
      "Epoch 64/100\n",
      "492/500 [============================>.] - ETA: 0s - loss: 1.8333 - categorical_accuracy: 0.5368INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 1.8336 - categorical_accuracy: 0.5365 - val_loss: 1.9834 - val_categorical_accuracy: 0.4770\n",
      "Epoch 65/100\n",
      "499/500 [============================>.] - ETA: 0s - loss: 1.8170 - categorical_accuracy: 0.5433INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 1.8162 - categorical_accuracy: 0.5440 - val_loss: 1.9597 - val_categorical_accuracy: 0.4840\n",
      "Epoch 66/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.8123 - categorical_accuracy: 0.5487 - val_loss: 2.0008 - val_categorical_accuracy: 0.4700\n",
      "Epoch 67/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.8097 - categorical_accuracy: 0.5477 - val_loss: 2.0154 - val_categorical_accuracy: 0.4480\n",
      "Epoch 68/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.7971 - categorical_accuracy: 0.5515 - val_loss: 1.9714 - val_categorical_accuracy: 0.4740\n",
      "Epoch 69/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.7853 - categorical_accuracy: 0.5660 - val_loss: 1.9674 - val_categorical_accuracy: 0.4730\n",
      "Epoch 70/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.7875 - categorical_accuracy: 0.5620 - val_loss: 2.0018 - val_categorical_accuracy: 0.4650\n",
      "Epoch 71/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.7710 - categorical_accuracy: 0.5690 - val_loss: 1.9898 - val_categorical_accuracy: 0.4800\n",
      "Epoch 72/100\n",
      "494/500 [============================>.] - ETA: 0s - loss: 1.7739 - categorical_accuracy: 0.5744INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 1.7738 - categorical_accuracy: 0.5745 - val_loss: 1.9736 - val_categorical_accuracy: 0.4860\n",
      "Epoch 73/100\n",
      "491/500 [============================>.] - ETA: 0s - loss: 1.7643 - categorical_accuracy: 0.5782INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 1.7642 - categorical_accuracy: 0.5775 - val_loss: 1.9471 - val_categorical_accuracy: 0.4960\n",
      "Epoch 74/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.7610 - categorical_accuracy: 0.5828 - val_loss: 1.9566 - val_categorical_accuracy: 0.4900\n",
      "Epoch 75/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.7545 - categorical_accuracy: 0.5850 - val_loss: 1.9730 - val_categorical_accuracy: 0.4930\n",
      "Epoch 76/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.7455 - categorical_accuracy: 0.5895 - val_loss: 1.9685 - val_categorical_accuracy: 0.4940\n",
      "Epoch 77/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.7349 - categorical_accuracy: 0.5938INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 1.7349 - categorical_accuracy: 0.5938 - val_loss: 1.9566 - val_categorical_accuracy: 0.4980\n",
      "Epoch 78/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.7346 - categorical_accuracy: 0.6010 - val_loss: 1.9772 - val_categorical_accuracy: 0.4900\n",
      "Epoch 79/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.7297 - categorical_accuracy: 0.6012 - val_loss: 2.0076 - val_categorical_accuracy: 0.4850\n",
      "Epoch 80/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.7294 - categorical_accuracy: 0.6035 - val_loss: 1.9587 - val_categorical_accuracy: 0.4950\n",
      "Epoch 81/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.7206 - categorical_accuracy: 0.6003 - val_loss: 1.9667 - val_categorical_accuracy: 0.4930\n",
      "Epoch 82/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.7164 - categorical_accuracy: 0.6093INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 1.7164 - categorical_accuracy: 0.6093 - val_loss: 1.9402 - val_categorical_accuracy: 0.5050\n",
      "Epoch 83/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.7155 - categorical_accuracy: 0.6120 - val_loss: 1.9724 - val_categorical_accuracy: 0.4890\n",
      "Epoch 84/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.7063 - categorical_accuracy: 0.6160 - val_loss: 1.9633 - val_categorical_accuracy: 0.4930\n",
      "Epoch 85/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.7045 - categorical_accuracy: 0.6080 - val_loss: 1.9726 - val_categorical_accuracy: 0.4890\n",
      "Epoch 86/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.6985 - categorical_accuracy: 0.6175 - val_loss: 1.9674 - val_categorical_accuracy: 0.4920\n",
      "Epoch 87/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.6974 - categorical_accuracy: 0.6173 - val_loss: 1.9541 - val_categorical_accuracy: 0.5030\n",
      "Epoch 88/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.6922 - categorical_accuracy: 0.6168 - val_loss: 1.9422 - val_categorical_accuracy: 0.4900\n",
      "Epoch 89/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.6891 - categorical_accuracy: 0.6177 - val_loss: 1.9528 - val_categorical_accuracy: 0.5030\n",
      "Epoch 90/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.6817 - categorical_accuracy: 0.6233 - val_loss: 1.9250 - val_categorical_accuracy: 0.5030\n",
      "Epoch 91/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.6753 - categorical_accuracy: 0.6290 - val_loss: 1.9459 - val_categorical_accuracy: 0.4980\n",
      "Epoch 92/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.6737 - categorical_accuracy: 0.6290 - val_loss: 1.9785 - val_categorical_accuracy: 0.4870\n",
      "Epoch 93/100\n",
      "497/500 [============================>.] - ETA: 0s - loss: 1.6767 - categorical_accuracy: 0.6215INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 1.6748 - categorical_accuracy: 0.6230 - val_loss: 1.9523 - val_categorical_accuracy: 0.5070\n",
      "Epoch 94/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.6754 - categorical_accuracy: 0.6200 - val_loss: 1.9660 - val_categorical_accuracy: 0.5010\n",
      "Epoch 95/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.6653 - categorical_accuracy: 0.6280 - val_loss: 1.9647 - val_categorical_accuracy: 0.5040\n",
      "Epoch 96/100\n",
      "499/500 [============================>.] - ETA: 0s - loss: 1.6635 - categorical_accuracy: 0.6335INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM1-3.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 1.6625 - categorical_accuracy: 0.6340 - val_loss: 1.9257 - val_categorical_accuracy: 0.5210\n",
      "Epoch 97/100\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 1.6668 - categorical_accuracy: 0.6308 - val_loss: 1.9498 - val_categorical_accuracy: 0.5150\n",
      "Epoch 98/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.6534 - categorical_accuracy: 0.6415 - val_loss: 1.9518 - val_categorical_accuracy: 0.5090\n",
      "Epoch 99/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.6566 - categorical_accuracy: 0.6370 - val_loss: 1.9675 - val_categorical_accuracy: 0.4930\n",
      "Epoch 100/100\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 1.7232 - categorical_accuracy: 0.6025 - val_loss: 1.9353 - val_categorical_accuracy: 0.4980\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b099dbe65c1045bc86545b8fedf8870d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁▂▃▃▃▄▄▄▄▅▅▅▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇██████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁███▇▇▇▆▆▅▅▄▄▃▃▂▂▁▁▁█</td></tr><tr><td>epoch/loss</td><td>█▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁▂▃▄▄▅▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇██▇████▇███</td></tr><tr><td>epoch/val_loss</td><td>█▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.6025</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>1.72324</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.498</td></tr><tr><td>epoch/val_loss</td><td>1.93533</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM64-Dense128-Dense256-LipsEyesHandsPose_5k</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/iu5byyhv' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/iu5byyhv</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240416_142143-iu5byyhv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f954c786a90>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(64, return_sequences=False, input_shape=(X_shape[0], X_shape[1]),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"LSTM1-3.tf\"),\n",
    "            run_name=\"LSTM64-Dense128-Dense256-LipsEyesHandsPose_5k\",\n",
    "            mediapipe_features=\"reduced-LipsEyesHandsPose\",\n",
    "            USE_WANDB=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(256, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, 1629),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"LSTM256-Dense128-Dense256-LipsEyesHandsPose_5k.tf\"),\n",
    "            run_name=\"LSTM256-Dense128-Dense256-LipsEyesHandsPose_5k\",\n",
    "            mediapipe_features=\"reduced-LipsEyesHandsPose\",\n",
    "            USE_WANDB=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(128, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, 1629),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"LSTM128-Dense256-allfeatures.tf\"),\n",
    "            run_name=\"LSTM128-Dense256-allfeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(128, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, 1629),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"LSTM128-Dense128-allfeatures.tf\"),\n",
    "            run_name=\"LSTM128-Dense128-allfeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(128, return_sequences=False, activation='relu', input_shape=(CONFIG.VIDEO_LENGTH, 1629),\n",
    "               kernel_regularizer=l2(0.0000001), \n",
    "               activity_regularizer=l2(0.0000001)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"LSTM128_l2-Dense128-Dense256-allfeatures.tf\"),\n",
    "            run_name=\"LSTM128_l2-Dense128-Dense256-allfeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:g1jztv1q) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁</td></tr><tr><td>epoch/epoch</td><td>▁</td></tr><tr><td>epoch/learning_rate</td><td>▁</td></tr><tr><td>epoch/loss</td><td>▁</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁</td></tr><tr><td>epoch/val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.36889</td></tr><tr><td>epoch/epoch</td><td>0</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>2.82214</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.45</td></tr><tr><td>epoch/val_loss</td><td>2.39018</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM64-Dense128-Dense256-allfeatures_bigger_reg</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/g1jztv1q' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/g1jztv1q</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240408_085721-g1jztv1q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:g1jztv1q). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae32d08edf64e04a8009a32b4a48b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113070862160788, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240408_085904-b2cvthn0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/b2cvthn0' target=\"_blank\">LSTM64-Dense128-Dense256-allfeatures_bigger_reg</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/b2cvthn0' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/b2cvthn0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 2.8411 - categorical_accuracy: 0.4000INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 60s 499ms/step - loss: 2.8411 - categorical_accuracy: 0.4000 - val_loss: 2.4088 - val_categorical_accuracy: 0.4200\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 2.1274 - categorical_accuracy: 0.4056INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 502ms/step - loss: 2.1274 - categorical_accuracy: 0.4056 - val_loss: 1.8624 - val_categorical_accuracy: 0.4300\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - 49s 439ms/step - loss: 1.7043 - categorical_accuracy: 0.4044 - val_loss: 1.5451 - val_categorical_accuracy: 0.4300\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 1.4585 - categorical_accuracy: 0.4056 - val_loss: 1.3563 - val_categorical_accuracy: 0.4400\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - 50s 441ms/step - loss: 1.3106 - categorical_accuracy: 0.4056 - val_loss: 1.2412 - val_categorical_accuracy: 0.4300\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.2187 - categorical_accuracy: 0.4167INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 503ms/step - loss: 1.2187 - categorical_accuracy: 0.4167 - val_loss: 1.1653 - val_categorical_accuracy: 0.4200\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.1668 - categorical_accuracy: 0.4211INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 505ms/step - loss: 1.1668 - categorical_accuracy: 0.4211 - val_loss: 1.0656 - val_categorical_accuracy: 0.5000\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - 50s 442ms/step - loss: 1.1402 - categorical_accuracy: 0.4167 - val_loss: 1.0608 - val_categorical_accuracy: 0.5100\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0638 - categorical_accuracy: 0.4833INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 505ms/step - loss: 1.0638 - categorical_accuracy: 0.4833 - val_loss: 0.9921 - val_categorical_accuracy: 0.5600\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0240 - categorical_accuracy: 0.4889INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 500ms/step - loss: 1.0240 - categorical_accuracy: 0.4889 - val_loss: 1.0163 - val_categorical_accuracy: 0.5200\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0002 - categorical_accuracy: 0.5100INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 512ms/step - loss: 1.0002 - categorical_accuracy: 0.5100 - val_loss: 0.9776 - val_categorical_accuracy: 0.5200\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.9623 - categorical_accuracy: 0.5144INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 512ms/step - loss: 0.9623 - categorical_accuracy: 0.5144 - val_loss: 0.9214 - val_categorical_accuracy: 0.5500\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.9209 - categorical_accuracy: 0.5544INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 505ms/step - loss: 0.9209 - categorical_accuracy: 0.5544 - val_loss: 0.8942 - val_categorical_accuracy: 0.5600\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.9315 - categorical_accuracy: 0.5333 - val_loss: 0.9100 - val_categorical_accuracy: 0.5400\n",
      "Epoch 15/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8799 - categorical_accuracy: 0.5733INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 512ms/step - loss: 0.8799 - categorical_accuracy: 0.5733 - val_loss: 0.8732 - val_categorical_accuracy: 0.5500\n",
      "Epoch 16/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.8731 - categorical_accuracy: 0.5589 - val_loss: 0.8565 - val_categorical_accuracy: 0.5900\n",
      "Epoch 17/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.8757 - categorical_accuracy: 0.5556 - val_loss: 0.8595 - val_categorical_accuracy: 0.5800\n",
      "Epoch 18/100\n",
      "113/113 [==============================] - 50s 444ms/step - loss: 0.8616 - categorical_accuracy: 0.5733 - val_loss: 0.8362 - val_categorical_accuracy: 0.6100\n",
      "Epoch 19/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8751 - categorical_accuracy: 0.5800INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 505ms/step - loss: 0.8751 - categorical_accuracy: 0.5800 - val_loss: 0.8408 - val_categorical_accuracy: 0.5700\n",
      "Epoch 20/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8239 - categorical_accuracy: 0.6022INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 56s 502ms/step - loss: 0.8239 - categorical_accuracy: 0.6022 - val_loss: 0.8111 - val_categorical_accuracy: 0.6000\n",
      "Epoch 21/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8207 - categorical_accuracy: 0.6067INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 511ms/step - loss: 0.8207 - categorical_accuracy: 0.6067 - val_loss: 0.8012 - val_categorical_accuracy: 0.6100\n",
      "Epoch 22/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7945 - categorical_accuracy: 0.6511INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 510ms/step - loss: 0.7945 - categorical_accuracy: 0.6511 - val_loss: 0.7536 - val_categorical_accuracy: 0.6500\n",
      "Epoch 23/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.7951 - categorical_accuracy: 0.6400 - val_loss: 0.9351 - val_categorical_accuracy: 0.6400\n",
      "Epoch 24/100\n",
      "113/113 [==============================] - 50s 449ms/step - loss: 0.8354 - categorical_accuracy: 0.6267 - val_loss: 1.3834 - val_categorical_accuracy: 0.3300\n",
      "Epoch 25/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7755 - categorical_accuracy: 0.6633INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 56s 499ms/step - loss: 0.7755 - categorical_accuracy: 0.6633 - val_loss: 0.7559 - val_categorical_accuracy: 0.6800\n",
      "Epoch 26/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7431 - categorical_accuracy: 0.7156INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 515ms/step - loss: 0.7431 - categorical_accuracy: 0.7156 - val_loss: 0.8453 - val_categorical_accuracy: 0.5700\n",
      "Epoch 27/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7037 - categorical_accuracy: 0.7278INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 56s 502ms/step - loss: 0.7037 - categorical_accuracy: 0.7278 - val_loss: 0.7106 - val_categorical_accuracy: 0.7100\n",
      "Epoch 28/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6386 - categorical_accuracy: 0.7778INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 502ms/step - loss: 0.6386 - categorical_accuracy: 0.7778 - val_loss: 0.7482 - val_categorical_accuracy: 0.6600\n",
      "Epoch 29/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.6493 - categorical_accuracy: 0.7622 - val_loss: 0.5469 - val_categorical_accuracy: 0.8400\n",
      "Epoch 30/100\n",
      "113/113 [==============================] - 50s 441ms/step - loss: 0.6398 - categorical_accuracy: 0.7711 - val_loss: 0.5749 - val_categorical_accuracy: 0.7700\n",
      "Epoch 31/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6062 - categorical_accuracy: 0.7833INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 508ms/step - loss: 0.6062 - categorical_accuracy: 0.7833 - val_loss: 0.7759 - val_categorical_accuracy: 0.6900\n",
      "Epoch 32/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5734 - categorical_accuracy: 0.8022INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 510ms/step - loss: 0.5734 - categorical_accuracy: 0.8022 - val_loss: 0.6057 - val_categorical_accuracy: 0.7800\n",
      "Epoch 33/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5675 - categorical_accuracy: 0.8044INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 506ms/step - loss: 0.5675 - categorical_accuracy: 0.8044 - val_loss: 0.7343 - val_categorical_accuracy: 0.7800\n",
      "Epoch 34/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.5910 - categorical_accuracy: 0.7911 - val_loss: 0.5328 - val_categorical_accuracy: 0.8200\n",
      "Epoch 35/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.6100 - categorical_accuracy: 0.7944 - val_loss: 0.9632 - val_categorical_accuracy: 0.7100\n",
      "Epoch 36/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.6669 - categorical_accuracy: 0.7811 - val_loss: 0.5713 - val_categorical_accuracy: 0.8100\n",
      "Epoch 37/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.5737 - categorical_accuracy: 0.8033 - val_loss: 0.8288 - val_categorical_accuracy: 0.6900\n",
      "Epoch 38/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5330 - categorical_accuracy: 0.8344INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 510ms/step - loss: 0.5330 - categorical_accuracy: 0.8344 - val_loss: 0.8726 - val_categorical_accuracy: 0.6300\n",
      "Epoch 39/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.5579 - categorical_accuracy: 0.8111 - val_loss: 0.7435 - val_categorical_accuracy: 0.7500\n",
      "Epoch 40/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.5458 - categorical_accuracy: 0.8233 - val_loss: 0.8340 - val_categorical_accuracy: 0.7000\n",
      "Epoch 41/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.6143 - categorical_accuracy: 0.7856 - val_loss: 0.7396 - val_categorical_accuracy: 0.7700\n",
      "Epoch 42/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.5929 - categorical_accuracy: 0.7967 - val_loss: 0.7475 - val_categorical_accuracy: 0.6900\n",
      "Epoch 43/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.5422 - categorical_accuracy: 0.8333 - val_loss: 0.6000 - val_categorical_accuracy: 0.8200\n",
      "Epoch 44/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4798 - categorical_accuracy: 0.8567INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 507ms/step - loss: 0.4798 - categorical_accuracy: 0.8567 - val_loss: 0.6378 - val_categorical_accuracy: 0.7800\n",
      "Epoch 45/100\n",
      "113/113 [==============================] - 49s 438ms/step - loss: 0.5517 - categorical_accuracy: 0.8100 - val_loss: 0.6610 - val_categorical_accuracy: 0.8000\n",
      "Epoch 46/100\n",
      "113/113 [==============================] - 50s 444ms/step - loss: 0.5207 - categorical_accuracy: 0.8422 - val_loss: 0.5422 - val_categorical_accuracy: 0.8600\n",
      "Epoch 47/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.5088 - categorical_accuracy: 0.8489 - val_loss: 0.5806 - val_categorical_accuracy: 0.7900\n",
      "Epoch 48/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.5605 - categorical_accuracy: 0.8167 - val_loss: 0.4904 - val_categorical_accuracy: 0.8400\n",
      "Epoch 49/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.5473 - categorical_accuracy: 0.8278 - val_loss: 0.6565 - val_categorical_accuracy: 0.7600\n",
      "Epoch 50/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.5422 - categorical_accuracy: 0.8378 - val_loss: 0.6482 - val_categorical_accuracy: 0.8100\n",
      "Epoch 51/100\n",
      "113/113 [==============================] - 50s 444ms/step - loss: 0.4920 - categorical_accuracy: 0.8533 - val_loss: 0.9195 - val_categorical_accuracy: 0.7200\n",
      "Epoch 52/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.5176 - categorical_accuracy: 0.8456 - val_loss: 0.4050 - val_categorical_accuracy: 0.8900\n",
      "Epoch 53/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.5224 - categorical_accuracy: 0.8367 - val_loss: 0.6238 - val_categorical_accuracy: 0.8300\n",
      "Epoch 54/100\n",
      "113/113 [==============================] - 51s 446ms/step - loss: 0.4998 - categorical_accuracy: 0.8511 - val_loss: 0.7593 - val_categorical_accuracy: 0.6900\n",
      "Epoch 55/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4259 - categorical_accuracy: 0.8800INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 505ms/step - loss: 0.4259 - categorical_accuracy: 0.8800 - val_loss: 0.6574 - val_categorical_accuracy: 0.7800\n",
      "Epoch 56/100\n",
      "113/113 [==============================] - 51s 446ms/step - loss: 0.5203 - categorical_accuracy: 0.8444 - val_loss: 0.5315 - val_categorical_accuracy: 0.8300\n",
      "Epoch 57/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.5094 - categorical_accuracy: 0.8478 - val_loss: 0.6249 - val_categorical_accuracy: 0.8100\n",
      "Epoch 58/100\n",
      "113/113 [==============================] - 51s 447ms/step - loss: 0.5697 - categorical_accuracy: 0.8200 - val_loss: 0.4367 - val_categorical_accuracy: 0.9000\n",
      "Epoch 59/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.4575 - categorical_accuracy: 0.8767 - val_loss: 0.6292 - val_categorical_accuracy: 0.8000\n",
      "Epoch 65/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.4987 - categorical_accuracy: 0.8511 - val_loss: 0.5974 - val_categorical_accuracy: 0.8300\n",
      "Epoch 66/100\n",
      "113/113 [==============================] - 50s 448ms/step - loss: 0.5852 - categorical_accuracy: 0.8289 - val_loss: 0.6959 - val_categorical_accuracy: 0.7500\n",
      "Epoch 67/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.5027 - categorical_accuracy: 0.8533 - val_loss: 0.5093 - val_categorical_accuracy: 0.8500\n",
      "Epoch 68/100\n",
      "113/113 [==============================] - 50s 441ms/step - loss: 0.4957 - categorical_accuracy: 0.8622 - val_loss: 0.4846 - val_categorical_accuracy: 0.8500\n",
      "Epoch 69/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.5055 - categorical_accuracy: 0.8533 - val_loss: 0.5980 - val_categorical_accuracy: 0.8600\n",
      "Epoch 70/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.5464 - categorical_accuracy: 0.8367 - val_loss: 0.5365 - val_categorical_accuracy: 0.8400\n",
      "Epoch 71/100\n",
      "113/113 [==============================] - 50s 448ms/step - loss: 0.4602 - categorical_accuracy: 0.8689 - val_loss: 0.4441 - val_categorical_accuracy: 0.8800\n",
      "Epoch 72/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.5463 - categorical_accuracy: 0.8189 - val_loss: 0.5223 - val_categorical_accuracy: 0.8000\n",
      "Epoch 73/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4312 - categorical_accuracy: 0.8878INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 513ms/step - loss: 0.4312 - categorical_accuracy: 0.8878 - val_loss: 0.4676 - val_categorical_accuracy: 0.8600\n",
      "Epoch 74/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.4691 - categorical_accuracy: 0.8589 - val_loss: 0.4604 - val_categorical_accuracy: 0.8700\n",
      "Epoch 75/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.4864 - categorical_accuracy: 0.8567 - val_loss: 0.4465 - val_categorical_accuracy: 0.9000\n",
      "Epoch 76/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.4778 - categorical_accuracy: 0.8678 - val_loss: 0.4316 - val_categorical_accuracy: 0.8800\n",
      "Epoch 77/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.4969 - categorical_accuracy: 0.8444 - val_loss: 0.4481 - val_categorical_accuracy: 0.8800\n",
      "Epoch 78/100\n",
      "113/113 [==============================] - 50s 448ms/step - loss: 0.4584 - categorical_accuracy: 0.8644 - val_loss: 0.4431 - val_categorical_accuracy: 0.9100\n",
      "Epoch 79/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4070 - categorical_accuracy: 0.8900INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 505ms/step - loss: 0.4070 - categorical_accuracy: 0.8900 - val_loss: 0.4321 - val_categorical_accuracy: 0.8800\n",
      "Epoch 80/100\n",
      "113/113 [==============================] - 50s 442ms/step - loss: 0.4880 - categorical_accuracy: 0.8633 - val_loss: 0.4699 - val_categorical_accuracy: 0.8800\n",
      "Epoch 81/100\n",
      "113/113 [==============================] - 50s 439ms/step - loss: 0.4324 - categorical_accuracy: 0.8822 - val_loss: 0.4674 - val_categorical_accuracy: 0.8600\n",
      "Epoch 82/100\n",
      "113/113 [==============================] - 50s 439ms/step - loss: 0.4607 - categorical_accuracy: 0.8722 - val_loss: 0.4195 - val_categorical_accuracy: 0.9000\n",
      "Epoch 83/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.5813 - categorical_accuracy: 0.8144 - val_loss: 0.7107 - val_categorical_accuracy: 0.8300\n",
      "Epoch 84/100\n",
      "113/113 [==============================] - 50s 440ms/step - loss: 0.5578 - categorical_accuracy: 0.8311 - val_loss: 0.4452 - val_categorical_accuracy: 0.8800\n",
      "Epoch 85/100\n",
      "113/113 [==============================] - 50s 439ms/step - loss: 0.4656 - categorical_accuracy: 0.8722 - val_loss: 0.4972 - val_categorical_accuracy: 0.8600\n",
      "Epoch 86/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.4155 - categorical_accuracy: 0.8800 - val_loss: 0.7409 - val_categorical_accuracy: 0.8000\n",
      "Epoch 87/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.5258 - categorical_accuracy: 0.8367 - val_loss: 0.4980 - val_categorical_accuracy: 0.8300\n",
      "Epoch 88/100\n",
      "113/113 [==============================] - 50s 442ms/step - loss: 0.4853 - categorical_accuracy: 0.8689 - val_loss: 0.5915 - val_categorical_accuracy: 0.8200\n",
      "Epoch 89/100\n",
      "113/113 [==============================] - 50s 439ms/step - loss: 0.4376 - categorical_accuracy: 0.8811 - val_loss: 0.4558 - val_categorical_accuracy: 0.8700\n",
      "Epoch 90/100\n",
      "113/113 [==============================] - 50s 441ms/step - loss: 0.4365 - categorical_accuracy: 0.8822 - val_loss: 0.5814 - val_categorical_accuracy: 0.8300\n",
      "Epoch 91/100\n",
      "113/113 [==============================] - 50s 440ms/step - loss: 0.5081 - categorical_accuracy: 0.8378 - val_loss: 0.7659 - val_categorical_accuracy: 0.7900\n",
      "Epoch 92/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.4477 - categorical_accuracy: 0.8767 - val_loss: 0.4377 - val_categorical_accuracy: 0.8800\n",
      "Epoch 93/100\n",
      "113/113 [==============================] - 50s 438ms/step - loss: 0.4153 - categorical_accuracy: 0.8900 - val_loss: 0.3688 - val_categorical_accuracy: 0.9100\n",
      "Epoch 94/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4063 - categorical_accuracy: 0.8922INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM-L64-D128-D256-reg=0.005.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 56s 498ms/step - loss: 0.4063 - categorical_accuracy: 0.8922 - val_loss: 0.4536 - val_categorical_accuracy: 0.8800\n",
      "Epoch 95/100\n",
      "113/113 [==============================] - 51s 444ms/step - loss: 0.4751 - categorical_accuracy: 0.8656 - val_loss: 0.5127 - val_categorical_accuracy: 0.8400\n",
      "Epoch 96/100\n",
      "113/113 [==============================] - 50s 442ms/step - loss: 0.4397 - categorical_accuracy: 0.8767 - val_loss: 0.4737 - val_categorical_accuracy: 0.8700\n",
      "Epoch 97/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.4376 - categorical_accuracy: 0.8722 - val_loss: 0.4013 - val_categorical_accuracy: 0.8800\n",
      "Epoch 98/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.4417 - categorical_accuracy: 0.8722 - val_loss: 0.4582 - val_categorical_accuracy: 0.8600\n",
      "Epoch 99/100\n",
      "113/113 [==============================] - 50s 441ms/step - loss: 0.4192 - categorical_accuracy: 0.8867 - val_loss: 0.4077 - val_categorical_accuracy: 0.9000\n",
      "Epoch 100/100\n",
      "113/113 [==============================] - 50s 444ms/step - loss: 0.5048 - categorical_accuracy: 0.8467 - val_loss: 0.6696 - val_categorical_accuracy: 0.8000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁▁▁▁▃▃▃▃▄▄▅▆▆▇▆▇▆▇▇▇▇▇▇█▇█▇▇▇█▇██▇▇████▇</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▅▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁▁▁▂▂▃▃▄▄▄▃▅▅▇▇▆▆▆▇▆▅▅▇▇▇▇▇▇▇█████▇████▇</td></tr><tr><td>epoch/val_loss</td><td>█▅▄▃▃▃▃▃▂▃▃▂▂▁▂▂▂▂▁▂▃▂▁▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.84667</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>0.50475</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.8</td></tr><tr><td>epoch/val_loss</td><td>0.66957</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM64-Dense128-Dense256-allfeatures_bigger_reg</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/b2cvthn0' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/b2cvthn0</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240408_085904-b2cvthn0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fda6bc02690>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(64, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, 1629),\n",
    "               kernel_regularizer=l2(0.005), \n",
    "               activity_regularizer=l2(0.005)))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"LSTM-L64-D128-D256-reg=0.005.tf\"),\n",
    "            run_name=\"LSTM64-Dense128-Dense256-allfeatures_bigger_reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ehqx0ymo) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.010 MB uploaded\\r'), FloatProgress(value=0.5380860274477296, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM128-Dense128-Dense256-allfeatures</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/ehqx0ymo' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/ehqx0ymo</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240408_102806-ehqx0ymo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ehqx0ymo). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02c443cf9c24d67afce99e0dd73feaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111298907134268, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240408_102839-tfbow8kv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/tfbow8kv' target=\"_blank\">LSTM128-Dense128-Dense256-allfeatures</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/tfbow8kv' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/tfbow8kv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.1593 - categorical_accuracy: 0.4111INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 59s 497ms/step - loss: 1.1593 - categorical_accuracy: 0.4111 - val_loss: 1.1264 - val_categorical_accuracy: 0.4300\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - 50s 440ms/step - loss: 1.1159 - categorical_accuracy: 0.4122 - val_loss: 1.0794 - val_categorical_accuracy: 0.4300\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - 50s 444ms/step - loss: 1.0872 - categorical_accuracy: 0.4156 - val_loss: 1.0639 - val_categorical_accuracy: 0.4300\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0789 - categorical_accuracy: 0.4222INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 504ms/step - loss: 1.0789 - categorical_accuracy: 0.4222 - val_loss: 1.0364 - val_categorical_accuracy: 0.4500\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0382 - categorical_accuracy: 0.4522INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 504ms/step - loss: 1.0382 - categorical_accuracy: 0.4522 - val_loss: 0.9582 - val_categorical_accuracy: 0.4800\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.9295 - categorical_accuracy: 0.5167INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 56s 500ms/step - loss: 0.9295 - categorical_accuracy: 0.5167 - val_loss: 0.8747 - val_categorical_accuracy: 0.5400\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.8825 - categorical_accuracy: 0.5411 - val_loss: 0.8312 - val_categorical_accuracy: 0.5300\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8269 - categorical_accuracy: 0.5622INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 509ms/step - loss: 0.8269 - categorical_accuracy: 0.5622 - val_loss: 0.8254 - val_categorical_accuracy: 0.5500\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8556 - categorical_accuracy: 0.5433INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 507ms/step - loss: 0.8556 - categorical_accuracy: 0.5433 - val_loss: 0.8159 - val_categorical_accuracy: 0.6200\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.8348 - categorical_accuracy: 0.5756 - val_loss: 0.7958 - val_categorical_accuracy: 0.5500\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - 50s 442ms/step - loss: 0.8885 - categorical_accuracy: 0.5233 - val_loss: 0.9448 - val_categorical_accuracy: 0.5100\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.8151 - categorical_accuracy: 0.5722 - val_loss: 0.8031 - val_categorical_accuracy: 0.5500\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.7978 - categorical_accuracy: 0.5844 - val_loss: 0.8093 - val_categorical_accuracy: 0.5400\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - 50s 444ms/step - loss: 0.7737 - categorical_accuracy: 0.5944 - val_loss: 0.7904 - val_categorical_accuracy: 0.5900\n",
      "Epoch 15/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.7715 - categorical_accuracy: 0.6011 - val_loss: 0.7445 - val_categorical_accuracy: 0.6100\n",
      "Epoch 16/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.7729 - categorical_accuracy: 0.6289 - val_loss: 0.7602 - val_categorical_accuracy: 0.6000\n",
      "Epoch 17/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7333 - categorical_accuracy: 0.6744INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 506ms/step - loss: 0.7333 - categorical_accuracy: 0.6744 - val_loss: 0.7445 - val_categorical_accuracy: 0.6900\n",
      "Epoch 18/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6788 - categorical_accuracy: 0.7311INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 503ms/step - loss: 0.6788 - categorical_accuracy: 0.7311 - val_loss: 0.6787 - val_categorical_accuracy: 0.7300\n",
      "Epoch 19/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6468 - categorical_accuracy: 0.7200INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 508ms/step - loss: 0.6468 - categorical_accuracy: 0.7200 - val_loss: 0.5513 - val_categorical_accuracy: 0.8400\n",
      "Epoch 20/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.6418 - categorical_accuracy: 0.7500 - val_loss: 0.5769 - val_categorical_accuracy: 0.7900\n",
      "Epoch 21/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.5473 - categorical_accuracy: 0.8067 - val_loss: 0.5113 - val_categorical_accuracy: 0.8200\n",
      "Epoch 22/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.5497 - categorical_accuracy: 0.8022 - val_loss: 0.4657 - val_categorical_accuracy: 0.8200\n",
      "Epoch 23/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5541 - categorical_accuracy: 0.7900INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 509ms/step - loss: 0.5541 - categorical_accuracy: 0.7900 - val_loss: 0.5049 - val_categorical_accuracy: 0.8500\n",
      "Epoch 24/100\n",
      "113/113 [==============================] - 50s 444ms/step - loss: 0.5154 - categorical_accuracy: 0.7989 - val_loss: 0.5346 - val_categorical_accuracy: 0.7900\n",
      "Epoch 25/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.5237 - categorical_accuracy: 0.8133 - val_loss: 0.4664 - val_categorical_accuracy: 0.8300\n",
      "Epoch 26/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.5831 - categorical_accuracy: 0.8033 - val_loss: 0.5966 - val_categorical_accuracy: 0.8000\n",
      "Epoch 27/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.5225 - categorical_accuracy: 0.8033 - val_loss: 0.7948 - val_categorical_accuracy: 0.7300\n",
      "Epoch 28/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.5959 - categorical_accuracy: 0.7822 - val_loss: 0.6207 - val_categorical_accuracy: 0.7900\n",
      "Epoch 29/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4713 - categorical_accuracy: 0.8367INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 512ms/step - loss: 0.4713 - categorical_accuracy: 0.8367 - val_loss: 0.4706 - val_categorical_accuracy: 0.8600\n",
      "Epoch 30/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.4891 - categorical_accuracy: 0.8333 - val_loss: 0.4781 - val_categorical_accuracy: 0.8400\n",
      "Epoch 31/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.5176 - categorical_accuracy: 0.8200 - val_loss: 0.5093 - val_categorical_accuracy: 0.8400\n",
      "Epoch 32/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.4791 - categorical_accuracy: 0.8400 - val_loss: 0.4815 - val_categorical_accuracy: 0.8500\n",
      "Epoch 33/100\n",
      "113/113 [==============================] - 50s 448ms/step - loss: 0.4326 - categorical_accuracy: 0.8689 - val_loss: 0.5219 - val_categorical_accuracy: 0.8000\n",
      "Epoch 34/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4221 - categorical_accuracy: 0.8611INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 509ms/step - loss: 0.4221 - categorical_accuracy: 0.8611 - val_loss: 0.4338 - val_categorical_accuracy: 0.8700\n",
      "Epoch 35/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.4176 - categorical_accuracy: 0.8600 - val_loss: 0.5111 - val_categorical_accuracy: 0.8100\n",
      "Epoch 36/100\n",
      "113/113 [==============================] - 50s 441ms/step - loss: 0.3978 - categorical_accuracy: 0.8722 - val_loss: 0.4874 - val_categorical_accuracy: 0.8100\n",
      "Epoch 37/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4454 - categorical_accuracy: 0.8511INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 504ms/step - loss: 0.4454 - categorical_accuracy: 0.8511 - val_loss: 0.3816 - val_categorical_accuracy: 0.8900\n",
      "Epoch 38/100\n",
      "113/113 [==============================] - 49s 437ms/step - loss: 0.3786 - categorical_accuracy: 0.8889 - val_loss: 0.4801 - val_categorical_accuracy: 0.8500\n",
      "Epoch 39/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.3902 - categorical_accuracy: 0.8922 - val_loss: 0.5937 - val_categorical_accuracy: 0.8000\n",
      "Epoch 40/100\n",
      "113/113 [==============================] - 50s 441ms/step - loss: 0.3852 - categorical_accuracy: 0.8889 - val_loss: 0.4676 - val_categorical_accuracy: 0.8500\n",
      "Epoch 41/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.3807 - categorical_accuracy: 0.8856 - val_loss: 0.4871 - val_categorical_accuracy: 0.8300\n",
      "Epoch 42/100\n",
      "113/113 [==============================] - 50s 444ms/step - loss: 0.3750 - categorical_accuracy: 0.8822 - val_loss: 0.4893 - val_categorical_accuracy: 0.8600\n",
      "Epoch 43/100\n",
      "113/113 [==============================] - 50s 441ms/step - loss: 0.4044 - categorical_accuracy: 0.8911 - val_loss: 0.6310 - val_categorical_accuracy: 0.7900\n",
      "Epoch 44/100\n",
      "113/113 [==============================] - 50s 442ms/step - loss: 0.3771 - categorical_accuracy: 0.8867 - val_loss: 0.4822 - val_categorical_accuracy: 0.8500\n",
      "Epoch 45/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.3903 - categorical_accuracy: 0.8844INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 56s 499ms/step - loss: 0.3903 - categorical_accuracy: 0.8844 - val_loss: 0.3689 - val_categorical_accuracy: 0.9000\n",
      "Epoch 46/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.3459 - categorical_accuracy: 0.9078 - val_loss: 0.3768 - val_categorical_accuracy: 0.9000\n",
      "Epoch 47/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3704 - categorical_accuracy: 0.8956 - val_loss: 0.4289 - val_categorical_accuracy: 0.8800\n",
      "Epoch 48/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.3126 - categorical_accuracy: 0.9122 - val_loss: 0.4029 - val_categorical_accuracy: 0.8800\n",
      "Epoch 49/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3530 - categorical_accuracy: 0.8922 - val_loss: 0.4759 - val_categorical_accuracy: 0.8700\n",
      "Epoch 50/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.4171 - categorical_accuracy: 0.8767 - val_loss: 0.6324 - val_categorical_accuracy: 0.7900\n",
      "Epoch 51/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.4903 - categorical_accuracy: 0.8400 - val_loss: 0.4580 - val_categorical_accuracy: 0.8600\n",
      "Epoch 52/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.5004 - categorical_accuracy: 0.8322 - val_loss: 0.7386 - val_categorical_accuracy: 0.7800\n",
      "Epoch 53/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.5495 - categorical_accuracy: 0.8144 - val_loss: 0.5707 - val_categorical_accuracy: 0.8200\n",
      "Epoch 54/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.4326 - categorical_accuracy: 0.8522 - val_loss: 0.4949 - val_categorical_accuracy: 0.8600\n",
      "Epoch 55/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.4168 - categorical_accuracy: 0.8711 - val_loss: 0.3718 - val_categorical_accuracy: 0.8700\n",
      "Epoch 56/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.4029 - categorical_accuracy: 0.8656 - val_loss: 0.3996 - val_categorical_accuracy: 0.8900\n",
      "Epoch 57/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.3701 - categorical_accuracy: 0.8833 - val_loss: 0.4646 - val_categorical_accuracy: 0.8700\n",
      "Epoch 58/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.3946 - categorical_accuracy: 0.8789 - val_loss: 0.7404 - val_categorical_accuracy: 0.7200\n",
      "Epoch 59/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.3892 - categorical_accuracy: 0.8789 - val_loss: 0.4167 - val_categorical_accuracy: 0.8900\n",
      "Epoch 60/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.3286 - categorical_accuracy: 0.8978 - val_loss: 0.4500 - val_categorical_accuracy: 0.8700\n",
      "Epoch 61/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.4057 - categorical_accuracy: 0.8778 - val_loss: 0.5199 - val_categorical_accuracy: 0.8400\n",
      "Epoch 62/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.4515 - categorical_accuracy: 0.8611 - val_loss: 0.4424 - val_categorical_accuracy: 0.8700\n",
      "Epoch 63/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.4480 - categorical_accuracy: 0.8656 - val_loss: 0.4351 - val_categorical_accuracy: 0.8900\n",
      "Epoch 64/100\n",
      "113/113 [==============================] - 51s 447ms/step - loss: 0.3490 - categorical_accuracy: 0.8967 - val_loss: 0.5264 - val_categorical_accuracy: 0.8400\n",
      "Epoch 65/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3577 - categorical_accuracy: 0.9011 - val_loss: 0.4346 - val_categorical_accuracy: 0.8300\n",
      "Epoch 66/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.3784 - categorical_accuracy: 0.8800 - val_loss: 0.4018 - val_categorical_accuracy: 0.8700\n",
      "Epoch 67/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.3576 - categorical_accuracy: 0.8956 - val_loss: 0.5138 - val_categorical_accuracy: 0.8200\n",
      "Epoch 68/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.3531 - categorical_accuracy: 0.8933 - val_loss: 0.4497 - val_categorical_accuracy: 0.8500\n",
      "Epoch 69/100\n",
      "113/113 [==============================] - 51s 446ms/step - loss: 0.3818 - categorical_accuracy: 0.8944 - val_loss: 0.5373 - val_categorical_accuracy: 0.8200\n",
      "Epoch 70/100\n",
      "113/113 [==============================] - 51s 447ms/step - loss: 0.3838 - categorical_accuracy: 0.8922 - val_loss: 0.5568 - val_categorical_accuracy: 0.8300\n",
      "Epoch 71/100\n",
      "113/113 [==============================] - 50s 443ms/step - loss: 0.2919 - categorical_accuracy: 0.9233 - val_loss: 0.4551 - val_categorical_accuracy: 0.8700\n",
      "Epoch 72/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3211 - categorical_accuracy: 0.9078 - val_loss: 0.4649 - val_categorical_accuracy: 0.8700\n",
      "Epoch 73/100\n",
      "113/113 [==============================] - 50s 446ms/step - loss: 0.3066 - categorical_accuracy: 0.9122 - val_loss: 0.4409 - val_categorical_accuracy: 0.8600\n",
      "Epoch 74/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.3514 - categorical_accuracy: 0.8922 - val_loss: 0.4000 - val_categorical_accuracy: 0.8900\n",
      "Epoch 75/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.3193 - categorical_accuracy: 0.9122 - val_loss: 0.3849 - val_categorical_accuracy: 0.8700\n",
      "Epoch 76/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.3559 - categorical_accuracy: 0.8922 - val_loss: 0.4093 - val_categorical_accuracy: 0.8900\n",
      "Epoch 77/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.2585 - categorical_accuracy: 0.9344 - val_loss: 0.3891 - val_categorical_accuracy: 0.8900\n",
      "Epoch 78/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.2845 - categorical_accuracy: 0.9200INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM128-Dense128-Dense256-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 506ms/step - loss: 0.2845 - categorical_accuracy: 0.9200 - val_loss: 0.3175 - val_categorical_accuracy: 0.9200\n",
      "Epoch 79/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.3147 - categorical_accuracy: 0.9156 - val_loss: 0.3852 - val_categorical_accuracy: 0.8900\n",
      "Epoch 80/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.2673 - categorical_accuracy: 0.9200 - val_loss: 0.3926 - val_categorical_accuracy: 0.9000\n",
      "Epoch 81/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.2707 - categorical_accuracy: 0.9256 - val_loss: 0.3431 - val_categorical_accuracy: 0.9000\n",
      "Epoch 82/100\n",
      "  6/113 [>.............................] - ETA: 38s - loss: 0.2516 - categorical_accuracy: 0.9375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(128, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, 1629),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"LSTM128-Dense128-Dense256-allfeatures.tf\"),\n",
    "            run_name=\"LSTM128-Dense128-Dense256-allfeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:vxwg2tiq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁</td></tr><tr><td>epoch/epoch</td><td>▁</td></tr><tr><td>epoch/learning_rate</td><td>▁</td></tr><tr><td>epoch/loss</td><td>▁</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁</td></tr><tr><td>epoch/val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.41667</td></tr><tr><td>epoch/epoch</td><td>0</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>1.13316</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.44</td></tr><tr><td>epoch/val_loss</td><td>1.11618</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM64-Dense64-allfeatures</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/vxwg2tiq' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/vxwg2tiq</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240408_124116-vxwg2tiq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:vxwg2tiq). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a843215891450f8950dbbea742785b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112615217765172, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240408_124402-tyh5w2ae</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/tyh5w2ae' target=\"_blank\">LSTM64-Dense64-allfeatures</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/tyh5w2ae' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/tyh5w2ae</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.1343 - categorical_accuracy: 0.4444INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 59s 497ms/step - loss: 1.1343 - categorical_accuracy: 0.4444 - val_loss: 1.1175 - val_categorical_accuracy: 0.4300\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 1.1170 - categorical_accuracy: 0.4256 - val_loss: 1.0923 - val_categorical_accuracy: 0.4300\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - 52s 454ms/step - loss: 1.0976 - categorical_accuracy: 0.4200 - val_loss: 1.0688 - val_categorical_accuracy: 0.4300\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 1.0772 - categorical_accuracy: 0.4267 - val_loss: 1.0476 - val_categorical_accuracy: 0.4300\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 1.0660 - categorical_accuracy: 0.4222 - val_loss: 1.0532 - val_categorical_accuracy: 0.4300\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 1.0527 - categorical_accuracy: 0.4122 - val_loss: 1.0162 - val_categorical_accuracy: 0.4300\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0104 - categorical_accuracy: 0.4322INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 518ms/step - loss: 1.0104 - categorical_accuracy: 0.4322 - val_loss: 0.9230 - val_categorical_accuracy: 0.5300\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.9788 - categorical_accuracy: 0.4911 - val_loss: 1.0028 - val_categorical_accuracy: 0.4700\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.9554 - categorical_accuracy: 0.5089INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 512ms/step - loss: 0.9554 - categorical_accuracy: 0.5089 - val_loss: 0.9205 - val_categorical_accuracy: 0.5400\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8856 - categorical_accuracy: 0.5389INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 514ms/step - loss: 0.8856 - categorical_accuracy: 0.5389 - val_loss: 0.8232 - val_categorical_accuracy: 0.5600\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.8843 - categorical_accuracy: 0.5400 - val_loss: 0.8389 - val_categorical_accuracy: 0.5600\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.8609 - categorical_accuracy: 0.5311 - val_loss: 0.8181 - val_categorical_accuracy: 0.5600\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.8422 - categorical_accuracy: 0.5622 - val_loss: 0.8075 - val_categorical_accuracy: 0.5500\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.8400 - categorical_accuracy: 0.5556 - val_loss: 0.8344 - val_categorical_accuracy: 0.5600\n",
      "Epoch 15/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.8375 - categorical_accuracy: 0.5533 - val_loss: 0.8198 - val_categorical_accuracy: 0.5400\n",
      "Epoch 16/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.8186 - categorical_accuracy: 0.5600 - val_loss: 0.8137 - val_categorical_accuracy: 0.5500\n",
      "Epoch 17/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.8218 - categorical_accuracy: 0.5478 - val_loss: 0.8052 - val_categorical_accuracy: 0.5300\n",
      "Epoch 18/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.8168 - categorical_accuracy: 0.5567 - val_loss: 0.8125 - val_categorical_accuracy: 0.5600\n",
      "Epoch 19/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7975 - categorical_accuracy: 0.5889INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 514ms/step - loss: 0.7975 - categorical_accuracy: 0.5889 - val_loss: 0.7677 - val_categorical_accuracy: 0.5700\n",
      "Epoch 20/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7678 - categorical_accuracy: 0.5900INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 510ms/step - loss: 0.7678 - categorical_accuracy: 0.5900 - val_loss: 0.7368 - val_categorical_accuracy: 0.6100\n",
      "Epoch 21/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.7924 - categorical_accuracy: 0.5856 - val_loss: 0.8150 - val_categorical_accuracy: 0.5900\n",
      "Epoch 22/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.7752 - categorical_accuracy: 0.6200 - val_loss: 0.8004 - val_categorical_accuracy: 0.5900\n",
      "Epoch 23/100\n",
      "113/113 [==============================] - 51s 446ms/step - loss: 0.7590 - categorical_accuracy: 0.6222 - val_loss: 0.7709 - val_categorical_accuracy: 0.6000\n",
      "Epoch 24/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7470 - categorical_accuracy: 0.6400INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 503ms/step - loss: 0.7470 - categorical_accuracy: 0.6400 - val_loss: 0.7407 - val_categorical_accuracy: 0.6800\n",
      "Epoch 25/100\n",
      "113/113 [==============================] - 51s 456ms/step - loss: 0.6949 - categorical_accuracy: 0.6856 - val_loss: 0.6845 - val_categorical_accuracy: 0.6800\n",
      "Epoch 26/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7088 - categorical_accuracy: 0.7156INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 515ms/step - loss: 0.7088 - categorical_accuracy: 0.7156 - val_loss: 0.6359 - val_categorical_accuracy: 0.7800\n",
      "Epoch 27/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.6057 - categorical_accuracy: 0.7789 - val_loss: 0.6122 - val_categorical_accuracy: 0.7800\n",
      "Epoch 28/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.5726 - categorical_accuracy: 0.7922 - val_loss: 0.5490 - val_categorical_accuracy: 0.7800\n",
      "Epoch 29/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.5773 - categorical_accuracy: 0.7700 - val_loss: 0.6536 - val_categorical_accuracy: 0.7500\n",
      "Epoch 30/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.6203 - categorical_accuracy: 0.7578 - val_loss: 0.5957 - val_categorical_accuracy: 0.7600\n",
      "Epoch 31/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5099 - categorical_accuracy: 0.8278INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 59s 525ms/step - loss: 0.5099 - categorical_accuracy: 0.8278 - val_loss: 0.5750 - val_categorical_accuracy: 0.7900\n",
      "Epoch 32/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.5296 - categorical_accuracy: 0.8056 - val_loss: 0.6380 - val_categorical_accuracy: 0.7300\n",
      "Epoch 33/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.5025 - categorical_accuracy: 0.8167 - val_loss: 0.6393 - val_categorical_accuracy: 0.7700\n",
      "Epoch 34/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.5316 - categorical_accuracy: 0.8089 - val_loss: 0.6577 - val_categorical_accuracy: 0.7400\n",
      "Epoch 35/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5025 - categorical_accuracy: 0.8278INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 503ms/step - loss: 0.5025 - categorical_accuracy: 0.8278 - val_loss: 0.5503 - val_categorical_accuracy: 0.8000\n",
      "Epoch 36/100\n",
      "113/113 [==============================] - 52s 460ms/step - loss: 0.4798 - categorical_accuracy: 0.8322 - val_loss: 0.5658 - val_categorical_accuracy: 0.7900\n",
      "Epoch 37/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4456 - categorical_accuracy: 0.8544INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 510ms/step - loss: 0.4456 - categorical_accuracy: 0.8544 - val_loss: 0.5746 - val_categorical_accuracy: 0.8100\n",
      "Epoch 38/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.4612 - categorical_accuracy: 0.8422 - val_loss: 0.6220 - val_categorical_accuracy: 0.7800\n",
      "Epoch 39/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.4847 - categorical_accuracy: 0.8411 - val_loss: 0.5608 - val_categorical_accuracy: 0.7900\n",
      "Epoch 40/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.4923 - categorical_accuracy: 0.8311 - val_loss: 0.5668 - val_categorical_accuracy: 0.7900\n",
      "Epoch 41/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.4842 - categorical_accuracy: 0.8278 - val_loss: 0.7195 - val_categorical_accuracy: 0.7000\n",
      "Epoch 42/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.4352 - categorical_accuracy: 0.8600 - val_loss: 0.5334 - val_categorical_accuracy: 0.8000\n",
      "Epoch 43/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4023 - categorical_accuracy: 0.8844INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 59s 523ms/step - loss: 0.4023 - categorical_accuracy: 0.8844 - val_loss: 0.4396 - val_categorical_accuracy: 0.8700\n",
      "Epoch 44/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3983 - categorical_accuracy: 0.8722 - val_loss: 0.6895 - val_categorical_accuracy: 0.7600\n",
      "Epoch 45/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.4343 - categorical_accuracy: 0.8678 - val_loss: 0.4985 - val_categorical_accuracy: 0.8100\n",
      "Epoch 46/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.3535 - categorical_accuracy: 0.8922 - val_loss: 0.7079 - val_categorical_accuracy: 0.7500\n",
      "Epoch 47/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.4056 - categorical_accuracy: 0.8678 - val_loss: 0.4280 - val_categorical_accuracy: 0.8600\n",
      "Epoch 48/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.3670 - categorical_accuracy: 0.8867 - val_loss: 0.6978 - val_categorical_accuracy: 0.7400\n",
      "Epoch 49/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.4121 - categorical_accuracy: 0.8656 - val_loss: 0.7472 - val_categorical_accuracy: 0.7600\n",
      "Epoch 50/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.3920 - categorical_accuracy: 0.8778 - val_loss: 0.4649 - val_categorical_accuracy: 0.8300\n",
      "Epoch 51/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.3776 - categorical_accuracy: 0.8800 - val_loss: 0.5739 - val_categorical_accuracy: 0.8000\n",
      "Epoch 52/100\n",
      "113/113 [==============================] - 51s 456ms/step - loss: 0.3885 - categorical_accuracy: 0.8822 - val_loss: 0.6278 - val_categorical_accuracy: 0.7800\n",
      "Epoch 53/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.3540 - categorical_accuracy: 0.8956 - val_loss: 0.5146 - val_categorical_accuracy: 0.8400\n",
      "Epoch 54/100\n",
      "113/113 [==============================] - 52s 463ms/step - loss: 0.3355 - categorical_accuracy: 0.9022 - val_loss: 0.4200 - val_categorical_accuracy: 0.8600\n",
      "Epoch 55/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.3524 - categorical_accuracy: 0.8911 - val_loss: 0.4268 - val_categorical_accuracy: 0.8500\n",
      "Epoch 56/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.3640 - categorical_accuracy: 0.8911 - val_loss: 0.4965 - val_categorical_accuracy: 0.8400\n",
      "Epoch 57/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.3157 - categorical_accuracy: 0.9111 - val_loss: 0.4252 - val_categorical_accuracy: 0.8500\n",
      "Epoch 58/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.3352 - categorical_accuracy: 0.9033 - val_loss: 0.7226 - val_categorical_accuracy: 0.7800\n",
      "Epoch 59/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.3047 - categorical_accuracy: 0.9144 - val_loss: 0.4384 - val_categorical_accuracy: 0.8700\n",
      "Epoch 60/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.3595 - categorical_accuracy: 0.8933 - val_loss: 0.4577 - val_categorical_accuracy: 0.8600\n",
      "Epoch 61/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.2885 - categorical_accuracy: 0.9200INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 508ms/step - loss: 0.2885 - categorical_accuracy: 0.9200 - val_loss: 0.4066 - val_categorical_accuracy: 0.8900\n",
      "Epoch 62/100\n",
      "113/113 [==============================] - 52s 460ms/step - loss: 0.3012 - categorical_accuracy: 0.9133 - val_loss: 0.4599 - val_categorical_accuracy: 0.8600\n",
      "Epoch 63/100\n",
      "113/113 [==============================] - 51s 457ms/step - loss: 0.2946 - categorical_accuracy: 0.9133 - val_loss: 0.5087 - val_categorical_accuracy: 0.8200\n",
      "Epoch 64/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.2807 - categorical_accuracy: 0.9178 - val_loss: 0.4382 - val_categorical_accuracy: 0.8700\n",
      "Epoch 65/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.2937 - categorical_accuracy: 0.9189 - val_loss: 0.4842 - val_categorical_accuracy: 0.8600\n",
      "Epoch 66/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.3033 - categorical_accuracy: 0.9200 - val_loss: 0.4485 - val_categorical_accuracy: 0.8700\n",
      "Epoch 67/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.3023 - categorical_accuracy: 0.9122 - val_loss: 0.4437 - val_categorical_accuracy: 0.8600\n",
      "Epoch 68/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.2777 - categorical_accuracy: 0.9244 - val_loss: 0.4381 - val_categorical_accuracy: 0.8700\n",
      "Epoch 69/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.2983 - categorical_accuracy: 0.9200 - val_loss: 0.3792 - val_categorical_accuracy: 0.8900\n",
      "Epoch 70/100\n",
      "113/113 [==============================] - 51s 456ms/step - loss: 0.3051 - categorical_accuracy: 0.9122 - val_loss: 0.4292 - val_categorical_accuracy: 0.8800\n",
      "Epoch 71/100\n",
      "113/113 [==============================] - 52s 457ms/step - loss: 0.3089 - categorical_accuracy: 0.9144 - val_loss: 0.4018 - val_categorical_accuracy: 0.8800\n",
      "Epoch 72/100\n",
      "113/113 [==============================] - 52s 457ms/step - loss: 0.2753 - categorical_accuracy: 0.9244 - val_loss: 0.4442 - val_categorical_accuracy: 0.8700\n",
      "Epoch 73/100\n",
      "113/113 [==============================] - 52s 459ms/step - loss: 0.3125 - categorical_accuracy: 0.9044 - val_loss: 0.5365 - val_categorical_accuracy: 0.8400\n",
      "Epoch 74/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.2727 - categorical_accuracy: 0.9278 - val_loss: 0.4091 - val_categorical_accuracy: 0.8700\n",
      "Epoch 75/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.4557 - categorical_accuracy: 0.8600 - val_loss: 0.4815 - val_categorical_accuracy: 0.8500\n",
      "Epoch 76/100\n",
      "113/113 [==============================] - 51s 456ms/step - loss: 0.4240 - categorical_accuracy: 0.8578 - val_loss: 0.4209 - val_categorical_accuracy: 0.8900\n",
      "Epoch 77/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.4365 - categorical_accuracy: 0.8600 - val_loss: 0.6409 - val_categorical_accuracy: 0.8000\n",
      "Epoch 78/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.4681 - categorical_accuracy: 0.8456 - val_loss: 0.7268 - val_categorical_accuracy: 0.7500\n",
      "Epoch 79/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.4132 - categorical_accuracy: 0.8644 - val_loss: 0.4595 - val_categorical_accuracy: 0.8700\n",
      "Epoch 80/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.4216 - categorical_accuracy: 0.8600 - val_loss: 0.9288 - val_categorical_accuracy: 0.6800\n",
      "Epoch 81/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.4877 - categorical_accuracy: 0.8344 - val_loss: 0.6403 - val_categorical_accuracy: 0.7500\n",
      "Epoch 82/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.4549 - categorical_accuracy: 0.8511 - val_loss: 0.4718 - val_categorical_accuracy: 0.8200\n",
      "Epoch 83/100\n",
      "113/113 [==============================] - 52s 459ms/step - loss: 0.4132 - categorical_accuracy: 0.8622 - val_loss: 0.5724 - val_categorical_accuracy: 0.8000\n",
      "Epoch 84/100\n",
      "113/113 [==============================] - 52s 461ms/step - loss: 0.4099 - categorical_accuracy: 0.8733 - val_loss: 0.3596 - val_categorical_accuracy: 0.8900\n",
      "Epoch 85/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.3584 - categorical_accuracy: 0.8867 - val_loss: 0.5718 - val_categorical_accuracy: 0.8200\n",
      "Epoch 86/100\n",
      "113/113 [==============================] - 52s 457ms/step - loss: 0.3383 - categorical_accuracy: 0.8967 - val_loss: 0.5884 - val_categorical_accuracy: 0.8100\n",
      "Epoch 87/100\n",
      "113/113 [==============================] - 52s 459ms/step - loss: 0.3716 - categorical_accuracy: 0.8844 - val_loss: 0.4360 - val_categorical_accuracy: 0.8800\n",
      "Epoch 88/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.3714 - categorical_accuracy: 0.8889 - val_loss: 0.4109 - val_categorical_accuracy: 0.8600\n",
      "Epoch 89/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.3686 - categorical_accuracy: 0.8833 - val_loss: 0.4547 - val_categorical_accuracy: 0.8600\n",
      "Epoch 90/100\n",
      "113/113 [==============================] - 52s 462ms/step - loss: 0.3794 - categorical_accuracy: 0.8800 - val_loss: 0.4483 - val_categorical_accuracy: 0.8600\n",
      "Epoch 91/100\n",
      "113/113 [==============================] - 52s 460ms/step - loss: 0.3467 - categorical_accuracy: 0.8967 - val_loss: 0.4917 - val_categorical_accuracy: 0.8300\n",
      "Epoch 92/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4276 - categorical_accuracy: 0.8656INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense64-allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 514ms/step - loss: 0.4276 - categorical_accuracy: 0.8656 - val_loss: 0.3346 - val_categorical_accuracy: 0.9000\n",
      "Epoch 93/100\n",
      "113/113 [==============================] - 52s 459ms/step - loss: 0.3803 - categorical_accuracy: 0.8844 - val_loss: 0.5051 - val_categorical_accuracy: 0.8300\n",
      "Epoch 94/100\n",
      "113/113 [==============================] - 51s 456ms/step - loss: 0.3907 - categorical_accuracy: 0.8700 - val_loss: 0.4872 - val_categorical_accuracy: 0.8200\n",
      "Epoch 95/100\n",
      "113/113 [==============================] - 52s 460ms/step - loss: 0.3270 - categorical_accuracy: 0.9056 - val_loss: 0.4560 - val_categorical_accuracy: 0.8700\n",
      "Epoch 96/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.3558 - categorical_accuracy: 0.8944 - val_loss: 0.3986 - val_categorical_accuracy: 0.8800\n",
      "Epoch 97/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.3054 - categorical_accuracy: 0.9033 - val_loss: 0.4988 - val_categorical_accuracy: 0.8500\n",
      "Epoch 98/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.3453 - categorical_accuracy: 0.8889 - val_loss: 0.5048 - val_categorical_accuracy: 0.8400\n",
      "Epoch 99/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.3556 - categorical_accuracy: 0.8967 - val_loss: 0.3912 - val_categorical_accuracy: 0.8700\n",
      "Epoch 100/100\n",
      "113/113 [==============================] - 52s 457ms/step - loss: 0.3030 - categorical_accuracy: 0.9111 - val_loss: 0.7505 - val_categorical_accuracy: 0.7400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁▁▁▂▃▃▃▃▃▄▅▆▇▆▇▇▇▇█▇▇█████████▇▇▇▇▇▇▇▇██</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>█████▇▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁█████▇▇▇▇▆</td></tr><tr><td>epoch/loss</td><td>██▇▇▆▆▅▅▅▅▅▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▁▁</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁▁▁▂▃▃▃▃▃▄▆▆▆▆▆▆▅▆▆▆▇▇▇███▇███▇█▇██▇█▇▇▆</td></tr><tr><td>epoch/val_loss</td><td>██▇▇▆▅▅▅▅▅▄▃▃▄▃▃▄▄▄▅▃▂▂▂▂▂▂▁▂▂▄▂▂▁▂▂▁▂▂▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.91111</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>8e-05</td></tr><tr><td>epoch/loss</td><td>0.30305</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.74</td></tr><tr><td>epoch/val_loss</td><td>0.75053</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM64-Dense64-allfeatures</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/tyh5w2ae' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/tyh5w2ae</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240408_124402-tyh5w2ae/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fda527cdad0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(64, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, 1629),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"LSTM64-Dense64-allfeatures.tf\"),\n",
    "            run_name=\"LSTM64-Dense64-allfeatures\", T_max=75, epochs=100, \n",
    "            max_lr = 1e-4, min_lr = 2.5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240408_141317-jhepc329</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/jhepc329' target=\"_blank\">LSTM64-Dense128-allfeatures</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/jhepc329' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/jhepc329</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.1320 - categorical_accuracy: 0.3978INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 67s 557ms/step - loss: 1.1320 - categorical_accuracy: 0.3978 - val_loss: 1.1157 - val_categorical_accuracy: 0.4100\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.1137 - categorical_accuracy: 0.4156INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 517ms/step - loss: 1.1137 - categorical_accuracy: 0.4156 - val_loss: 1.0914 - val_categorical_accuracy: 0.4600\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 1.0936 - categorical_accuracy: 0.4167 - val_loss: 1.0644 - val_categorical_accuracy: 0.4300\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0683 - categorical_accuracy: 0.3889INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 56s 501ms/step - loss: 1.0683 - categorical_accuracy: 0.3889 - val_loss: 1.0305 - val_categorical_accuracy: 0.4700\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - 52s 457ms/step - loss: 1.0345 - categorical_accuracy: 0.4256 - val_loss: 0.9855 - val_categorical_accuracy: 0.4300\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0035 - categorical_accuracy: 0.4456INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 59s 520ms/step - loss: 1.0035 - categorical_accuracy: 0.4456 - val_loss: 0.9437 - val_categorical_accuracy: 0.4900\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0103 - categorical_accuracy: 0.4544INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 513ms/step - loss: 1.0103 - categorical_accuracy: 0.4544 - val_loss: 0.9660 - val_categorical_accuracy: 0.5200\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.9675 - categorical_accuracy: 0.4767 - val_loss: 1.0216 - val_categorical_accuracy: 0.4900\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.9119 - categorical_accuracy: 0.5267INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 59s 517ms/step - loss: 0.9119 - categorical_accuracy: 0.5267 - val_loss: 0.8566 - val_categorical_accuracy: 0.5600\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.9029 - categorical_accuracy: 0.5400INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 59s 518ms/step - loss: 0.9029 - categorical_accuracy: 0.5400 - val_loss: 0.8066 - val_categorical_accuracy: 0.5800\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8586 - categorical_accuracy: 0.5511INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 511ms/step - loss: 0.8586 - categorical_accuracy: 0.5511 - val_loss: 0.8088 - val_categorical_accuracy: 0.5900\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - 52s 457ms/step - loss: 0.8127 - categorical_accuracy: 0.5533 - val_loss: 0.7929 - val_categorical_accuracy: 0.5800\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.8108 - categorical_accuracy: 0.5578 - val_loss: 0.7927 - val_categorical_accuracy: 0.5700\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.8790 - categorical_accuracy: 0.5344 - val_loss: 0.8307 - val_categorical_accuracy: 0.5500\n",
      "Epoch 15/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.7909 - categorical_accuracy: 0.5622 - val_loss: 0.8010 - val_categorical_accuracy: 0.5500\n",
      "Epoch 16/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.8140 - categorical_accuracy: 0.5711 - val_loss: 0.8049 - val_categorical_accuracy: 0.5600\n",
      "Epoch 17/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.8103 - categorical_accuracy: 0.5778INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 508ms/step - loss: 0.8103 - categorical_accuracy: 0.5778 - val_loss: 0.8149 - val_categorical_accuracy: 0.6000\n",
      "Epoch 18/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.8039 - categorical_accuracy: 0.5756 - val_loss: 0.7945 - val_categorical_accuracy: 0.5800\n",
      "Epoch 19/100\n",
      "113/113 [==============================] - 52s 457ms/step - loss: 0.7993 - categorical_accuracy: 0.5856 - val_loss: 0.7896 - val_categorical_accuracy: 0.5500\n",
      "Epoch 20/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.7795 - categorical_accuracy: 0.5911 - val_loss: 0.8051 - val_categorical_accuracy: 0.5300\n",
      "Epoch 21/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.7706 - categorical_accuracy: 0.6022 - val_loss: 0.7732 - val_categorical_accuracy: 0.5700\n",
      "Epoch 22/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.7396 - categorical_accuracy: 0.6156 - val_loss: 0.8178 - val_categorical_accuracy: 0.5900\n",
      "Epoch 23/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.7452 - categorical_accuracy: 0.6011 - val_loss: 0.7757 - val_categorical_accuracy: 0.5900\n",
      "Epoch 24/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.7296 - categorical_accuracy: 0.6189 - val_loss: 0.7378 - val_categorical_accuracy: 0.6000\n",
      "Epoch 25/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.7152 - categorical_accuracy: 0.6333 - val_loss: 0.7172 - val_categorical_accuracy: 0.5900\n",
      "Epoch 26/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6897 - categorical_accuracy: 0.6811INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 515ms/step - loss: 0.6897 - categorical_accuracy: 0.6811 - val_loss: 0.6801 - val_categorical_accuracy: 0.6400\n",
      "Epoch 27/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6367 - categorical_accuracy: 0.7233INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 512ms/step - loss: 0.6367 - categorical_accuracy: 0.7233 - val_loss: 0.6453 - val_categorical_accuracy: 0.7200\n",
      "Epoch 28/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5524 - categorical_accuracy: 0.7744INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 515ms/step - loss: 0.5524 - categorical_accuracy: 0.7744 - val_loss: 0.5173 - val_categorical_accuracy: 0.8400\n",
      "Epoch 29/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.5915 - categorical_accuracy: 0.7633 - val_loss: 0.6240 - val_categorical_accuracy: 0.7300\n",
      "Epoch 30/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.5347 - categorical_accuracy: 0.7911 - val_loss: 0.4735 - val_categorical_accuracy: 0.8400\n",
      "Epoch 31/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.5165 - categorical_accuracy: 0.7944 - val_loss: 0.4232 - val_categorical_accuracy: 0.8100\n",
      "Epoch 32/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.4915 - categorical_accuracy: 0.8178 - val_loss: 0.4979 - val_categorical_accuracy: 0.8100\n",
      "Epoch 33/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4810 - categorical_accuracy: 0.8333INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 518ms/step - loss: 0.4810 - categorical_accuracy: 0.8333 - val_loss: 0.4634 - val_categorical_accuracy: 0.8500\n",
      "Epoch 34/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4596 - categorical_accuracy: 0.8411INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 515ms/step - loss: 0.4596 - categorical_accuracy: 0.8411 - val_loss: 0.4470 - val_categorical_accuracy: 0.8600\n",
      "Epoch 35/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4790 - categorical_accuracy: 0.8244INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 516ms/step - loss: 0.4790 - categorical_accuracy: 0.8244 - val_loss: 0.3842 - val_categorical_accuracy: 0.8700\n",
      "Epoch 36/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.4455 - categorical_accuracy: 0.8478 - val_loss: 0.4243 - val_categorical_accuracy: 0.8600\n",
      "Epoch 37/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.4786 - categorical_accuracy: 0.8322 - val_loss: 0.4498 - val_categorical_accuracy: 0.8400\n",
      "Epoch 38/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.4863 - categorical_accuracy: 0.8233 - val_loss: 0.4660 - val_categorical_accuracy: 0.8300\n",
      "Epoch 39/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.4091 - categorical_accuracy: 0.8544 - val_loss: 0.4194 - val_categorical_accuracy: 0.8700\n",
      "Epoch 40/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.4525 - categorical_accuracy: 0.8433 - val_loss: 0.5789 - val_categorical_accuracy: 0.8100\n",
      "Epoch 41/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.3981 - categorical_accuracy: 0.8767INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 59s 520ms/step - loss: 0.3981 - categorical_accuracy: 0.8767 - val_loss: 0.2976 - val_categorical_accuracy: 0.9100\n",
      "Epoch 42/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.4388 - categorical_accuracy: 0.8467 - val_loss: 0.6030 - val_categorical_accuracy: 0.7800\n",
      "Epoch 43/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.4349 - categorical_accuracy: 0.8567 - val_loss: 0.3512 - val_categorical_accuracy: 0.8700\n",
      "Epoch 44/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3927 - categorical_accuracy: 0.8689 - val_loss: 0.4066 - val_categorical_accuracy: 0.8600\n",
      "Epoch 45/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.3653 - categorical_accuracy: 0.8867 - val_loss: 0.3749 - val_categorical_accuracy: 0.8900\n",
      "Epoch 46/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.3591 - categorical_accuracy: 0.8867 - val_loss: 0.5302 - val_categorical_accuracy: 0.8400\n",
      "Epoch 47/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.4030 - categorical_accuracy: 0.8644 - val_loss: 0.4458 - val_categorical_accuracy: 0.8300\n",
      "Epoch 48/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.3572 - categorical_accuracy: 0.8889 - val_loss: 0.3102 - val_categorical_accuracy: 0.9100\n",
      "Epoch 49/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.3615 - categorical_accuracy: 0.8844 - val_loss: 0.3618 - val_categorical_accuracy: 0.8900\n",
      "Epoch 50/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.3644 - categorical_accuracy: 0.8856 - val_loss: 0.3790 - val_categorical_accuracy: 0.8900\n",
      "Epoch 51/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.3868 - categorical_accuracy: 0.8800 - val_loss: 0.3699 - val_categorical_accuracy: 0.8800\n",
      "Epoch 52/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.3769 - categorical_accuracy: 0.8833 - val_loss: 0.3959 - val_categorical_accuracy: 0.8700\n",
      "Epoch 53/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.3448 - categorical_accuracy: 0.8978 - val_loss: 0.4130 - val_categorical_accuracy: 0.8800\n",
      "Epoch 54/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.3351 - categorical_accuracy: 0.9000 - val_loss: 0.5915 - val_categorical_accuracy: 0.7800\n",
      "Epoch 55/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.3425 - categorical_accuracy: 0.8944 - val_loss: 0.3477 - val_categorical_accuracy: 0.9000\n",
      "Epoch 56/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.3734 - categorical_accuracy: 0.8778 - val_loss: 0.4047 - val_categorical_accuracy: 0.8800\n",
      "Epoch 57/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3462 - categorical_accuracy: 0.8978 - val_loss: 0.4394 - val_categorical_accuracy: 0.8500\n",
      "Epoch 58/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.3559 - categorical_accuracy: 0.8944 - val_loss: 0.4795 - val_categorical_accuracy: 0.8300\n",
      "Epoch 59/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.3530 - categorical_accuracy: 0.8922 - val_loss: 0.3734 - val_categorical_accuracy: 0.8900\n",
      "Epoch 60/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.3217 - categorical_accuracy: 0.9044 - val_loss: 0.3667 - val_categorical_accuracy: 0.8900\n",
      "Epoch 61/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3137 - categorical_accuracy: 0.9100 - val_loss: 0.3653 - val_categorical_accuracy: 0.9100\n",
      "Epoch 62/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.3203 - categorical_accuracy: 0.9033 - val_loss: 0.3331 - val_categorical_accuracy: 0.9000\n",
      "Epoch 63/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.3133 - categorical_accuracy: 0.9033 - val_loss: 0.3486 - val_categorical_accuracy: 0.9000\n",
      "Epoch 64/100\n",
      "113/113 [==============================] - 52s 460ms/step - loss: 0.3524 - categorical_accuracy: 0.9022 - val_loss: 0.3530 - val_categorical_accuracy: 0.8900\n",
      "Epoch 65/100\n",
      "113/113 [==============================] - 52s 452ms/step - loss: 0.2923 - categorical_accuracy: 0.9189 - val_loss: 0.3919 - val_categorical_accuracy: 0.8800\n",
      "Epoch 66/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.3018 - categorical_accuracy: 0.9178 - val_loss: 0.5833 - val_categorical_accuracy: 0.8100\n",
      "Epoch 67/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.3623 - categorical_accuracy: 0.8911 - val_loss: 0.3731 - val_categorical_accuracy: 0.8900\n",
      "Epoch 68/100\n",
      "113/113 [==============================] - 52s 457ms/step - loss: 0.3082 - categorical_accuracy: 0.9022 - val_loss: 0.4030 - val_categorical_accuracy: 0.8600\n",
      "Epoch 69/100\n",
      "113/113 [==============================] - 52s 456ms/step - loss: 0.3097 - categorical_accuracy: 0.9122 - val_loss: 0.3692 - val_categorical_accuracy: 0.8800\n",
      "Epoch 70/100\n",
      "113/113 [==============================] - 52s 459ms/step - loss: 0.2826 - categorical_accuracy: 0.9233 - val_loss: 0.4324 - val_categorical_accuracy: 0.8600\n",
      "Epoch 71/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.2819 - categorical_accuracy: 0.9189 - val_loss: 0.5258 - val_categorical_accuracy: 0.8300\n",
      "Epoch 72/100\n",
      "113/113 [==============================] - 52s 459ms/step - loss: 0.3141 - categorical_accuracy: 0.9111 - val_loss: 0.4164 - val_categorical_accuracy: 0.8800\n",
      "Epoch 73/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.3128 - categorical_accuracy: 0.9156 - val_loss: 0.3882 - val_categorical_accuracy: 0.8800\n",
      "Epoch 74/100\n",
      "113/113 [==============================] - 51s 446ms/step - loss: 0.3042 - categorical_accuracy: 0.9144 - val_loss: 0.4380 - val_categorical_accuracy: 0.8700\n",
      "Epoch 75/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.4798 - categorical_accuracy: 0.8467 - val_loss: 0.5084 - val_categorical_accuracy: 0.8300\n",
      "Epoch 76/100\n",
      "113/113 [==============================] - 52s 457ms/step - loss: 0.5408 - categorical_accuracy: 0.8244 - val_loss: 0.4815 - val_categorical_accuracy: 0.8300\n",
      "Epoch 77/100\n",
      "113/113 [==============================] - 52s 457ms/step - loss: 0.4122 - categorical_accuracy: 0.8633 - val_loss: 0.6058 - val_categorical_accuracy: 0.8200\n",
      "Epoch 78/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.5175 - categorical_accuracy: 0.8189 - val_loss: 0.4488 - val_categorical_accuracy: 0.8500\n",
      "Epoch 79/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.4475 - categorical_accuracy: 0.8511 - val_loss: 0.3743 - val_categorical_accuracy: 0.9000\n",
      "Epoch 80/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.4989 - categorical_accuracy: 0.8378 - val_loss: 0.8412 - val_categorical_accuracy: 0.7000\n",
      "Epoch 81/100\n",
      "113/113 [==============================] - 52s 459ms/step - loss: 0.4617 - categorical_accuracy: 0.8389 - val_loss: 0.3929 - val_categorical_accuracy: 0.8900\n",
      "Epoch 82/100\n",
      "113/113 [==============================] - 52s 460ms/step - loss: 0.3690 - categorical_accuracy: 0.8878 - val_loss: 0.5399 - val_categorical_accuracy: 0.8200\n",
      "Epoch 83/100\n",
      "113/113 [==============================] - 52s 458ms/step - loss: 0.5458 - categorical_accuracy: 0.8256 - val_loss: 0.6096 - val_categorical_accuracy: 0.7900\n",
      "Epoch 84/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.3830 - categorical_accuracy: 0.8844 - val_loss: 0.3760 - val_categorical_accuracy: 0.8900\n",
      "Epoch 85/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.4080 - categorical_accuracy: 0.8711 - val_loss: 0.4618 - val_categorical_accuracy: 0.8400\n",
      "Epoch 86/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.4076 - categorical_accuracy: 0.8622 - val_loss: 0.6547 - val_categorical_accuracy: 0.7900\n",
      "Epoch 87/100\n",
      "113/113 [==============================] - 51s 455ms/step - loss: 0.3966 - categorical_accuracy: 0.8689 - val_loss: 0.6459 - val_categorical_accuracy: 0.7500\n",
      "Epoch 88/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3849 - categorical_accuracy: 0.8789 - val_loss: 0.3922 - val_categorical_accuracy: 0.8800\n",
      "Epoch 89/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.3967 - categorical_accuracy: 0.8722 - val_loss: 0.4036 - val_categorical_accuracy: 0.8500\n",
      "Epoch 90/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.4118 - categorical_accuracy: 0.8667 - val_loss: 0.3600 - val_categorical_accuracy: 0.8700\n",
      "Epoch 91/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.4056 - categorical_accuracy: 0.8733 - val_loss: 0.5976 - val_categorical_accuracy: 0.7800\n",
      "Epoch 92/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.4430 - categorical_accuracy: 0.8622 - val_loss: 0.3574 - val_categorical_accuracy: 0.8900\n",
      "Epoch 93/100\n",
      "113/113 [==============================] - 52s 455ms/step - loss: 0.3914 - categorical_accuracy: 0.8733 - val_loss: 0.4379 - val_categorical_accuracy: 0.8500\n",
      "Epoch 94/100\n",
      "113/113 [==============================] - 52s 461ms/step - loss: 0.3451 - categorical_accuracy: 0.8922 - val_loss: 0.4518 - val_categorical_accuracy: 0.8600\n",
      "Epoch 95/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.3373 - categorical_accuracy: 0.9000 - val_loss: 0.5741 - val_categorical_accuracy: 0.8000\n",
      "Epoch 96/100\n",
      "113/113 [==============================] - 51s 457ms/step - loss: 0.3771 - categorical_accuracy: 0.8856 - val_loss: 0.4136 - val_categorical_accuracy: 0.8600\n",
      "Epoch 97/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.3577 - categorical_accuracy: 0.8922 - val_loss: 0.4052 - val_categorical_accuracy: 0.8700\n",
      "Epoch 98/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.3453 - categorical_accuracy: 0.9000INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense128allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 516ms/step - loss: 0.3453 - categorical_accuracy: 0.9000 - val_loss: 0.2897 - val_categorical_accuracy: 0.9200\n",
      "Epoch 99/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.3037 - categorical_accuracy: 0.9133 - val_loss: 0.4313 - val_categorical_accuracy: 0.8800\n",
      "Epoch 100/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.3411 - categorical_accuracy: 0.8978 - val_loss: 0.4559 - val_categorical_accuracy: 0.8600\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁▁▂▂▃▃▃▃▄▄▅▆▆▇▇▇▇▇████████████▇▇██▇▇▇███</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>█████▇▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁█████▇▇▇▇▆</td></tr><tr><td>epoch/loss</td><td>██▇▇▆▅▅▅▅▅▄▃▃▂▂▂▂▂▁▁▂▁▂▁▁▁▁▁▁▁▂▂▂▂▂▂▂▁▁▁</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁▁▂▂▄▃▃▃▃▄▄▇▇▇▇▇█▇▇██▆███████▇▇█▇█▆▇█▇▇▇</td></tr><tr><td>epoch/val_loss</td><td>██▇▇▅▅▅▅▅▅▄▃▂▂▂▂▁▂▃▂▂▄▂▂▂▁▂▂▂▂▄▂▃▂▄▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.89778</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>8e-05</td></tr><tr><td>epoch/loss</td><td>0.3411</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.86</td></tr><tr><td>epoch/val_loss</td><td>0.45587</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM64-Dense128-allfeatures</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/jhepc329' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/jhepc329</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240408_141317-jhepc329/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fda52b45090>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(64, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, 1629),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"LSTM64-Dense128allfeatures.tf\"),\n",
    "            run_name=\"LSTM64-Dense128-allfeatures\", T_max=75, epochs=100, \n",
    "            max_lr = 1e-4, min_lr = 2.5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240408_154309-dtxcqcb5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/dtxcqcb5' target=\"_blank\">LSTM64-Dense257-allfeatures</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/dtxcqcb5' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/dtxcqcb5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.1294 - categorical_accuracy: 0.4156INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 62s 512ms/step - loss: 1.1294 - categorical_accuracy: 0.4156 - val_loss: 1.1076 - val_categorical_accuracy: 0.4400\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 1.1075 - categorical_accuracy: 0.4222 - val_loss: 1.0894 - val_categorical_accuracy: 0.4300\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 1.0856 - categorical_accuracy: 0.4189 - val_loss: 1.0600 - val_categorical_accuracy: 0.4300\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 1.0583 - categorical_accuracy: 0.4211INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 500ms/step - loss: 1.0583 - categorical_accuracy: 0.4211 - val_loss: 1.0436 - val_categorical_accuracy: 0.4500\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - 53s 466ms/step - loss: 1.0402 - categorical_accuracy: 0.4322 - val_loss: 1.0101 - val_categorical_accuracy: 0.4200\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.9952 - categorical_accuracy: 0.4556INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 59s 520ms/step - loss: 0.9952 - categorical_accuracy: 0.4556 - val_loss: 0.9143 - val_categorical_accuracy: 0.5800\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.9315 - categorical_accuracy: 0.5089INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 518ms/step - loss: 0.9315 - categorical_accuracy: 0.5089 - val_loss: 0.8479 - val_categorical_accuracy: 0.5900\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.9245 - categorical_accuracy: 0.5300 - val_loss: 0.8585 - val_categorical_accuracy: 0.5500\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.8849 - categorical_accuracy: 0.5322 - val_loss: 0.8620 - val_categorical_accuracy: 0.5700\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.9289 - categorical_accuracy: 0.5156 - val_loss: 0.8300 - val_categorical_accuracy: 0.5600\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.8394 - categorical_accuracy: 0.5600 - val_loss: 0.8156 - val_categorical_accuracy: 0.5700\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.8493 - categorical_accuracy: 0.5567 - val_loss: 0.8283 - val_categorical_accuracy: 0.5800\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - 51s 446ms/step - loss: 0.8212 - categorical_accuracy: 0.5533 - val_loss: 0.8123 - val_categorical_accuracy: 0.5100\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.8168 - categorical_accuracy: 0.5822 - val_loss: 0.8011 - val_categorical_accuracy: 0.5700\n",
      "Epoch 15/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.8150 - categorical_accuracy: 0.5711 - val_loss: 0.7711 - val_categorical_accuracy: 0.5600\n",
      "Epoch 16/100\n",
      "113/113 [==============================] - 51s 447ms/step - loss: 0.7867 - categorical_accuracy: 0.5933 - val_loss: 0.7957 - val_categorical_accuracy: 0.5300\n",
      "Epoch 17/100\n",
      "113/113 [==============================] - 51s 447ms/step - loss: 0.7756 - categorical_accuracy: 0.5911 - val_loss: 0.7544 - val_categorical_accuracy: 0.5900\n",
      "Epoch 18/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7765 - categorical_accuracy: 0.5956INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 508ms/step - loss: 0.7765 - categorical_accuracy: 0.5956 - val_loss: 0.7695 - val_categorical_accuracy: 0.6000\n",
      "Epoch 19/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.7625 - categorical_accuracy: 0.6044 - val_loss: 0.8354 - val_categorical_accuracy: 0.5600\n",
      "Epoch 20/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.7641 - categorical_accuracy: 0.5956 - val_loss: 0.7856 - val_categorical_accuracy: 0.5800\n",
      "Epoch 21/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.7537 - categorical_accuracy: 0.6067 - val_loss: 0.8254 - val_categorical_accuracy: 0.5800\n",
      "Epoch 22/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.7807 - categorical_accuracy: 0.6044 - val_loss: 0.7774 - val_categorical_accuracy: 0.5600\n",
      "Epoch 23/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.7218 - categorical_accuracy: 0.6544INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 515ms/step - loss: 0.7218 - categorical_accuracy: 0.6544 - val_loss: 0.7005 - val_categorical_accuracy: 0.6600\n",
      "Epoch 24/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6934 - categorical_accuracy: 0.6756INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 512ms/step - loss: 0.6934 - categorical_accuracy: 0.6756 - val_loss: 0.7214 - val_categorical_accuracy: 0.6900\n",
      "Epoch 25/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6997 - categorical_accuracy: 0.7067INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 515ms/step - loss: 0.6997 - categorical_accuracy: 0.7067 - val_loss: 0.6414 - val_categorical_accuracy: 0.7300\n",
      "Epoch 26/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.6192 - categorical_accuracy: 0.7644INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 508ms/step - loss: 0.6192 - categorical_accuracy: 0.7644 - val_loss: 0.6036 - val_categorical_accuracy: 0.8000\n",
      "Epoch 27/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.5595 - categorical_accuracy: 0.7889 - val_loss: 0.6018 - val_categorical_accuracy: 0.7900\n",
      "Epoch 28/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.5250 - categorical_accuracy: 0.8167 - val_loss: 0.6052 - val_categorical_accuracy: 0.7800\n",
      "Epoch 29/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.5321 - categorical_accuracy: 0.7844 - val_loss: 0.5875 - val_categorical_accuracy: 0.7800\n",
      "Epoch 30/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5018 - categorical_accuracy: 0.8189INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 58s 511ms/step - loss: 0.5018 - categorical_accuracy: 0.8189 - val_loss: 0.4842 - val_categorical_accuracy: 0.8100\n",
      "Epoch 31/100\n",
      "113/113 [==============================] - 50s 445ms/step - loss: 0.5003 - categorical_accuracy: 0.8244 - val_loss: 0.7470 - val_categorical_accuracy: 0.7200\n",
      "Epoch 32/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.5084 - categorical_accuracy: 0.8100INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 56s 494ms/step - loss: 0.5084 - categorical_accuracy: 0.8100 - val_loss: 0.4414 - val_categorical_accuracy: 0.8500\n",
      "Epoch 33/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.4533 - categorical_accuracy: 0.8344 - val_loss: 0.4862 - val_categorical_accuracy: 0.8400\n",
      "Epoch 34/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.4792 - categorical_accuracy: 0.8311 - val_loss: 0.5741 - val_categorical_accuracy: 0.7900\n",
      "Epoch 35/100\n",
      "113/113 [==============================] - 50s 441ms/step - loss: 0.4670 - categorical_accuracy: 0.8356 - val_loss: 0.5930 - val_categorical_accuracy: 0.7900\n",
      "Epoch 36/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.4843 - categorical_accuracy: 0.8300 - val_loss: 0.5214 - val_categorical_accuracy: 0.8300\n",
      "Epoch 37/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4420 - categorical_accuracy: 0.8489INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 56s 494ms/step - loss: 0.4420 - categorical_accuracy: 0.8489 - val_loss: 0.4315 - val_categorical_accuracy: 0.8700\n",
      "Epoch 38/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.4192 - categorical_accuracy: 0.8567 - val_loss: 0.4976 - val_categorical_accuracy: 0.8400\n",
      "Epoch 39/100\n",
      "113/113 [==============================] - 51s 456ms/step - loss: 0.4574 - categorical_accuracy: 0.8400 - val_loss: 0.4369 - val_categorical_accuracy: 0.8600\n",
      "Epoch 40/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.4144 - categorical_accuracy: 0.8656 - val_loss: 0.5767 - val_categorical_accuracy: 0.7900\n",
      "Epoch 41/100\n",
      "113/113 [==============================] - 51s 456ms/step - loss: 0.4299 - categorical_accuracy: 0.8578 - val_loss: 0.5178 - val_categorical_accuracy: 0.8000\n",
      "Epoch 42/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.4259 - categorical_accuracy: 0.8633 - val_loss: 0.6478 - val_categorical_accuracy: 0.7800\n",
      "Epoch 43/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.4309 - categorical_accuracy: 0.8544 - val_loss: 0.5246 - val_categorical_accuracy: 0.8400\n",
      "Epoch 44/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.4161 - categorical_accuracy: 0.8633 - val_loss: 0.5034 - val_categorical_accuracy: 0.8600\n",
      "Epoch 45/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.3872 - categorical_accuracy: 0.8789INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 507ms/step - loss: 0.3872 - categorical_accuracy: 0.8789 - val_loss: 0.4209 - val_categorical_accuracy: 0.8900\n",
      "Epoch 46/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3671 - categorical_accuracy: 0.8922 - val_loss: 0.5633 - val_categorical_accuracy: 0.7900\n",
      "Epoch 47/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.3706 - categorical_accuracy: 0.8822 - val_loss: 0.6383 - val_categorical_accuracy: 0.7500\n",
      "Epoch 48/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.3800 - categorical_accuracy: 0.8856 - val_loss: 0.4459 - val_categorical_accuracy: 0.8700\n",
      "Epoch 49/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.3679 - categorical_accuracy: 0.8911 - val_loss: 0.5862 - val_categorical_accuracy: 0.8500\n",
      "Epoch 50/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.3488 - categorical_accuracy: 0.8989 - val_loss: 0.4686 - val_categorical_accuracy: 0.8700\n",
      "Epoch 51/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.3243 - categorical_accuracy: 0.9089 - val_loss: 0.4609 - val_categorical_accuracy: 0.8500\n",
      "Epoch 52/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3512 - categorical_accuracy: 0.8944 - val_loss: 0.5035 - val_categorical_accuracy: 0.8200\n",
      "Epoch 53/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.3587 - categorical_accuracy: 0.8933 - val_loss: 0.4697 - val_categorical_accuracy: 0.8700\n",
      "Epoch 54/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.3023 - categorical_accuracy: 0.9178 - val_loss: 0.6036 - val_categorical_accuracy: 0.8400\n",
      "Epoch 55/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.3401 - categorical_accuracy: 0.9056 - val_loss: 0.4467 - val_categorical_accuracy: 0.8700\n",
      "Epoch 56/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.3360 - categorical_accuracy: 0.9000 - val_loss: 0.5005 - val_categorical_accuracy: 0.8200\n",
      "Epoch 57/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.3344 - categorical_accuracy: 0.8989 - val_loss: 0.4966 - val_categorical_accuracy: 0.8300\n",
      "Epoch 58/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3392 - categorical_accuracy: 0.9033 - val_loss: 0.4649 - val_categorical_accuracy: 0.8600\n",
      "Epoch 59/100\n",
      "113/113 [==============================] - 51s 451ms/step - loss: 0.3297 - categorical_accuracy: 0.9033 - val_loss: 0.5340 - val_categorical_accuracy: 0.8500\n",
      "Epoch 60/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.3123 - categorical_accuracy: 0.9078 - val_loss: 0.4660 - val_categorical_accuracy: 0.8500\n",
      "Epoch 61/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.2969 - categorical_accuracy: 0.9167 - val_loss: 0.4637 - val_categorical_accuracy: 0.8800\n",
      "Epoch 62/100\n",
      "113/113 [==============================] - 50s 447ms/step - loss: 0.3339 - categorical_accuracy: 0.9022 - val_loss: 0.5756 - val_categorical_accuracy: 0.8000\n",
      "Epoch 63/100\n",
      "113/113 [==============================] - 50s 448ms/step - loss: 0.2775 - categorical_accuracy: 0.9267 - val_loss: 0.4496 - val_categorical_accuracy: 0.8500\n",
      "Epoch 64/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.2522 - categorical_accuracy: 0.9322 - val_loss: 0.4852 - val_categorical_accuracy: 0.8500\n",
      "Epoch 65/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.2856 - categorical_accuracy: 0.9233 - val_loss: 0.4737 - val_categorical_accuracy: 0.8700\n",
      "Epoch 66/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.3039 - categorical_accuracy: 0.9111 - val_loss: 0.6630 - val_categorical_accuracy: 0.8300\n",
      "Epoch 67/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.2949 - categorical_accuracy: 0.9167 - val_loss: 0.4312 - val_categorical_accuracy: 0.8700\n",
      "Epoch 68/100\n",
      "113/113 [==============================] - 51s 447ms/step - loss: 0.3138 - categorical_accuracy: 0.9156 - val_loss: 0.5354 - val_categorical_accuracy: 0.8400\n",
      "Epoch 69/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.2769 - categorical_accuracy: 0.9189 - val_loss: 0.5554 - val_categorical_accuracy: 0.8400\n",
      "Epoch 70/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.2758 - categorical_accuracy: 0.9256 - val_loss: 0.4695 - val_categorical_accuracy: 0.8600\n",
      "Epoch 71/100\n",
      "113/113 [==============================] - 50s 448ms/step - loss: 0.2844 - categorical_accuracy: 0.9222 - val_loss: 0.4849 - val_categorical_accuracy: 0.8600\n",
      "Epoch 72/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.2574 - categorical_accuracy: 0.9322 - val_loss: 0.4980 - val_categorical_accuracy: 0.8400\n",
      "Epoch 73/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.2820 - categorical_accuracy: 0.9200 - val_loss: 0.4960 - val_categorical_accuracy: 0.8400\n",
      "Epoch 74/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.2596 - categorical_accuracy: 0.9278 - val_loss: 0.4992 - val_categorical_accuracy: 0.8500\n",
      "Epoch 75/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.5701 - categorical_accuracy: 0.8200 - val_loss: 0.4703 - val_categorical_accuracy: 0.8100\n",
      "Epoch 76/100\n",
      "113/113 [==============================] - ETA: 0s - loss: 0.4652 - categorical_accuracy: 0.8444INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM64-Dense256allfeatures.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 57s 507ms/step - loss: 0.4652 - categorical_accuracy: 0.8444 - val_loss: 0.3677 - val_categorical_accuracy: 0.9100\n",
      "Epoch 77/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.4806 - categorical_accuracy: 0.8356 - val_loss: 0.4930 - val_categorical_accuracy: 0.8700\n",
      "Epoch 78/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.3906 - categorical_accuracy: 0.8889 - val_loss: 0.4066 - val_categorical_accuracy: 0.8600\n",
      "Epoch 79/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.4055 - categorical_accuracy: 0.8744 - val_loss: 0.8157 - val_categorical_accuracy: 0.7000\n",
      "Epoch 80/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.4152 - categorical_accuracy: 0.8711 - val_loss: 0.4717 - val_categorical_accuracy: 0.8200\n",
      "Epoch 81/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.3796 - categorical_accuracy: 0.8944 - val_loss: 0.5826 - val_categorical_accuracy: 0.8300\n",
      "Epoch 82/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.4741 - categorical_accuracy: 0.8289 - val_loss: 0.4194 - val_categorical_accuracy: 0.8500\n",
      "Epoch 83/100\n",
      "113/113 [==============================] - 51s 452ms/step - loss: 0.4464 - categorical_accuracy: 0.8422 - val_loss: 0.5595 - val_categorical_accuracy: 0.8300\n",
      "Epoch 84/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.4245 - categorical_accuracy: 0.8600 - val_loss: 0.4758 - val_categorical_accuracy: 0.8200\n",
      "Epoch 85/100\n",
      "113/113 [==============================] - 51s 447ms/step - loss: 0.4011 - categorical_accuracy: 0.8667 - val_loss: 0.5513 - val_categorical_accuracy: 0.8500\n",
      "Epoch 86/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3663 - categorical_accuracy: 0.8900 - val_loss: 0.4687 - val_categorical_accuracy: 0.8500\n",
      "Epoch 87/100\n",
      "113/113 [==============================] - 51s 446ms/step - loss: 0.4279 - categorical_accuracy: 0.8689 - val_loss: 0.6647 - val_categorical_accuracy: 0.8100\n",
      "Epoch 88/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.3741 - categorical_accuracy: 0.8833 - val_loss: 0.5761 - val_categorical_accuracy: 0.8500\n",
      "Epoch 89/100\n",
      "113/113 [==============================] - 51s 446ms/step - loss: 0.3585 - categorical_accuracy: 0.8900 - val_loss: 0.9366 - val_categorical_accuracy: 0.6600\n",
      "Epoch 90/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3772 - categorical_accuracy: 0.8844 - val_loss: 0.4098 - val_categorical_accuracy: 0.8700\n",
      "Epoch 91/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.3593 - categorical_accuracy: 0.8933 - val_loss: 0.5831 - val_categorical_accuracy: 0.7700\n",
      "Epoch 92/100\n",
      "113/113 [==============================] - 51s 454ms/step - loss: 0.4430 - categorical_accuracy: 0.8567 - val_loss: 0.5215 - val_categorical_accuracy: 0.7900\n",
      "Epoch 93/100\n",
      "113/113 [==============================] - 51s 448ms/step - loss: 0.4518 - categorical_accuracy: 0.8578 - val_loss: 0.4675 - val_categorical_accuracy: 0.8400\n",
      "Epoch 94/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3978 - categorical_accuracy: 0.8744 - val_loss: 0.5808 - val_categorical_accuracy: 0.8300\n",
      "Epoch 95/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.4023 - categorical_accuracy: 0.8756 - val_loss: 0.6677 - val_categorical_accuracy: 0.7900\n",
      "Epoch 96/100\n",
      "113/113 [==============================] - 51s 447ms/step - loss: 0.3559 - categorical_accuracy: 0.8789 - val_loss: 0.3913 - val_categorical_accuracy: 0.8900\n",
      "Epoch 97/100\n",
      "113/113 [==============================] - 51s 450ms/step - loss: 0.3396 - categorical_accuracy: 0.8956 - val_loss: 0.4530 - val_categorical_accuracy: 0.8700\n",
      "Epoch 98/100\n",
      "113/113 [==============================] - 51s 449ms/step - loss: 0.4212 - categorical_accuracy: 0.8733 - val_loss: 0.4849 - val_categorical_accuracy: 0.8300\n",
      "Epoch 99/100\n",
      "113/113 [==============================] - 51s 453ms/step - loss: 0.3324 - categorical_accuracy: 0.8944 - val_loss: 0.5087 - val_categorical_accuracy: 0.8200\n",
      "Epoch 100/100\n",
      "113/113 [==============================] - 50s 444ms/step - loss: 0.3433 - categorical_accuracy: 0.8956 - val_loss: 0.3723 - val_categorical_accuracy: 0.8800\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁▁▂▃▃▃▃▃▄▄▆▆▇▇▇▇▇▇▇▇██████████▇▇▇▇▇▇▇▇██</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>█████▇▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁█████▇▇▇▇▆</td></tr><tr><td>epoch/loss</td><td>██▇▆▆▆▅▅▅▅▄▃▃▃▃▃▂▂▂▂▂▁▂▂▁▁▁▁▁▁▃▂▃▂▂▂▃▂▂▂</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁▁▃▃▃▂▃▄▃▅▇▆▆▇▇█▇█▇██▇▇████▇▇██▅█▇▇▅▇▇██</td></tr><tr><td>epoch/val_loss</td><td>██▆▆▅▅▅▅▅▄▃▃▅▃▂▂▂▂▃▃▂▃▂▃▂▂▂▃▂▂▂▅▁▂▄▆▂▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.89556</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>8e-05</td></tr><tr><td>epoch/loss</td><td>0.34329</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.88</td></tr><tr><td>epoch/val_loss</td><td>0.3723</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM64-Dense257-allfeatures</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/dtxcqcb5' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/dtxcqcb5</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240408_154309-dtxcqcb5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fda49b2fa50>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#x,y,z -> y,z as the input shape\n",
    "model.add(LSTM(64, return_sequences=False, input_shape=(CONFIG.VIDEO_LENGTH, 1629),\n",
    "               kernel_regularizer=l2(0.0001), \n",
    "               activity_regularizer=l2(0.0001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(train_dataset_parquet.unique_labels), activation='softmax'))\n",
    "\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"LSTM64-Dense256allfeatures.tf\"),\n",
    "            run_name=\"LSTM64-Dense257-allfeatures\", T_max=75, epochs=100, \n",
    "            max_lr = 1e-4, min_lr = 2.5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_shape = (25, 1629), Y_shape = (13,)\n"
     ]
    }
   ],
   "source": [
    "X_shape = train_dataset_parquet[0][0].shape\n",
    "Y_shape = train_dataset_parquet[0][1].shape\n",
    "print(f\"X_shape = {X_shape}, Y_shape = {Y_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        return x + positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:8g8ds4yd) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM256-Dense128-Dense256-LipsEyesHandsPose_5k</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/8g8ds4yd' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/8g8ds4yd</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240416_144023-8g8ds4yd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:8g8ds4yd). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0bda949114c426181685c50087a2ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011114080829752817, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240416_144046-o25m0mz7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/o25m0mz7' target=\"_blank\">Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/o25m0mz7' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/o25m0mz7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "    497/Unknown - 9s 10ms/step - loss: 2.5642 - categorical_accuracy: 0.1066"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 14:41:03.907399: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8104596135730168633\n",
      "2024-04-16 14:41:03.907462: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7063810106286413230\n",
      "2024-04-16 14:41:04.886187: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 3094190343660208166\n",
      "2024-04-16 14:41:04.886254: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 17406417933654264596\n",
      "2024-04-16 14:41:04.886267: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 6462147652021642174\n",
      "2024-04-16 14:41:04.886293: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4153888742695321310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 15s 22ms/step - loss: 2.5637 - categorical_accuracy: 0.1063 - val_loss: 2.4410 - val_categorical_accuracy: 0.1770\n",
      "Epoch 2/100\n",
      "499/500 [============================>.] - ETA: 0s - loss: 2.4526 - categorical_accuracy: 0.1586INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 2.4525 - categorical_accuracy: 0.1583 - val_loss: 2.3826 - val_categorical_accuracy: 0.2170\n",
      "Epoch 3/100\n",
      "499/500 [============================>.] - ETA: 0s - loss: 2.4056 - categorical_accuracy: 0.1829INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 2.4053 - categorical_accuracy: 0.1832 - val_loss: 2.3468 - val_categorical_accuracy: 0.2340\n",
      "Epoch 4/100\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.3575 - categorical_accuracy: 0.2017INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 2.3568 - categorical_accuracy: 0.2027 - val_loss: 2.3265 - val_categorical_accuracy: 0.2370\n",
      "Epoch 5/100\n",
      "498/500 [============================>.] - ETA: 0s - loss: 2.3225 - categorical_accuracy: 0.2161INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.3223 - categorical_accuracy: 0.2160 - val_loss: 2.2558 - val_categorical_accuracy: 0.2840\n",
      "Epoch 6/100\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.2850 - categorical_accuracy: 0.2404INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 2.2841 - categorical_accuracy: 0.2415 - val_loss: 2.2454 - val_categorical_accuracy: 0.2850\n",
      "Epoch 7/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.2539 - categorical_accuracy: 0.2630INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 2.2539 - categorical_accuracy: 0.2630 - val_loss: 2.2394 - val_categorical_accuracy: 0.3010\n",
      "Epoch 8/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.2169 - categorical_accuracy: 0.2930INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 2.2169 - categorical_accuracy: 0.2930 - val_loss: 2.2031 - val_categorical_accuracy: 0.3120\n",
      "Epoch 9/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.1895 - categorical_accuracy: 0.3065INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 2.1895 - categorical_accuracy: 0.3065 - val_loss: 2.1413 - val_categorical_accuracy: 0.3480\n",
      "Epoch 10/100\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.1722 - categorical_accuracy: 0.3239INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 2.1706 - categorical_accuracy: 0.3250 - val_loss: 2.1352 - val_categorical_accuracy: 0.3570\n",
      "Epoch 11/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.1411 - categorical_accuracy: 0.3350INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 2.1411 - categorical_accuracy: 0.3350 - val_loss: 2.1161 - val_categorical_accuracy: 0.3780\n",
      "Epoch 12/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 2.1237 - categorical_accuracy: 0.3530 - val_loss: 2.1283 - val_categorical_accuracy: 0.3730\n",
      "Epoch 13/100\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.1007 - categorical_accuracy: 0.3649INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.0994 - categorical_accuracy: 0.3660 - val_loss: 2.0815 - val_categorical_accuracy: 0.4020\n",
      "Epoch 14/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 2.0837 - categorical_accuracy: 0.3747 - val_loss: 2.1352 - val_categorical_accuracy: 0.3750\n",
      "Epoch 15/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.0698 - categorical_accuracy: 0.3758INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 2.0698 - categorical_accuracy: 0.3758 - val_loss: 2.0697 - val_categorical_accuracy: 0.4090\n",
      "Epoch 16/100\n",
      "498/500 [============================>.] - ETA: 0s - loss: 2.0529 - categorical_accuracy: 0.3981INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 17ms/step - loss: 2.0521 - categorical_accuracy: 0.3983 - val_loss: 2.0441 - val_categorical_accuracy: 0.4320\n",
      "Epoch 17/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 2.0318 - categorical_accuracy: 0.4100 - val_loss: 2.0290 - val_categorical_accuracy: 0.4290\n",
      "Epoch 18/100\n",
      "499/500 [============================>.] - ETA: 0s - loss: 2.0151 - categorical_accuracy: 0.4181INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 2.0145 - categorical_accuracy: 0.4187 - val_loss: 2.0040 - val_categorical_accuracy: 0.4380\n",
      "Epoch 19/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.9912 - categorical_accuracy: 0.4363 - val_loss: 2.0059 - val_categorical_accuracy: 0.4360\n",
      "Epoch 20/100\n",
      "499/500 [============================>.] - ETA: 0s - loss: 1.9707 - categorical_accuracy: 0.4439INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 1.9703 - categorical_accuracy: 0.4440 - val_loss: 1.9635 - val_categorical_accuracy: 0.4650\n",
      "Epoch 21/100\n",
      "497/500 [============================>.] - ETA: 0s - loss: 1.9628 - categorical_accuracy: 0.4497INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 1.9622 - categorical_accuracy: 0.4502 - val_loss: 1.9552 - val_categorical_accuracy: 0.4660\n",
      "Epoch 22/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9492 - categorical_accuracy: 0.4610INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 1.9492 - categorical_accuracy: 0.4610 - val_loss: 1.9396 - val_categorical_accuracy: 0.4700\n",
      "Epoch 23/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.9306 - categorical_accuracy: 0.4767INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 1.9306 - categorical_accuracy: 0.4767 - val_loss: 1.9275 - val_categorical_accuracy: 0.4880\n",
      "Epoch 24/100\n",
      "499/500 [============================>.] - ETA: 0s - loss: 1.9128 - categorical_accuracy: 0.4907INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 1.9124 - categorical_accuracy: 0.4908 - val_loss: 1.8978 - val_categorical_accuracy: 0.4980\n",
      "Epoch 25/100\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.9010 - categorical_accuracy: 0.4937INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 1.9001 - categorical_accuracy: 0.4942 - val_loss: 1.8863 - val_categorical_accuracy: 0.5130\n",
      "Epoch 26/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.8846 - categorical_accuracy: 0.5008 - val_loss: 1.8806 - val_categorical_accuracy: 0.5130\n",
      "Epoch 27/100\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.8740 - categorical_accuracy: 0.5113INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 1.8739 - categorical_accuracy: 0.5113 - val_loss: 1.8562 - val_categorical_accuracy: 0.5320\n",
      "Epoch 28/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.8537 - categorical_accuracy: 0.5295 - val_loss: 1.8730 - val_categorical_accuracy: 0.5160\n",
      "Epoch 29/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.8375 - categorical_accuracy: 0.5315 - val_loss: 1.8677 - val_categorical_accuracy: 0.5260\n",
      "Epoch 30/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.8323 - categorical_accuracy: 0.5347 - val_loss: 1.8621 - val_categorical_accuracy: 0.5290\n",
      "Epoch 31/100\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.8232 - categorical_accuracy: 0.5391INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 1.8222 - categorical_accuracy: 0.5400 - val_loss: 1.8554 - val_categorical_accuracy: 0.5370\n",
      "Epoch 32/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.8130 - categorical_accuracy: 0.5477INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 1.8130 - categorical_accuracy: 0.5477 - val_loss: 1.8451 - val_categorical_accuracy: 0.5410\n",
      "Epoch 33/100\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.8044 - categorical_accuracy: 0.5522INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 19ms/step - loss: 1.8042 - categorical_accuracy: 0.5520 - val_loss: 1.8561 - val_categorical_accuracy: 0.5420\n",
      "Epoch 34/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.7991 - categorical_accuracy: 0.5592INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 1.7991 - categorical_accuracy: 0.5592 - val_loss: 1.8426 - val_categorical_accuracy: 0.5500\n",
      "Epoch 35/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.7870 - categorical_accuracy: 0.5608INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 1.7870 - categorical_accuracy: 0.5608 - val_loss: 1.8362 - val_categorical_accuracy: 0.5560\n",
      "Epoch 36/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.7817 - categorical_accuracy: 0.5617INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 1.7817 - categorical_accuracy: 0.5617 - val_loss: 1.8141 - val_categorical_accuracy: 0.5670\n",
      "Epoch 37/100\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.7763 - categorical_accuracy: 0.5751INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 1.7753 - categorical_accuracy: 0.5755 - val_loss: 1.8230 - val_categorical_accuracy: 0.5700\n",
      "Epoch 38/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7616 - categorical_accuracy: 0.5803 - val_loss: 1.8388 - val_categorical_accuracy: 0.5530\n",
      "Epoch 39/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.7548 - categorical_accuracy: 0.5788INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 1.7548 - categorical_accuracy: 0.5788 - val_loss: 1.8051 - val_categorical_accuracy: 0.5730\n",
      "Epoch 40/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7525 - categorical_accuracy: 0.5828 - val_loss: 1.8235 - val_categorical_accuracy: 0.5640\n",
      "Epoch 41/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.7422 - categorical_accuracy: 0.5893 - val_loss: 1.8186 - val_categorical_accuracy: 0.5590\n",
      "Epoch 42/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.7401 - categorical_accuracy: 0.5903 - val_loss: 1.8079 - val_categorical_accuracy: 0.5710\n",
      "Epoch 43/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7330 - categorical_accuracy: 0.5950 - val_loss: 1.8170 - val_categorical_accuracy: 0.5600\n",
      "Epoch 44/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.7240 - categorical_accuracy: 0.5960 - val_loss: 1.8233 - val_categorical_accuracy: 0.5590\n",
      "Epoch 45/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7182 - categorical_accuracy: 0.6047 - val_loss: 1.8407 - val_categorical_accuracy: 0.5610\n",
      "Epoch 46/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7176 - categorical_accuracy: 0.6000 - val_loss: 1.8124 - val_categorical_accuracy: 0.5720\n",
      "Epoch 47/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7088 - categorical_accuracy: 0.6033 - val_loss: 1.8191 - val_categorical_accuracy: 0.5730\n",
      "Epoch 48/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.7030 - categorical_accuracy: 0.6200INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 1.7030 - categorical_accuracy: 0.6200 - val_loss: 1.8060 - val_categorical_accuracy: 0.5780\n",
      "Epoch 49/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7019 - categorical_accuracy: 0.6115 - val_loss: 1.8178 - val_categorical_accuracy: 0.5630\n",
      "Epoch 50/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.7680 - categorical_accuracy: 0.5765 - val_loss: 1.8046 - val_categorical_accuracy: 0.5600\n",
      "Epoch 51/100\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.7563 - categorical_accuracy: 0.5748INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 1.7557 - categorical_accuracy: 0.5753 - val_loss: 1.7978 - val_categorical_accuracy: 0.5820\n",
      "Epoch 52/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.7510 - categorical_accuracy: 0.5810 - val_loss: 1.8712 - val_categorical_accuracy: 0.5540\n",
      "Epoch 53/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7417 - categorical_accuracy: 0.5853 - val_loss: 1.8068 - val_categorical_accuracy: 0.5570\n",
      "Epoch 54/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7295 - categorical_accuracy: 0.5880 - val_loss: 1.8239 - val_categorical_accuracy: 0.5620\n",
      "Epoch 55/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7309 - categorical_accuracy: 0.5910 - val_loss: 1.8488 - val_categorical_accuracy: 0.5520\n",
      "Epoch 56/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7183 - categorical_accuracy: 0.6053 - val_loss: 1.8426 - val_categorical_accuracy: 0.5610\n",
      "Epoch 57/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.7104 - categorical_accuracy: 0.5990 - val_loss: 1.8623 - val_categorical_accuracy: 0.5350\n",
      "Epoch 58/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7138 - categorical_accuracy: 0.5930 - val_loss: 1.8272 - val_categorical_accuracy: 0.5620\n",
      "Epoch 59/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7024 - categorical_accuracy: 0.6083 - val_loss: 1.8635 - val_categorical_accuracy: 0.5460\n",
      "Epoch 60/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6863 - categorical_accuracy: 0.6080 - val_loss: 1.8260 - val_categorical_accuracy: 0.5730\n",
      "Epoch 61/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6903 - categorical_accuracy: 0.6183 - val_loss: 1.8453 - val_categorical_accuracy: 0.5680\n",
      "Epoch 62/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6764 - categorical_accuracy: 0.6292 - val_loss: 1.8362 - val_categorical_accuracy: 0.5680\n",
      "Epoch 63/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.6755 - categorical_accuracy: 0.6212 - val_loss: 1.8167 - val_categorical_accuracy: 0.5770\n",
      "Epoch 64/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.6608 - categorical_accuracy: 0.6365 - val_loss: 1.8232 - val_categorical_accuracy: 0.5630\n",
      "Epoch 65/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6648 - categorical_accuracy: 0.6283 - val_loss: 1.8192 - val_categorical_accuracy: 0.5680\n",
      "Epoch 66/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6520 - categorical_accuracy: 0.6435 - val_loss: 1.8179 - val_categorical_accuracy: 0.5650\n",
      "Epoch 67/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6513 - categorical_accuracy: 0.6385 - val_loss: 1.8330 - val_categorical_accuracy: 0.5770\n",
      "Epoch 68/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6366 - categorical_accuracy: 0.6485 - val_loss: 1.8068 - val_categorical_accuracy: 0.5710\n",
      "Epoch 69/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.6300 - categorical_accuracy: 0.6560 - val_loss: 1.8123 - val_categorical_accuracy: 0.5730\n",
      "Epoch 70/100\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.6224 - categorical_accuracy: 0.6637INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 1.6214 - categorical_accuracy: 0.6645 - val_loss: 1.8264 - val_categorical_accuracy: 0.5830\n",
      "Epoch 71/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6211 - categorical_accuracy: 0.6637 - val_loss: 1.7917 - val_categorical_accuracy: 0.5830\n",
      "Epoch 72/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6197 - categorical_accuracy: 0.6585 - val_loss: 1.8046 - val_categorical_accuracy: 0.5830\n",
      "Epoch 73/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6104 - categorical_accuracy: 0.6658 - val_loss: 1.8124 - val_categorical_accuracy: 0.5670\n",
      "Epoch 74/100\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.6037 - categorical_accuracy: 0.6687INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 1.6037 - categorical_accuracy: 0.6687 - val_loss: 1.7971 - val_categorical_accuracy: 0.5880\n",
      "Epoch 75/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.6046 - categorical_accuracy: 0.6710 - val_loss: 1.8234 - val_categorical_accuracy: 0.5830\n",
      "Epoch 76/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5908 - categorical_accuracy: 0.6810 - val_loss: 1.8274 - val_categorical_accuracy: 0.5650\n",
      "Epoch 77/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5845 - categorical_accuracy: 0.6765 - val_loss: 1.8203 - val_categorical_accuracy: 0.5800\n",
      "Epoch 78/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5857 - categorical_accuracy: 0.6835 - val_loss: 1.8220 - val_categorical_accuracy: 0.5800\n",
      "Epoch 79/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5817 - categorical_accuracy: 0.6817 - val_loss: 1.7942 - val_categorical_accuracy: 0.5860\n",
      "Epoch 80/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5765 - categorical_accuracy: 0.6905 - val_loss: 1.8197 - val_categorical_accuracy: 0.5790\n",
      "Epoch 81/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5637 - categorical_accuracy: 0.7048 - val_loss: 1.8201 - val_categorical_accuracy: 0.5770\n",
      "Epoch 82/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5620 - categorical_accuracy: 0.7038 - val_loss: 1.8203 - val_categorical_accuracy: 0.5860\n",
      "Epoch 83/100\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.5583 - categorical_accuracy: 0.7051INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 1.5573 - categorical_accuracy: 0.7057 - val_loss: 1.8164 - val_categorical_accuracy: 0.5930\n",
      "Epoch 84/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.5525 - categorical_accuracy: 0.7072 - val_loss: 1.8036 - val_categorical_accuracy: 0.5900\n",
      "Epoch 85/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5526 - categorical_accuracy: 0.7023 - val_loss: 1.8339 - val_categorical_accuracy: 0.5760\n",
      "Epoch 86/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5386 - categorical_accuracy: 0.7103 - val_loss: 1.8749 - val_categorical_accuracy: 0.5560\n",
      "Epoch 87/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5388 - categorical_accuracy: 0.7105 - val_loss: 1.8253 - val_categorical_accuracy: 0.5810\n",
      "Epoch 88/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5337 - categorical_accuracy: 0.7160 - val_loss: 1.8721 - val_categorical_accuracy: 0.5690\n",
      "Epoch 89/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5359 - categorical_accuracy: 0.7188 - val_loss: 1.8741 - val_categorical_accuracy: 0.5600\n",
      "Epoch 90/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5259 - categorical_accuracy: 0.7222 - val_loss: 1.8525 - val_categorical_accuracy: 0.5730\n",
      "Epoch 91/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5276 - categorical_accuracy: 0.7222 - val_loss: 1.8336 - val_categorical_accuracy: 0.5780\n",
      "Epoch 92/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.5213 - categorical_accuracy: 0.7232 - val_loss: 1.8477 - val_categorical_accuracy: 0.5700\n",
      "Epoch 93/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5263 - categorical_accuracy: 0.7220 - val_loss: 1.8536 - val_categorical_accuracy: 0.5790\n",
      "Epoch 94/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.5179 - categorical_accuracy: 0.7300 - val_loss: 1.8477 - val_categorical_accuracy: 0.5830\n",
      "Epoch 95/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5075 - categorical_accuracy: 0.7337 - val_loss: 1.8519 - val_categorical_accuracy: 0.5630\n",
      "Epoch 96/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5105 - categorical_accuracy: 0.7335 - val_loss: 1.8563 - val_categorical_accuracy: 0.5700\n",
      "Epoch 97/100\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.5074 - categorical_accuracy: 0.7452 - val_loss: 1.8447 - val_categorical_accuracy: 0.5710\n",
      "Epoch 98/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5020 - categorical_accuracy: 0.7380 - val_loss: 1.8290 - val_categorical_accuracy: 0.5840\n",
      "Epoch 99/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5026 - categorical_accuracy: 0.7418 - val_loss: 1.8382 - val_categorical_accuracy: 0.5810\n",
      "Epoch 100/100\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5383 - categorical_accuracy: 0.7180 - val_loss: 1.8332 - val_categorical_accuracy: 0.5780\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁▂▂▃▄▄▄▄▅▅▅▆▆▆▆▆▆▆▆▇▆▆▆▆▇▇▇▇▇▇▇▇████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁███▇▇▇▆▆▅▅▄▄▃▃▂▂▁▁▁█</td></tr><tr><td>epoch/loss</td><td>█▇▆▆▅▅▅▄▄▄▃▃▃▃▃▃▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁▂▃▃▄▅▅▅▆▆▇▇▇▇██▇▇█████▇███████████▇████</td></tr><tr><td>epoch/val_loss</td><td>█▇▆▅▄▄▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▂▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.718</td></tr><tr><td>epoch/epoch</td><td>99</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>1.53828</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.578</td></tr><tr><td>epoch/val_loss</td><td>1.83316</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/o25m0mz7' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/o25m0mz7</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240416_144046-o25m0mz7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f9500ab0710>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dim = 64  # Embedding size for each token\n",
    "num_heads = 4  # Number of attention heads\n",
    "ff_dim = 64  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = layers.Input(shape=X_shape)\n",
    "embedding_layer = TokenAndPositionEmbedding(X_shape[0], X_shape[1])\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(X_shape[1], num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = Dense(len(train_dataset_parquet.unique_labels), activation='softmax')(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", \"Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k.tf\"),\n",
    "            run_name=\"Transformer-D=0.1-Dense=128-LipsEyesHandsPose_5k\",\n",
    "            mediapipe_features=\"reduced-LipsEyesHandsPose\",\n",
    "            USE_WANDB=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:k5c6maot) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁</td></tr><tr><td>epoch/epoch</td><td>▁</td></tr><tr><td>epoch/learning_rate</td><td>▁</td></tr><tr><td>epoch/loss</td><td>▁</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁</td></tr><tr><td>epoch/val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.121</td></tr><tr><td>epoch/epoch</td><td>0</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>2.5331</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.206</td></tr><tr><td>epoch/val_loss</td><td>2.42635</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/k5c6maot' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/k5c6maot</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240416_150234-k5c6maot/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:k5c6maot). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a23036c3b44d3cbdea0da41ee22146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112549859616492, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240416_150354-tqe9uruf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/tqe9uruf' target=\"_blank\">Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/tqe9uruf' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/tqe9uruf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "    497/Unknown - 10s 11ms/step - loss: 2.5373 - categorical_accuracy: 0.1031"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 15:04:11.861998: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 5374993014047653291\n",
      "2024-04-16 15:04:11.862066: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8438867370503118669\n",
      "2024-04-16 15:04:11.862087: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7325566132977353672\n",
      "2024-04-16 15:04:12.853987: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 12812880993351260521\n",
      "2024-04-16 15:04:12.854041: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 534613766666025935\n",
      "2024-04-16 15:04:12.854053: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 516448505323512496\n",
      "2024-04-16 15:04:12.854060: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 13327268128286139334\n",
      "2024-04-16 15:04:12.854088: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7726172902863430400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 16s 23ms/step - loss: 2.5371 - categorical_accuracy: 0.1037 - val_loss: 2.4879 - val_categorical_accuracy: 0.1490\n",
      "Epoch 2/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.4198 - categorical_accuracy: 0.1652INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 2.4199 - categorical_accuracy: 0.1653 - val_loss: 2.3782 - val_categorical_accuracy: 0.1880\n",
      "Epoch 3/300\n",
      "496/500 [============================>.] - ETA: 0s - loss: 2.3815 - categorical_accuracy: 0.1875INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 22ms/step - loss: 2.3809 - categorical_accuracy: 0.1880 - val_loss: 2.3528 - val_categorical_accuracy: 0.2010\n",
      "Epoch 4/300\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.3464 - categorical_accuracy: 0.2015INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 19ms/step - loss: 2.3464 - categorical_accuracy: 0.2015 - val_loss: 2.3278 - val_categorical_accuracy: 0.2190\n",
      "Epoch 5/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.3176 - categorical_accuracy: 0.2188INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 2.3173 - categorical_accuracy: 0.2188 - val_loss: 2.2920 - val_categorical_accuracy: 0.2280\n",
      "Epoch 6/300\n",
      "498/500 [============================>.] - ETA: 0s - loss: 2.2915 - categorical_accuracy: 0.2324INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 2.2909 - categorical_accuracy: 0.2327 - val_loss: 2.2780 - val_categorical_accuracy: 0.2410\n",
      "Epoch 7/300\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.2636 - categorical_accuracy: 0.2525INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 22ms/step - loss: 2.2636 - categorical_accuracy: 0.2525 - val_loss: 2.2664 - val_categorical_accuracy: 0.2580\n",
      "Epoch 8/300\n",
      "498/500 [============================>.] - ETA: 0s - loss: 2.2438 - categorical_accuracy: 0.2676INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 22ms/step - loss: 2.2428 - categorical_accuracy: 0.2680 - val_loss: 2.2554 - val_categorical_accuracy: 0.2810\n",
      "Epoch 9/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.2222 - categorical_accuracy: 0.2782INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 19ms/step - loss: 2.2205 - categorical_accuracy: 0.2795 - val_loss: 2.2389 - val_categorical_accuracy: 0.2870\n",
      "Epoch 10/300\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.1982 - categorical_accuracy: 0.2955INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 22ms/step - loss: 2.1982 - categorical_accuracy: 0.2955 - val_loss: 2.2161 - val_categorical_accuracy: 0.2880\n",
      "Epoch 11/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.1808 - categorical_accuracy: 0.2995INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 19ms/step - loss: 2.1791 - categorical_accuracy: 0.3010 - val_loss: 2.1700 - val_categorical_accuracy: 0.3140\n",
      "Epoch 12/300\n",
      "496/500 [============================>.] - ETA: 0s - loss: 2.1617 - categorical_accuracy: 0.3304INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 22ms/step - loss: 2.1594 - categorical_accuracy: 0.3318 - val_loss: 2.1288 - val_categorical_accuracy: 0.3360\n",
      "Epoch 13/300\n",
      "498/500 [============================>.] - ETA: 0s - loss: 2.1319 - categorical_accuracy: 0.3386INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 22ms/step - loss: 2.1306 - categorical_accuracy: 0.3390 - val_loss: 2.1218 - val_categorical_accuracy: 0.3410\n",
      "Epoch 14/300\n",
      "498/500 [============================>.] - ETA: 0s - loss: 2.1130 - categorical_accuracy: 0.3509INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 19ms/step - loss: 2.1116 - categorical_accuracy: 0.3512 - val_loss: 2.1239 - val_categorical_accuracy: 0.3550\n",
      "Epoch 15/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.0892 - categorical_accuracy: 0.3607INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 22ms/step - loss: 2.0869 - categorical_accuracy: 0.3625 - val_loss: 2.1136 - val_categorical_accuracy: 0.3800\n",
      "Epoch 16/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.0593 - categorical_accuracy: 0.3919INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 19ms/step - loss: 2.0576 - categorical_accuracy: 0.3930 - val_loss: 2.0695 - val_categorical_accuracy: 0.4000\n",
      "Epoch 17/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 2.0388 - categorical_accuracy: 0.3880 - val_loss: 2.1097 - val_categorical_accuracy: 0.3890\n",
      "Epoch 18/300\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.0221 - categorical_accuracy: 0.4085INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 2.0221 - categorical_accuracy: 0.4085 - val_loss: 2.0939 - val_categorical_accuracy: 0.4100\n",
      "Epoch 19/300\n",
      "498/500 [============================>.] - ETA: 0s - loss: 2.0007 - categorical_accuracy: 0.4212INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 19ms/step - loss: 1.9995 - categorical_accuracy: 0.4215 - val_loss: 2.0792 - val_categorical_accuracy: 0.4150\n",
      "Epoch 20/300\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 1.9972 - categorical_accuracy: 0.4218 - val_loss: 2.0787 - val_categorical_accuracy: 0.4070\n",
      "Epoch 21/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.9722 - categorical_accuracy: 0.4372 - val_loss: 2.0706 - val_categorical_accuracy: 0.4070\n",
      "Epoch 22/300\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.9489 - categorical_accuracy: 0.4558INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 22ms/step - loss: 1.9482 - categorical_accuracy: 0.4557 - val_loss: 2.0420 - val_categorical_accuracy: 0.4280\n",
      "Epoch 23/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.9357 - categorical_accuracy: 0.4620 - val_loss: 2.0453 - val_categorical_accuracy: 0.4220\n",
      "Epoch 24/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.9158 - categorical_accuracy: 0.4700 - val_loss: 2.0506 - val_categorical_accuracy: 0.4220\n",
      "Epoch 25/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.9124 - categorical_accuracy: 0.4780 - val_loss: 2.0527 - val_categorical_accuracy: 0.4270\n",
      "Epoch 26/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.9023 - categorical_accuracy: 0.4733 - val_loss: 2.0364 - val_categorical_accuracy: 0.4270\n",
      "Epoch 27/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.8808 - categorical_accuracy: 0.4938 - val_loss: 2.0443 - val_categorical_accuracy: 0.4230\n",
      "Epoch 28/300\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.8679 - categorical_accuracy: 0.5013INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 1.8669 - categorical_accuracy: 0.5020 - val_loss: 2.0141 - val_categorical_accuracy: 0.4310\n",
      "Epoch 29/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.8450 - categorical_accuracy: 0.5125 - val_loss: 2.0509 - val_categorical_accuracy: 0.4230\n",
      "Epoch 30/300\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.8551 - categorical_accuracy: 0.5040INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 19ms/step - loss: 1.8535 - categorical_accuracy: 0.5058 - val_loss: 2.0087 - val_categorical_accuracy: 0.4380\n",
      "Epoch 31/300\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.8239 - categorical_accuracy: 0.5261INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 22ms/step - loss: 1.8227 - categorical_accuracy: 0.5272 - val_loss: 2.0286 - val_categorical_accuracy: 0.4430\n",
      "Epoch 32/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.8051 - categorical_accuracy: 0.5403 - val_loss: 2.0569 - val_categorical_accuracy: 0.4310\n",
      "Epoch 33/300\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.7920 - categorical_accuracy: 0.5508INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 19ms/step - loss: 1.7920 - categorical_accuracy: 0.5508 - val_loss: 2.0022 - val_categorical_accuracy: 0.4530\n",
      "Epoch 34/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7777 - categorical_accuracy: 0.5548 - val_loss: 2.0407 - val_categorical_accuracy: 0.4220\n",
      "Epoch 35/300\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.7624 - categorical_accuracy: 0.5693INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 22ms/step - loss: 1.7618 - categorical_accuracy: 0.5700 - val_loss: 2.0034 - val_categorical_accuracy: 0.4650\n",
      "Epoch 36/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.7542 - categorical_accuracy: 0.5773 - val_loss: 2.0069 - val_categorical_accuracy: 0.4480\n",
      "Epoch 37/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.7416 - categorical_accuracy: 0.5853 - val_loss: 1.9936 - val_categorical_accuracy: 0.4580\n",
      "Epoch 38/300\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.7222 - categorical_accuracy: 0.5956INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 23ms/step - loss: 1.7209 - categorical_accuracy: 0.5970 - val_loss: 1.9826 - val_categorical_accuracy: 0.4730\n",
      "Epoch 39/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7116 - categorical_accuracy: 0.5925 - val_loss: 1.9828 - val_categorical_accuracy: 0.4630\n",
      "Epoch 40/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6992 - categorical_accuracy: 0.6022 - val_loss: 1.9956 - val_categorical_accuracy: 0.4560\n",
      "Epoch 41/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.6924 - categorical_accuracy: 0.6183 - val_loss: 2.0027 - val_categorical_accuracy: 0.4730\n",
      "Epoch 42/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 1.6864 - categorical_accuracy: 0.6164INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 19ms/step - loss: 1.6852 - categorical_accuracy: 0.6177 - val_loss: 1.9843 - val_categorical_accuracy: 0.4760\n",
      "Epoch 43/300\n",
      "499/500 [============================>.] - ETA: 0s - loss: 1.6653 - categorical_accuracy: 0.6250INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 22ms/step - loss: 1.6645 - categorical_accuracy: 0.6255 - val_loss: 1.9784 - val_categorical_accuracy: 0.4860\n",
      "Epoch 44/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6696 - categorical_accuracy: 0.6263 - val_loss: 2.0002 - val_categorical_accuracy: 0.4710\n",
      "Epoch 45/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.6499 - categorical_accuracy: 0.6313 - val_loss: 1.9902 - val_categorical_accuracy: 0.4740\n",
      "Epoch 46/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6291 - categorical_accuracy: 0.6472 - val_loss: 1.9902 - val_categorical_accuracy: 0.4810\n",
      "Epoch 47/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6312 - categorical_accuracy: 0.6500 - val_loss: 1.9858 - val_categorical_accuracy: 0.4840\n",
      "Epoch 48/300\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.6203 - categorical_accuracy: 0.6532INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 19ms/step - loss: 1.6203 - categorical_accuracy: 0.6532 - val_loss: 2.0085 - val_categorical_accuracy: 0.4930\n",
      "Epoch 49/300\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 1.6146 - categorical_accuracy: 0.6605 - val_loss: 1.9846 - val_categorical_accuracy: 0.4920\n",
      "Epoch 50/300\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 1.5992 - categorical_accuracy: 0.6658 - val_loss: 2.0185 - val_categorical_accuracy: 0.4640\n",
      "Epoch 51/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.5935 - categorical_accuracy: 0.6720 - val_loss: 2.0112 - val_categorical_accuracy: 0.4710\n",
      "Epoch 52/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.5770 - categorical_accuracy: 0.6827 - val_loss: 2.0080 - val_categorical_accuracy: 0.4820\n",
      "Epoch 53/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.5738 - categorical_accuracy: 0.6855 - val_loss: 2.0038 - val_categorical_accuracy: 0.4900\n",
      "Epoch 54/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.5654 - categorical_accuracy: 0.6923 - val_loss: 2.0222 - val_categorical_accuracy: 0.4730\n",
      "Epoch 55/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.5621 - categorical_accuracy: 0.6982 - val_loss: 2.0401 - val_categorical_accuracy: 0.4740\n",
      "Epoch 56/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.5556 - categorical_accuracy: 0.6873 - val_loss: 2.0995 - val_categorical_accuracy: 0.4650\n",
      "Epoch 57/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.5395 - categorical_accuracy: 0.6980 - val_loss: 2.0595 - val_categorical_accuracy: 0.4770\n",
      "Epoch 58/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5349 - categorical_accuracy: 0.7020 - val_loss: 2.0905 - val_categorical_accuracy: 0.4530\n",
      "Epoch 59/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.5295 - categorical_accuracy: 0.7153 - val_loss: 2.0684 - val_categorical_accuracy: 0.4660\n",
      "Epoch 60/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5143 - categorical_accuracy: 0.7300 - val_loss: 2.0901 - val_categorical_accuracy: 0.4700\n",
      "Epoch 61/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5109 - categorical_accuracy: 0.7247 - val_loss: 2.0713 - val_categorical_accuracy: 0.4670\n",
      "Epoch 62/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.5064 - categorical_accuracy: 0.7220 - val_loss: 2.1147 - val_categorical_accuracy: 0.4390\n",
      "Epoch 63/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4923 - categorical_accuracy: 0.7335 - val_loss: 2.0633 - val_categorical_accuracy: 0.4840\n",
      "Epoch 64/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4817 - categorical_accuracy: 0.7435 - val_loss: 2.0833 - val_categorical_accuracy: 0.4790\n",
      "Epoch 65/300\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.4818 - categorical_accuracy: 0.7409INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 22ms/step - loss: 1.4811 - categorical_accuracy: 0.7415 - val_loss: 2.0877 - val_categorical_accuracy: 0.4950\n",
      "Epoch 66/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.4719 - categorical_accuracy: 0.7495 - val_loss: 2.0938 - val_categorical_accuracy: 0.4880\n",
      "Epoch 67/300\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 1.4577 - categorical_accuracy: 0.7535 - val_loss: 2.0786 - val_categorical_accuracy: 0.4880\n",
      "Epoch 68/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4655 - categorical_accuracy: 0.7495 - val_loss: 2.0968 - val_categorical_accuracy: 0.4770\n",
      "Epoch 69/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4440 - categorical_accuracy: 0.7655 - val_loss: 2.0839 - val_categorical_accuracy: 0.4820\n",
      "Epoch 70/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4599 - categorical_accuracy: 0.7580 - val_loss: 2.1008 - val_categorical_accuracy: 0.4810\n",
      "Epoch 71/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4412 - categorical_accuracy: 0.7667 - val_loss: 2.1198 - val_categorical_accuracy: 0.4820\n",
      "Epoch 72/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4322 - categorical_accuracy: 0.7700 - val_loss: 2.1085 - val_categorical_accuracy: 0.4900\n",
      "Epoch 73/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4294 - categorical_accuracy: 0.7728 - val_loss: 2.1103 - val_categorical_accuracy: 0.4800\n",
      "Epoch 74/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.4212 - categorical_accuracy: 0.7807 - val_loss: 2.0919 - val_categorical_accuracy: 0.4880\n",
      "Epoch 75/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4191 - categorical_accuracy: 0.7850 - val_loss: 2.1003 - val_categorical_accuracy: 0.4790\n",
      "Epoch 76/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4105 - categorical_accuracy: 0.7883 - val_loss: 2.1186 - val_categorical_accuracy: 0.4730\n",
      "Epoch 77/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.4168 - categorical_accuracy: 0.7790 - val_loss: 2.1199 - val_categorical_accuracy: 0.4750\n",
      "Epoch 78/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.4045 - categorical_accuracy: 0.7943 - val_loss: 2.1378 - val_categorical_accuracy: 0.4670\n",
      "Epoch 79/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.4093 - categorical_accuracy: 0.7845 - val_loss: 2.1418 - val_categorical_accuracy: 0.4650\n",
      "Epoch 80/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3902 - categorical_accuracy: 0.8015 - val_loss: 2.1049 - val_categorical_accuracy: 0.4780\n",
      "Epoch 81/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3825 - categorical_accuracy: 0.8020 - val_loss: 2.1366 - val_categorical_accuracy: 0.4690\n",
      "Epoch 82/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3948 - categorical_accuracy: 0.7983 - val_loss: 2.1153 - val_categorical_accuracy: 0.4780\n",
      "Epoch 83/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.3811 - categorical_accuracy: 0.8008 - val_loss: 2.1055 - val_categorical_accuracy: 0.4740\n",
      "Epoch 84/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3745 - categorical_accuracy: 0.8125 - val_loss: 2.0952 - val_categorical_accuracy: 0.4790\n",
      "Epoch 85/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.3676 - categorical_accuracy: 0.8150 - val_loss: 2.1104 - val_categorical_accuracy: 0.4760\n",
      "Epoch 86/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3654 - categorical_accuracy: 0.8183 - val_loss: 2.1094 - val_categorical_accuracy: 0.4880\n",
      "Epoch 87/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.3624 - categorical_accuracy: 0.8170 - val_loss: 2.1524 - val_categorical_accuracy: 0.4630\n",
      "Epoch 88/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.3522 - categorical_accuracy: 0.8232 - val_loss: 2.1183 - val_categorical_accuracy: 0.4920\n",
      "Epoch 89/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.3461 - categorical_accuracy: 0.8298 - val_loss: 2.1110 - val_categorical_accuracy: 0.4950\n",
      "Epoch 90/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3369 - categorical_accuracy: 0.8303 - val_loss: 2.1202 - val_categorical_accuracy: 0.4890\n",
      "Epoch 91/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3362 - categorical_accuracy: 0.8288 - val_loss: 2.1184 - val_categorical_accuracy: 0.4920\n",
      "Epoch 92/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3394 - categorical_accuracy: 0.8270 - val_loss: 2.1230 - val_categorical_accuracy: 0.4870\n",
      "Epoch 93/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.3364 - categorical_accuracy: 0.8325 - val_loss: 2.1120 - val_categorical_accuracy: 0.4920\n",
      "Epoch 94/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3330 - categorical_accuracy: 0.8353 - val_loss: 2.1151 - val_categorical_accuracy: 0.4800\n",
      "Epoch 95/300\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.3364 - categorical_accuracy: 0.8332INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 19ms/step - loss: 1.3359 - categorical_accuracy: 0.8335 - val_loss: 2.1024 - val_categorical_accuracy: 0.5000\n",
      "Epoch 96/300\n",
      "499/500 [============================>.] - ETA: 0s - loss: 1.3206 - categorical_accuracy: 0.8404INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 1.3202 - categorical_accuracy: 0.8407 - val_loss: 2.0999 - val_categorical_accuracy: 0.5110\n",
      "Epoch 97/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3237 - categorical_accuracy: 0.8410 - val_loss: 2.1189 - val_categorical_accuracy: 0.4990\n",
      "Epoch 98/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3229 - categorical_accuracy: 0.8465 - val_loss: 2.1259 - val_categorical_accuracy: 0.4970\n",
      "Epoch 99/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3091 - categorical_accuracy: 0.8485 - val_loss: 2.1368 - val_categorical_accuracy: 0.4970\n",
      "Epoch 100/300\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.3152 - categorical_accuracy: 0.8432INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 22ms/step - loss: 1.3143 - categorical_accuracy: 0.8440 - val_loss: 2.0867 - val_categorical_accuracy: 0.5150\n",
      "Epoch 101/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3104 - categorical_accuracy: 0.8493 - val_loss: 2.1164 - val_categorical_accuracy: 0.5080\n",
      "Epoch 102/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.3005 - categorical_accuracy: 0.8593 - val_loss: 2.0950 - val_categorical_accuracy: 0.5130\n",
      "Epoch 103/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3082 - categorical_accuracy: 0.8497 - val_loss: 2.0843 - val_categorical_accuracy: 0.5150\n",
      "Epoch 104/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2948 - categorical_accuracy: 0.8565 - val_loss: 2.0945 - val_categorical_accuracy: 0.5010\n",
      "Epoch 105/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2908 - categorical_accuracy: 0.8615 - val_loss: 2.0845 - val_categorical_accuracy: 0.5040\n",
      "Epoch 106/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2911 - categorical_accuracy: 0.8568 - val_loss: 2.0903 - val_categorical_accuracy: 0.5120\n",
      "Epoch 107/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2848 - categorical_accuracy: 0.8692 - val_loss: 2.1327 - val_categorical_accuracy: 0.4980\n",
      "Epoch 108/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2899 - categorical_accuracy: 0.8630 - val_loss: 2.1197 - val_categorical_accuracy: 0.5090\n",
      "Epoch 109/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2784 - categorical_accuracy: 0.8648 - val_loss: 2.1057 - val_categorical_accuracy: 0.5020\n",
      "Epoch 110/300\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.2874 - categorical_accuracy: 0.8650INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 1.2868 - categorical_accuracy: 0.8652 - val_loss: 2.1008 - val_categorical_accuracy: 0.5190\n",
      "Epoch 111/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2693 - categorical_accuracy: 0.8737 - val_loss: 2.0831 - val_categorical_accuracy: 0.5130\n",
      "Epoch 112/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2666 - categorical_accuracy: 0.8775 - val_loss: 2.1133 - val_categorical_accuracy: 0.4990\n",
      "Epoch 113/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2684 - categorical_accuracy: 0.8745 - val_loss: 2.0882 - val_categorical_accuracy: 0.5180\n",
      "Epoch 114/300\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 1.2698 - categorical_accuracy: 0.8715 - val_loss: 2.0992 - val_categorical_accuracy: 0.5070\n",
      "Epoch 115/300\n",
      "499/500 [============================>.] - ETA: 0s - loss: 1.2624 - categorical_accuracy: 0.8790INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 22ms/step - loss: 1.2621 - categorical_accuracy: 0.8792 - val_loss: 2.0957 - val_categorical_accuracy: 0.5220\n",
      "Epoch 116/300\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.2596 - categorical_accuracy: 0.8788INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 19ms/step - loss: 1.2594 - categorical_accuracy: 0.8790 - val_loss: 2.0835 - val_categorical_accuracy: 0.5270\n",
      "Epoch 117/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2587 - categorical_accuracy: 0.8805 - val_loss: 2.0938 - val_categorical_accuracy: 0.5240\n",
      "Epoch 118/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2558 - categorical_accuracy: 0.8823 - val_loss: 2.0946 - val_categorical_accuracy: 0.5200\n",
      "Epoch 119/300\n",
      "499/500 [============================>.] - ETA: 0s - loss: 1.2558 - categorical_accuracy: 0.8780INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 22ms/step - loss: 1.2556 - categorical_accuracy: 0.8783 - val_loss: 2.0817 - val_categorical_accuracy: 0.5280\n",
      "Epoch 120/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2511 - categorical_accuracy: 0.8790 - val_loss: 2.0972 - val_categorical_accuracy: 0.5200\n",
      "Epoch 121/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 1.2557 - categorical_accuracy: 0.8793INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 1.2550 - categorical_accuracy: 0.8798 - val_loss: 2.0880 - val_categorical_accuracy: 0.5360\n",
      "Epoch 122/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2465 - categorical_accuracy: 0.8867 - val_loss: 2.0852 - val_categorical_accuracy: 0.5210\n",
      "Epoch 123/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2418 - categorical_accuracy: 0.8917 - val_loss: 2.0874 - val_categorical_accuracy: 0.5170\n",
      "Epoch 124/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2500 - categorical_accuracy: 0.8820 - val_loss: 2.0782 - val_categorical_accuracy: 0.5130\n",
      "Epoch 125/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2350 - categorical_accuracy: 0.8957 - val_loss: 2.0869 - val_categorical_accuracy: 0.5280\n",
      "Epoch 126/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2437 - categorical_accuracy: 0.8870 - val_loss: 2.0725 - val_categorical_accuracy: 0.5230\n",
      "Epoch 127/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2399 - categorical_accuracy: 0.8955 - val_loss: 2.0771 - val_categorical_accuracy: 0.5280\n",
      "Epoch 128/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2317 - categorical_accuracy: 0.8932 - val_loss: 2.0736 - val_categorical_accuracy: 0.5210\n",
      "Epoch 129/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2279 - categorical_accuracy: 0.8970 - val_loss: 2.0895 - val_categorical_accuracy: 0.5210\n",
      "Epoch 130/300\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.2315 - categorical_accuracy: 0.8924INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 19ms/step - loss: 1.2315 - categorical_accuracy: 0.8925 - val_loss: 2.0840 - val_categorical_accuracy: 0.5390\n",
      "Epoch 131/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2291 - categorical_accuracy: 0.8907 - val_loss: 2.0585 - val_categorical_accuracy: 0.5320\n",
      "Epoch 132/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2237 - categorical_accuracy: 0.8975 - val_loss: 2.0736 - val_categorical_accuracy: 0.5310\n",
      "Epoch 133/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2241 - categorical_accuracy: 0.8992 - val_loss: 2.0737 - val_categorical_accuracy: 0.5230\n",
      "Epoch 134/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2258 - categorical_accuracy: 0.8970 - val_loss: 2.0748 - val_categorical_accuracy: 0.5250\n",
      "Epoch 135/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2151 - categorical_accuracy: 0.9070 - val_loss: 2.0834 - val_categorical_accuracy: 0.5250\n",
      "Epoch 136/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2171 - categorical_accuracy: 0.9078 - val_loss: 2.0779 - val_categorical_accuracy: 0.5270\n",
      "Epoch 137/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2126 - categorical_accuracy: 0.9078 - val_loss: 2.0565 - val_categorical_accuracy: 0.5300\n",
      "Epoch 138/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2070 - categorical_accuracy: 0.9135 - val_loss: 2.0681 - val_categorical_accuracy: 0.5240\n",
      "Epoch 139/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2159 - categorical_accuracy: 0.9060 - val_loss: 2.0707 - val_categorical_accuracy: 0.5300\n",
      "Epoch 140/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2123 - categorical_accuracy: 0.9080 - val_loss: 2.0650 - val_categorical_accuracy: 0.5230\n",
      "Epoch 141/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2113 - categorical_accuracy: 0.9135 - val_loss: 2.0682 - val_categorical_accuracy: 0.5380\n",
      "Epoch 142/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2055 - categorical_accuracy: 0.9093 - val_loss: 2.0842 - val_categorical_accuracy: 0.5280\n",
      "Epoch 143/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2002 - categorical_accuracy: 0.9183 - val_loss: 2.0708 - val_categorical_accuracy: 0.5260\n",
      "Epoch 144/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2063 - categorical_accuracy: 0.9145 - val_loss: 2.0711 - val_categorical_accuracy: 0.5310\n",
      "Epoch 145/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1832 - categorical_accuracy: 0.9283 - val_loss: 2.0665 - val_categorical_accuracy: 0.5330\n",
      "Epoch 146/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2004 - categorical_accuracy: 0.9107 - val_loss: 2.0896 - val_categorical_accuracy: 0.5260\n",
      "Epoch 147/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1984 - categorical_accuracy: 0.9135 - val_loss: 2.0673 - val_categorical_accuracy: 0.5370\n",
      "Epoch 148/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2018 - categorical_accuracy: 0.9115 - val_loss: 2.0653 - val_categorical_accuracy: 0.5280\n",
      "Epoch 149/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1884 - categorical_accuracy: 0.9243 - val_loss: 2.0773 - val_categorical_accuracy: 0.5270\n",
      "Epoch 150/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1957 - categorical_accuracy: 0.9185 - val_loss: 2.0549 - val_categorical_accuracy: 0.5390\n",
      "Epoch 151/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.4074 - categorical_accuracy: 0.7872 - val_loss: 2.1428 - val_categorical_accuracy: 0.5050\n",
      "Epoch 152/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.4069 - categorical_accuracy: 0.7900 - val_loss: 2.1898 - val_categorical_accuracy: 0.4870\n",
      "Epoch 153/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.3839 - categorical_accuracy: 0.8040 - val_loss: 2.1429 - val_categorical_accuracy: 0.5010\n",
      "Epoch 154/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3726 - categorical_accuracy: 0.8083 - val_loss: 2.1432 - val_categorical_accuracy: 0.4980\n",
      "Epoch 155/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3710 - categorical_accuracy: 0.8035 - val_loss: 2.1530 - val_categorical_accuracy: 0.4830\n",
      "Epoch 156/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3746 - categorical_accuracy: 0.8040 - val_loss: 2.1386 - val_categorical_accuracy: 0.5050\n",
      "Epoch 157/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3503 - categorical_accuracy: 0.8207 - val_loss: 2.1388 - val_categorical_accuracy: 0.5010\n",
      "Epoch 158/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3454 - categorical_accuracy: 0.8205 - val_loss: 2.1039 - val_categorical_accuracy: 0.5130\n",
      "Epoch 159/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3497 - categorical_accuracy: 0.8220 - val_loss: 2.1116 - val_categorical_accuracy: 0.4990\n",
      "Epoch 160/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3398 - categorical_accuracy: 0.8213 - val_loss: 2.1432 - val_categorical_accuracy: 0.4830\n",
      "Epoch 161/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3402 - categorical_accuracy: 0.8288 - val_loss: 2.1161 - val_categorical_accuracy: 0.5000\n",
      "Epoch 162/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3489 - categorical_accuracy: 0.8235 - val_loss: 2.1537 - val_categorical_accuracy: 0.4710\n",
      "Epoch 163/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3275 - categorical_accuracy: 0.8307 - val_loss: 2.1453 - val_categorical_accuracy: 0.4970\n",
      "Epoch 164/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3177 - categorical_accuracy: 0.8418 - val_loss: 2.1400 - val_categorical_accuracy: 0.4890\n",
      "Epoch 165/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3183 - categorical_accuracy: 0.8393 - val_loss: 2.1412 - val_categorical_accuracy: 0.4780\n",
      "Epoch 166/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3254 - categorical_accuracy: 0.8345 - val_loss: 2.1491 - val_categorical_accuracy: 0.4940\n",
      "Epoch 167/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3125 - categorical_accuracy: 0.8393 - val_loss: 2.1401 - val_categorical_accuracy: 0.5030\n",
      "Epoch 168/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3066 - categorical_accuracy: 0.8500 - val_loss: 2.1626 - val_categorical_accuracy: 0.4920\n",
      "Epoch 169/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3018 - categorical_accuracy: 0.8505 - val_loss: 2.1543 - val_categorical_accuracy: 0.5080\n",
      "Epoch 170/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2928 - categorical_accuracy: 0.8560 - val_loss: 2.1229 - val_categorical_accuracy: 0.5050\n",
      "Epoch 171/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2884 - categorical_accuracy: 0.8608 - val_loss: 2.1345 - val_categorical_accuracy: 0.5090\n",
      "Epoch 172/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2935 - categorical_accuracy: 0.8565 - val_loss: 2.1412 - val_categorical_accuracy: 0.4990\n",
      "Epoch 173/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2877 - categorical_accuracy: 0.8618 - val_loss: 2.1976 - val_categorical_accuracy: 0.4720\n",
      "Epoch 174/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2842 - categorical_accuracy: 0.8600 - val_loss: 2.1223 - val_categorical_accuracy: 0.5020\n",
      "Epoch 175/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2755 - categorical_accuracy: 0.8680 - val_loss: 2.0781 - val_categorical_accuracy: 0.4940\n",
      "Epoch 176/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2801 - categorical_accuracy: 0.8665 - val_loss: 2.1420 - val_categorical_accuracy: 0.4940\n",
      "Epoch 177/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2659 - categorical_accuracy: 0.8668 - val_loss: 2.1978 - val_categorical_accuracy: 0.4760\n",
      "Epoch 178/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2689 - categorical_accuracy: 0.8675 - val_loss: 2.1481 - val_categorical_accuracy: 0.4850\n",
      "Epoch 179/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2624 - categorical_accuracy: 0.8760 - val_loss: 2.1343 - val_categorical_accuracy: 0.5000\n",
      "Epoch 180/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2618 - categorical_accuracy: 0.8783 - val_loss: 2.1984 - val_categorical_accuracy: 0.4730\n",
      "Epoch 181/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2593 - categorical_accuracy: 0.8777 - val_loss: 2.1342 - val_categorical_accuracy: 0.4930\n",
      "Epoch 182/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2588 - categorical_accuracy: 0.8750 - val_loss: 2.1142 - val_categorical_accuracy: 0.4950\n",
      "Epoch 183/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2492 - categorical_accuracy: 0.8813 - val_loss: 2.1321 - val_categorical_accuracy: 0.4960\n",
      "Epoch 184/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2403 - categorical_accuracy: 0.8907 - val_loss: 2.0977 - val_categorical_accuracy: 0.5110\n",
      "Epoch 185/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2400 - categorical_accuracy: 0.8848 - val_loss: 2.1417 - val_categorical_accuracy: 0.4930\n",
      "Epoch 186/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2530 - categorical_accuracy: 0.8798 - val_loss: 2.1025 - val_categorical_accuracy: 0.5040\n",
      "Epoch 187/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2363 - categorical_accuracy: 0.8895 - val_loss: 2.1243 - val_categorical_accuracy: 0.5040\n",
      "Epoch 188/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2420 - categorical_accuracy: 0.8892 - val_loss: 2.1681 - val_categorical_accuracy: 0.4920\n",
      "Epoch 189/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2311 - categorical_accuracy: 0.8895 - val_loss: 2.1365 - val_categorical_accuracy: 0.4830\n",
      "Epoch 190/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2293 - categorical_accuracy: 0.8915 - val_loss: 2.1662 - val_categorical_accuracy: 0.4660\n",
      "Epoch 191/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2142 - categorical_accuracy: 0.9057 - val_loss: 2.1328 - val_categorical_accuracy: 0.5010\n",
      "Epoch 192/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2257 - categorical_accuracy: 0.8978 - val_loss: 2.1244 - val_categorical_accuracy: 0.5000\n",
      "Epoch 193/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2138 - categorical_accuracy: 0.9025 - val_loss: 2.1173 - val_categorical_accuracy: 0.5010\n",
      "Epoch 194/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2098 - categorical_accuracy: 0.9003 - val_loss: 2.1752 - val_categorical_accuracy: 0.4990\n",
      "Epoch 195/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2135 - categorical_accuracy: 0.9032 - val_loss: 2.1086 - val_categorical_accuracy: 0.5170\n",
      "Epoch 196/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2029 - categorical_accuracy: 0.9070 - val_loss: 2.1442 - val_categorical_accuracy: 0.5010\n",
      "Epoch 197/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2141 - categorical_accuracy: 0.9045 - val_loss: 2.1613 - val_categorical_accuracy: 0.4850\n",
      "Epoch 198/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2108 - categorical_accuracy: 0.9050 - val_loss: 2.1157 - val_categorical_accuracy: 0.5010\n",
      "Epoch 199/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2005 - categorical_accuracy: 0.9112 - val_loss: 2.1409 - val_categorical_accuracy: 0.4880\n",
      "Epoch 200/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2066 - categorical_accuracy: 0.9055 - val_loss: 2.1208 - val_categorical_accuracy: 0.5010\n",
      "Epoch 201/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1997 - categorical_accuracy: 0.9135 - val_loss: 2.1153 - val_categorical_accuracy: 0.4910\n",
      "Epoch 202/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1919 - categorical_accuracy: 0.9155 - val_loss: 2.1521 - val_categorical_accuracy: 0.4810\n",
      "Epoch 203/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1901 - categorical_accuracy: 0.9145 - val_loss: 2.1466 - val_categorical_accuracy: 0.4780\n",
      "Epoch 204/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1859 - categorical_accuracy: 0.9202 - val_loss: 2.1283 - val_categorical_accuracy: 0.4970\n",
      "Epoch 205/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1766 - categorical_accuracy: 0.9260 - val_loss: 2.1102 - val_categorical_accuracy: 0.5050\n",
      "Epoch 206/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1795 - categorical_accuracy: 0.9230 - val_loss: 2.1400 - val_categorical_accuracy: 0.4910\n",
      "Epoch 207/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1750 - categorical_accuracy: 0.9265 - val_loss: 2.1428 - val_categorical_accuracy: 0.4890\n",
      "Epoch 208/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1761 - categorical_accuracy: 0.9227 - val_loss: 2.1508 - val_categorical_accuracy: 0.5130\n",
      "Epoch 209/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1709 - categorical_accuracy: 0.9285 - val_loss: 2.1467 - val_categorical_accuracy: 0.4900\n",
      "Epoch 210/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1640 - categorical_accuracy: 0.9330 - val_loss: 2.1661 - val_categorical_accuracy: 0.4770\n",
      "Epoch 211/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1700 - categorical_accuracy: 0.9245 - val_loss: 2.1216 - val_categorical_accuracy: 0.5010\n",
      "Epoch 212/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1629 - categorical_accuracy: 0.9388 - val_loss: 2.1515 - val_categorical_accuracy: 0.4950\n",
      "Epoch 213/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1655 - categorical_accuracy: 0.9327 - val_loss: 2.0878 - val_categorical_accuracy: 0.5250\n",
      "Epoch 214/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1546 - categorical_accuracy: 0.9352 - val_loss: 2.0961 - val_categorical_accuracy: 0.5230\n",
      "Epoch 215/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1539 - categorical_accuracy: 0.9360 - val_loss: 2.1384 - val_categorical_accuracy: 0.4920\n",
      "Epoch 216/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1524 - categorical_accuracy: 0.9425 - val_loss: 2.1255 - val_categorical_accuracy: 0.4870\n",
      "Epoch 217/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1537 - categorical_accuracy: 0.9352 - val_loss: 2.1221 - val_categorical_accuracy: 0.5060\n",
      "Epoch 218/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1477 - categorical_accuracy: 0.9427 - val_loss: 2.1412 - val_categorical_accuracy: 0.4950\n",
      "Epoch 219/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1427 - categorical_accuracy: 0.9433 - val_loss: 2.1565 - val_categorical_accuracy: 0.4970\n",
      "Epoch 220/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1431 - categorical_accuracy: 0.9427 - val_loss: 2.1176 - val_categorical_accuracy: 0.4970\n",
      "Epoch 221/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1388 - categorical_accuracy: 0.9465 - val_loss: 2.1252 - val_categorical_accuracy: 0.4970\n",
      "Epoch 222/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1406 - categorical_accuracy: 0.9448 - val_loss: 2.1105 - val_categorical_accuracy: 0.5000\n",
      "Epoch 223/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1395 - categorical_accuracy: 0.9467 - val_loss: 2.0982 - val_categorical_accuracy: 0.5140\n",
      "Epoch 224/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1335 - categorical_accuracy: 0.9495 - val_loss: 2.1065 - val_categorical_accuracy: 0.5070\n",
      "Epoch 225/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1312 - categorical_accuracy: 0.9520 - val_loss: 2.1163 - val_categorical_accuracy: 0.5000\n",
      "Epoch 226/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1280 - categorical_accuracy: 0.9498 - val_loss: 2.1225 - val_categorical_accuracy: 0.4960\n",
      "Epoch 227/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1257 - categorical_accuracy: 0.9515 - val_loss: 2.1427 - val_categorical_accuracy: 0.4830\n",
      "Epoch 228/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1266 - categorical_accuracy: 0.9523 - val_loss: 2.1028 - val_categorical_accuracy: 0.4950\n",
      "Epoch 229/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1220 - categorical_accuracy: 0.9545 - val_loss: 2.0979 - val_categorical_accuracy: 0.5020\n",
      "Epoch 230/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1260 - categorical_accuracy: 0.9542 - val_loss: 2.0898 - val_categorical_accuracy: 0.5060\n",
      "Epoch 231/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1169 - categorical_accuracy: 0.9592 - val_loss: 2.0967 - val_categorical_accuracy: 0.5180\n",
      "Epoch 232/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1190 - categorical_accuracy: 0.9563 - val_loss: 2.1308 - val_categorical_accuracy: 0.4960\n",
      "Epoch 233/300\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 1.1161 - categorical_accuracy: 0.9555 - val_loss: 2.0706 - val_categorical_accuracy: 0.5220\n",
      "Epoch 234/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1131 - categorical_accuracy: 0.9607 - val_loss: 2.0693 - val_categorical_accuracy: 0.5160\n",
      "Epoch 235/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1152 - categorical_accuracy: 0.9578 - val_loss: 2.0684 - val_categorical_accuracy: 0.5280\n",
      "Epoch 236/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1081 - categorical_accuracy: 0.9610 - val_loss: 2.0701 - val_categorical_accuracy: 0.5310\n",
      "Epoch 237/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1112 - categorical_accuracy: 0.9575 - val_loss: 2.0781 - val_categorical_accuracy: 0.5240\n",
      "Epoch 238/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1093 - categorical_accuracy: 0.9572 - val_loss: 2.0812 - val_categorical_accuracy: 0.5180\n",
      "Epoch 239/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1058 - categorical_accuracy: 0.9650 - val_loss: 2.0540 - val_categorical_accuracy: 0.5200\n",
      "Epoch 240/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0983 - categorical_accuracy: 0.9678 - val_loss: 2.0687 - val_categorical_accuracy: 0.5220\n",
      "Epoch 241/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0976 - categorical_accuracy: 0.9650 - val_loss: 2.0646 - val_categorical_accuracy: 0.5130\n",
      "Epoch 242/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.0938 - categorical_accuracy: 0.9712 - val_loss: 2.0747 - val_categorical_accuracy: 0.5180\n",
      "Epoch 243/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0954 - categorical_accuracy: 0.9680 - val_loss: 2.0617 - val_categorical_accuracy: 0.5160\n",
      "Epoch 244/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0972 - categorical_accuracy: 0.9653 - val_loss: 2.0361 - val_categorical_accuracy: 0.5320\n",
      "Epoch 245/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0955 - categorical_accuracy: 0.9670 - val_loss: 2.0884 - val_categorical_accuracy: 0.5120\n",
      "Epoch 246/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0948 - categorical_accuracy: 0.9703 - val_loss: 2.0575 - val_categorical_accuracy: 0.5320\n",
      "Epoch 247/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0887 - categorical_accuracy: 0.9685 - val_loss: 2.0639 - val_categorical_accuracy: 0.5190\n",
      "Epoch 248/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0900 - categorical_accuracy: 0.9688 - val_loss: 2.0381 - val_categorical_accuracy: 0.5260\n",
      "Epoch 249/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0901 - categorical_accuracy: 0.9697 - val_loss: 2.0418 - val_categorical_accuracy: 0.5260\n",
      "Epoch 250/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0862 - categorical_accuracy: 0.9688 - val_loss: 2.0285 - val_categorical_accuracy: 0.5280\n",
      "Epoch 251/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0917 - categorical_accuracy: 0.9660 - val_loss: 2.0368 - val_categorical_accuracy: 0.5310\n",
      "Epoch 252/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0844 - categorical_accuracy: 0.9712 - val_loss: 2.0274 - val_categorical_accuracy: 0.5310\n",
      "Epoch 253/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0817 - categorical_accuracy: 0.9750 - val_loss: 2.0420 - val_categorical_accuracy: 0.5230\n",
      "Epoch 254/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0773 - categorical_accuracy: 0.9762 - val_loss: 2.0515 - val_categorical_accuracy: 0.5280\n",
      "Epoch 255/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.0836 - categorical_accuracy: 0.9695 - val_loss: 2.0420 - val_categorical_accuracy: 0.5280\n",
      "Epoch 256/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0772 - categorical_accuracy: 0.9728 - val_loss: 2.0315 - val_categorical_accuracy: 0.5230\n",
      "Epoch 257/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0770 - categorical_accuracy: 0.9770 - val_loss: 2.0451 - val_categorical_accuracy: 0.5290\n",
      "Epoch 258/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0820 - categorical_accuracy: 0.9745 - val_loss: 2.0372 - val_categorical_accuracy: 0.5250\n",
      "Epoch 259/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.0789 - categorical_accuracy: 0.9732 - val_loss: 2.0324 - val_categorical_accuracy: 0.5260\n",
      "Epoch 260/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0688 - categorical_accuracy: 0.9775 - val_loss: 2.0505 - val_categorical_accuracy: 0.5170\n",
      "Epoch 261/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0736 - categorical_accuracy: 0.9747 - val_loss: 2.0401 - val_categorical_accuracy: 0.5240\n",
      "Epoch 262/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.0709 - categorical_accuracy: 0.9812 - val_loss: 2.0309 - val_categorical_accuracy: 0.5300\n",
      "Epoch 263/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0739 - categorical_accuracy: 0.9718 - val_loss: 2.0204 - val_categorical_accuracy: 0.5300\n",
      "Epoch 264/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.0739 - categorical_accuracy: 0.9750 - val_loss: 2.0378 - val_categorical_accuracy: 0.5320\n",
      "Epoch 265/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0653 - categorical_accuracy: 0.9818 - val_loss: 2.0532 - val_categorical_accuracy: 0.5270\n",
      "Epoch 266/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0680 - categorical_accuracy: 0.9775 - val_loss: 2.0246 - val_categorical_accuracy: 0.5390\n",
      "Epoch 267/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0702 - categorical_accuracy: 0.9790 - val_loss: 2.0355 - val_categorical_accuracy: 0.5240\n",
      "Epoch 268/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.0660 - categorical_accuracy: 0.9793 - val_loss: 2.0352 - val_categorical_accuracy: 0.5280\n",
      "Epoch 269/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.0638 - categorical_accuracy: 0.9795 - val_loss: 2.0286 - val_categorical_accuracy: 0.5260\n",
      "Epoch 270/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0700 - categorical_accuracy: 0.9772 - val_loss: 2.0361 - val_categorical_accuracy: 0.5230\n",
      "Epoch 271/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0651 - categorical_accuracy: 0.9805 - val_loss: 2.0284 - val_categorical_accuracy: 0.5370\n",
      "Epoch 272/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0670 - categorical_accuracy: 0.9750 - val_loss: 2.0413 - val_categorical_accuracy: 0.5290\n",
      "Epoch 273/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.0629 - categorical_accuracy: 0.9793 - val_loss: 2.0324 - val_categorical_accuracy: 0.5290\n",
      "Epoch 274/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0631 - categorical_accuracy: 0.9803 - val_loss: 2.0275 - val_categorical_accuracy: 0.5280\n",
      "Epoch 275/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0623 - categorical_accuracy: 0.9820 - val_loss: 2.0303 - val_categorical_accuracy: 0.5330\n",
      "Epoch 276/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0620 - categorical_accuracy: 0.9812 - val_loss: 2.0379 - val_categorical_accuracy: 0.5230\n",
      "Epoch 277/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0571 - categorical_accuracy: 0.9833 - val_loss: 2.0293 - val_categorical_accuracy: 0.5240\n",
      "Epoch 278/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0594 - categorical_accuracy: 0.9820 - val_loss: 2.0151 - val_categorical_accuracy: 0.5350\n",
      "Epoch 279/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.0603 - categorical_accuracy: 0.9810 - val_loss: 2.0286 - val_categorical_accuracy: 0.5290\n",
      "Epoch 280/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0628 - categorical_accuracy: 0.9770 - val_loss: 2.0380 - val_categorical_accuracy: 0.5290\n",
      "Epoch 281/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.0575 - categorical_accuracy: 0.9810 - val_loss: 2.0269 - val_categorical_accuracy: 0.5330\n",
      "Epoch 282/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0561 - categorical_accuracy: 0.9843 - val_loss: 2.0206 - val_categorical_accuracy: 0.5300\n",
      "Epoch 283/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0567 - categorical_accuracy: 0.9818 - val_loss: 2.0248 - val_categorical_accuracy: 0.5350\n",
      "Epoch 284/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.0533 - categorical_accuracy: 0.9825 - val_loss: 2.0365 - val_categorical_accuracy: 0.5290\n",
      "Epoch 285/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.0554 - categorical_accuracy: 0.9827 - val_loss: 2.0439 - val_categorical_accuracy: 0.5260\n",
      "Epoch 286/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.0534 - categorical_accuracy: 0.9862 - val_loss: 2.0356 - val_categorical_accuracy: 0.5360\n",
      "Epoch 287/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 1.0578 - categorical_accuracy: 0.9814INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 1.0576 - categorical_accuracy: 0.9815 - val_loss: 2.0250 - val_categorical_accuracy: 0.5450\n",
      "Epoch 288/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0490 - categorical_accuracy: 0.9850 - val_loss: 2.0189 - val_categorical_accuracy: 0.5390\n",
      "Epoch 289/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0544 - categorical_accuracy: 0.9850 - val_loss: 2.0209 - val_categorical_accuracy: 0.5360\n",
      "Epoch 290/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.0533 - categorical_accuracy: 0.9820 - val_loss: 2.0082 - val_categorical_accuracy: 0.5380\n",
      "Epoch 291/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0540 - categorical_accuracy: 0.9827 - val_loss: 2.0284 - val_categorical_accuracy: 0.5390\n",
      "Epoch 292/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0528 - categorical_accuracy: 0.9833 - val_loss: 2.0162 - val_categorical_accuracy: 0.5330\n",
      "Epoch 293/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0517 - categorical_accuracy: 0.9835 - val_loss: 2.0257 - val_categorical_accuracy: 0.5290\n",
      "Epoch 294/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0507 - categorical_accuracy: 0.9843 - val_loss: 2.0274 - val_categorical_accuracy: 0.5300\n",
      "Epoch 295/300\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 1.0470 - categorical_accuracy: 0.9843 - val_loss: 2.0264 - val_categorical_accuracy: 0.5270\n",
      "Epoch 296/300\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 1.0524 - categorical_accuracy: 0.9843 - val_loss: 2.0246 - val_categorical_accuracy: 0.5260\n",
      "Epoch 297/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.0484 - categorical_accuracy: 0.9852 - val_loss: 2.0225 - val_categorical_accuracy: 0.5260\n",
      "Epoch 298/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0461 - categorical_accuracy: 0.9883 - val_loss: 2.0153 - val_categorical_accuracy: 0.5350\n",
      "Epoch 299/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.0477 - categorical_accuracy: 0.9820 - val_loss: 2.0162 - val_categorical_accuracy: 0.5320\n",
      "Epoch 300/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.0491 - categorical_accuracy: 0.9852 - val_loss: 2.0170 - val_categorical_accuracy: 0.5390\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁▂▃▃▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁████▇▇▆▆▅▅▄▄▃▃▂▂▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▇▆▆▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁▂▅▅▆▆▇▆▇▇▇▇▇▇▇█▇███▇▇▇▇▇▇▇▇▇▇▇█████████</td></tr><tr><td>epoch/val_loss</td><td>█▆▃▂▂▁▁▃▃▃▄▃▄▃▄▃▃▃▂▂▅▄▅▄▃▄▄▄▄▃▃▃▂▂▂▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.98525</td></tr><tr><td>epoch/epoch</td><td>299</td></tr><tr><td>epoch/learning_rate</td><td>5e-05</td></tr><tr><td>epoch/loss</td><td>1.04907</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.539</td></tr><tr><td>epoch/val_loss</td><td>2.01703</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/tqe9uruf' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/tqe9uruf</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240416_150354-tqe9uruf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f954f739d50>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dim = 128  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 128  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = layers.Input(shape=X_shape)\n",
    "embedding_layer = TokenAndPositionEmbedding(X_shape[0], X_shape[1])\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(X_shape[1], num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(128, activation=\"gelu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(256, activation=\"gelu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = Dense(len(train_dataset_parquet.unique_labels), activation='softmax')(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", f\"Transformer-embed={embed_dim}-heads={num_heads}-ff={ff_dim}-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf\"),\n",
    "            run_name=\"Transformer-embed=128-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k\",\n",
    "            mediapipe_features=\"reduced-LipsEyesHandsPose\",\n",
    "            T_max=151, epochs=300, \n",
    "            max_lr = 3e-4, min_lr = 5e-5,\n",
    "            USE_WANDB=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240416_154118-u544yx92</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/u544yx92' target=\"_blank\">Transformer-embed=128-heads=2-ff=128-D=0.2-Dense=256-LipsEyesHandsPose_5k</a></strong> to <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/u544yx92' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/u544yx92</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "    498/Unknown - 10s 11ms/step - loss: 2.5964 - categorical_accuracy: 0.1082"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 15:41:28.686715: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 16023303133924989040\n",
      "2024-04-16 15:41:28.686813: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 6700206116132250662\n",
      "2024-04-16 15:41:28.686842: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7063810106286413230\n",
      "2024-04-16 15:41:28.686864: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2724364067725415920\n",
      "2024-04-16 15:41:28.686873: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 13169199823044535358\n",
      "2024-04-16 15:41:28.686904: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 8397698261506511473\n",
      "2024-04-16 15:41:28.686946: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11728785306099647447\n",
      "2024-04-16 15:41:28.686962: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 13714633262705169763\n",
      "2024-04-16 15:41:29.625282: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 5551213241208487965\n",
      "2024-04-16 15:41:29.625367: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 3094190343660208166\n",
      "2024-04-16 15:41:29.625385: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7767172026591208356\n",
      "2024-04-16 15:41:29.625407: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 6462147652021642174\n",
      "2024-04-16 15:41:29.625454: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4153888742695321310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 14s 19ms/step - loss: 2.5963 - categorical_accuracy: 0.1082 - val_loss: 2.4758 - val_categorical_accuracy: 0.1380\n",
      "Epoch 2/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.4528 - categorical_accuracy: 0.1564INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 2.4527 - categorical_accuracy: 0.1565 - val_loss: 2.3993 - val_categorical_accuracy: 0.1880\n",
      "Epoch 3/300\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.4036 - categorical_accuracy: 0.1723INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.4036 - categorical_accuracy: 0.1723 - val_loss: 2.3626 - val_categorical_accuracy: 0.1930\n",
      "Epoch 4/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.3696 - categorical_accuracy: 0.1937INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 2.3690 - categorical_accuracy: 0.1940 - val_loss: 2.3445 - val_categorical_accuracy: 0.2080\n",
      "Epoch 5/300\n",
      "496/500 [============================>.] - ETA: 0s - loss: 2.3356 - categorical_accuracy: 0.2117INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.3349 - categorical_accuracy: 0.2118 - val_loss: 2.2991 - val_categorical_accuracy: 0.2360\n",
      "Epoch 6/300\n",
      "498/500 [============================>.] - ETA: 0s - loss: 2.3101 - categorical_accuracy: 0.2374INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 2.3098 - categorical_accuracy: 0.2373 - val_loss: 2.3202 - val_categorical_accuracy: 0.2390\n",
      "Epoch 7/300\n",
      "499/500 [============================>.] - ETA: 0s - loss: 2.2881 - categorical_accuracy: 0.2490INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 2.2880 - categorical_accuracy: 0.2490 - val_loss: 2.3125 - val_categorical_accuracy: 0.2440\n",
      "Epoch 8/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.2672 - categorical_accuracy: 0.2540INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 2.2661 - categorical_accuracy: 0.2545 - val_loss: 2.2288 - val_categorical_accuracy: 0.2680\n",
      "Epoch 9/300\n",
      "496/500 [============================>.] - ETA: 0s - loss: 2.2416 - categorical_accuracy: 0.2752INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.2405 - categorical_accuracy: 0.2763 - val_loss: 2.2395 - val_categorical_accuracy: 0.2820\n",
      "Epoch 10/300\n",
      "500/500 [==============================] - ETA: 0s - loss: 2.2199 - categorical_accuracy: 0.2870INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 2.2199 - categorical_accuracy: 0.2870 - val_loss: 2.2130 - val_categorical_accuracy: 0.2840\n",
      "Epoch 11/300\n",
      "499/500 [============================>.] - ETA: 0s - loss: 2.1954 - categorical_accuracy: 0.3071INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 2.1946 - categorical_accuracy: 0.3077 - val_loss: 2.1753 - val_categorical_accuracy: 0.3090\n",
      "Epoch 12/300\n",
      "495/500 [============================>.] - ETA: 0s - loss: 2.1768 - categorical_accuracy: 0.3139INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 2.1745 - categorical_accuracy: 0.3153 - val_loss: 2.1552 - val_categorical_accuracy: 0.3150\n",
      "Epoch 13/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 2.1623 - categorical_accuracy: 0.3302INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 2.1601 - categorical_accuracy: 0.3310 - val_loss: 2.1198 - val_categorical_accuracy: 0.3640\n",
      "Epoch 14/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 2.1316 - categorical_accuracy: 0.3512 - val_loss: 2.1175 - val_categorical_accuracy: 0.3450\n",
      "Epoch 15/300\n",
      "496/500 [============================>.] - ETA: 0s - loss: 2.1173 - categorical_accuracy: 0.3705INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 2.1152 - categorical_accuracy: 0.3715 - val_loss: 2.1046 - val_categorical_accuracy: 0.3670\n",
      "Epoch 16/300\n",
      "499/500 [============================>.] - ETA: 0s - loss: 2.0963 - categorical_accuracy: 0.3775INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 2.0953 - categorical_accuracy: 0.3780 - val_loss: 2.0665 - val_categorical_accuracy: 0.3880\n",
      "Epoch 17/300\n",
      "499/500 [============================>.] - ETA: 0s - loss: 2.0780 - categorical_accuracy: 0.3848INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 2.0774 - categorical_accuracy: 0.3853 - val_loss: 2.1007 - val_categorical_accuracy: 0.4030\n",
      "Epoch 18/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 2.0551 - categorical_accuracy: 0.4155 - val_loss: 2.0347 - val_categorical_accuracy: 0.3960\n",
      "Epoch 19/300\n",
      "498/500 [============================>.] - ETA: 0s - loss: 2.0356 - categorical_accuracy: 0.4081INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 2.0342 - categorical_accuracy: 0.4092 - val_loss: 2.0444 - val_categorical_accuracy: 0.4130\n",
      "Epoch 20/300\n",
      "499/500 [============================>.] - ETA: 0s - loss: 2.0143 - categorical_accuracy: 0.4243INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 2.0134 - categorical_accuracy: 0.4245 - val_loss: 2.0523 - val_categorical_accuracy: 0.4160\n",
      "Epoch 21/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.9961 - categorical_accuracy: 0.4383 - val_loss: 2.0486 - val_categorical_accuracy: 0.4160\n",
      "Epoch 22/300\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.9737 - categorical_accuracy: 0.4566INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 1.9724 - categorical_accuracy: 0.4570 - val_loss: 2.0074 - val_categorical_accuracy: 0.4450\n",
      "Epoch 23/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.9662 - categorical_accuracy: 0.4550 - val_loss: 2.0210 - val_categorical_accuracy: 0.4400\n",
      "Epoch 24/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.9454 - categorical_accuracy: 0.4708 - val_loss: 2.0270 - val_categorical_accuracy: 0.4260\n",
      "Epoch 25/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.9326 - categorical_accuracy: 0.4710 - val_loss: 2.0832 - val_categorical_accuracy: 0.4230\n",
      "Epoch 26/300\n",
      "499/500 [============================>.] - ETA: 0s - loss: 1.9131 - categorical_accuracy: 0.4975INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 1.9126 - categorical_accuracy: 0.4978 - val_loss: 1.9986 - val_categorical_accuracy: 0.4640\n",
      "Epoch 27/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 1.8975 - categorical_accuracy: 0.5005INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 1.8962 - categorical_accuracy: 0.5010 - val_loss: 1.9515 - val_categorical_accuracy: 0.4810\n",
      "Epoch 28/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.8794 - categorical_accuracy: 0.5075 - val_loss: 1.9579 - val_categorical_accuracy: 0.4650\n",
      "Epoch 29/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.8784 - categorical_accuracy: 0.5170 - val_loss: 1.9601 - val_categorical_accuracy: 0.4780\n",
      "Epoch 30/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.8600 - categorical_accuracy: 0.5235 - val_loss: 2.0044 - val_categorical_accuracy: 0.4620\n",
      "Epoch 31/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.8478 - categorical_accuracy: 0.5245 - val_loss: 2.0177 - val_categorical_accuracy: 0.4620\n",
      "Epoch 32/300\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.8412 - categorical_accuracy: 0.5325INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 19ms/step - loss: 1.8412 - categorical_accuracy: 0.5325 - val_loss: 1.9413 - val_categorical_accuracy: 0.4880\n",
      "Epoch 33/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.8198 - categorical_accuracy: 0.5525 - val_loss: 2.0235 - val_categorical_accuracy: 0.4590\n",
      "Epoch 34/300\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.7980 - categorical_accuracy: 0.5638INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 1.7976 - categorical_accuracy: 0.5635 - val_loss: 1.9754 - val_categorical_accuracy: 0.4910\n",
      "Epoch 35/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7974 - categorical_accuracy: 0.5560 - val_loss: 2.0355 - val_categorical_accuracy: 0.4760\n",
      "Epoch 36/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7982 - categorical_accuracy: 0.5565 - val_loss: 2.0018 - val_categorical_accuracy: 0.4740\n",
      "Epoch 37/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7745 - categorical_accuracy: 0.5673 - val_loss: 1.9823 - val_categorical_accuracy: 0.4820\n",
      "Epoch 38/300\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.7642 - categorical_accuracy: 0.5773INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 1.7639 - categorical_accuracy: 0.5773 - val_loss: 1.9670 - val_categorical_accuracy: 0.5030\n",
      "Epoch 39/300\n",
      "500/500 [==============================] - ETA: 0s - loss: 1.7620 - categorical_accuracy: 0.5803INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 1.7620 - categorical_accuracy: 0.5803 - val_loss: 1.9884 - val_categorical_accuracy: 0.5040\n",
      "Epoch 40/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7434 - categorical_accuracy: 0.5878 - val_loss: 1.9618 - val_categorical_accuracy: 0.4970\n",
      "Epoch 41/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7401 - categorical_accuracy: 0.5975 - val_loss: 1.9971 - val_categorical_accuracy: 0.4750\n",
      "Epoch 42/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7367 - categorical_accuracy: 0.6008 - val_loss: 1.9997 - val_categorical_accuracy: 0.4610\n",
      "Epoch 43/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7360 - categorical_accuracy: 0.5950 - val_loss: 1.9817 - val_categorical_accuracy: 0.4970\n",
      "Epoch 44/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.7068 - categorical_accuracy: 0.6162 - val_loss: 1.9821 - val_categorical_accuracy: 0.5020\n",
      "Epoch 45/300\n",
      "498/500 [============================>.] - ETA: 0s - loss: 1.7052 - categorical_accuracy: 0.6145INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 1.7043 - categorical_accuracy: 0.6148 - val_loss: 1.9819 - val_categorical_accuracy: 0.5060\n",
      "Epoch 46/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 1.6951 - categorical_accuracy: 0.6258INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 1.6942 - categorical_accuracy: 0.6255 - val_loss: 1.9664 - val_categorical_accuracy: 0.5120\n",
      "Epoch 47/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6912 - categorical_accuracy: 0.6298 - val_loss: 1.9862 - val_categorical_accuracy: 0.4920\n",
      "Epoch 48/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6791 - categorical_accuracy: 0.6265 - val_loss: 1.9644 - val_categorical_accuracy: 0.5060\n",
      "Epoch 49/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6778 - categorical_accuracy: 0.6367 - val_loss: 1.9659 - val_categorical_accuracy: 0.5090\n",
      "Epoch 50/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.6645 - categorical_accuracy: 0.6407 - val_loss: 1.9601 - val_categorical_accuracy: 0.5090\n",
      "Epoch 51/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 1.6527 - categorical_accuracy: 0.6476INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 17ms/step - loss: 1.6516 - categorical_accuracy: 0.6488 - val_loss: 1.9749 - val_categorical_accuracy: 0.5140\n",
      "Epoch 52/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6380 - categorical_accuracy: 0.6620 - val_loss: 1.9809 - val_categorical_accuracy: 0.5060\n",
      "Epoch 53/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6398 - categorical_accuracy: 0.6538 - val_loss: 1.9458 - val_categorical_accuracy: 0.5060\n",
      "Epoch 54/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.6234 - categorical_accuracy: 0.6637 - val_loss: 1.9547 - val_categorical_accuracy: 0.5100\n",
      "Epoch 55/300\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.6265 - categorical_accuracy: 0.6724INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 1.6260 - categorical_accuracy: 0.6733 - val_loss: 1.9731 - val_categorical_accuracy: 0.5150\n",
      "Epoch 56/300\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.6118 - categorical_accuracy: 0.6691INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 1.6109 - categorical_accuracy: 0.6700 - val_loss: 1.9866 - val_categorical_accuracy: 0.5230\n",
      "Epoch 57/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 1.6064 - categorical_accuracy: 0.6728INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 21ms/step - loss: 1.6056 - categorical_accuracy: 0.6735 - val_loss: 1.9542 - val_categorical_accuracy: 0.5260\n",
      "Epoch 58/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.5992 - categorical_accuracy: 0.6865 - val_loss: 1.9618 - val_categorical_accuracy: 0.5190\n",
      "Epoch 59/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5840 - categorical_accuracy: 0.6917 - val_loss: 1.9698 - val_categorical_accuracy: 0.5230\n",
      "Epoch 60/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5936 - categorical_accuracy: 0.6852 - val_loss: 1.9946 - val_categorical_accuracy: 0.4970\n",
      "Epoch 61/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5692 - categorical_accuracy: 0.7038 - val_loss: 1.9578 - val_categorical_accuracy: 0.5140\n",
      "Epoch 62/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.5693 - categorical_accuracy: 0.7035 - val_loss: 1.9759 - val_categorical_accuracy: 0.5070\n",
      "Epoch 63/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5578 - categorical_accuracy: 0.7120 - val_loss: 2.0007 - val_categorical_accuracy: 0.4960\n",
      "Epoch 64/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5613 - categorical_accuracy: 0.7000 - val_loss: 2.0088 - val_categorical_accuracy: 0.4940\n",
      "Epoch 65/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 1.5603 - categorical_accuracy: 0.7070INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 1.5590 - categorical_accuracy: 0.7082 - val_loss: 1.9659 - val_categorical_accuracy: 0.5310\n",
      "Epoch 66/300\n",
      "497/500 [============================>.] - ETA: 0s - loss: 1.5511 - categorical_accuracy: 0.7155INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 1.5496 - categorical_accuracy: 0.7168 - val_loss: 1.9475 - val_categorical_accuracy: 0.5360\n",
      "Epoch 67/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5458 - categorical_accuracy: 0.7220 - val_loss: 1.9624 - val_categorical_accuracy: 0.5240\n",
      "Epoch 68/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5417 - categorical_accuracy: 0.7245 - val_loss: 2.0123 - val_categorical_accuracy: 0.5180\n",
      "Epoch 69/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5264 - categorical_accuracy: 0.7295 - val_loss: 1.9954 - val_categorical_accuracy: 0.5270\n",
      "Epoch 70/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5217 - categorical_accuracy: 0.7290 - val_loss: 1.9860 - val_categorical_accuracy: 0.5270\n",
      "Epoch 71/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.5206 - categorical_accuracy: 0.7322 - val_loss: 2.0054 - val_categorical_accuracy: 0.5120\n",
      "Epoch 72/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.5176 - categorical_accuracy: 0.7327 - val_loss: 2.0168 - val_categorical_accuracy: 0.5110\n",
      "Epoch 73/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.5238 - categorical_accuracy: 0.7287 - val_loss: 2.0035 - val_categorical_accuracy: 0.5180\n",
      "Epoch 74/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.5014 - categorical_accuracy: 0.7470 - val_loss: 2.0106 - val_categorical_accuracy: 0.5250\n",
      "Epoch 75/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.4966 - categorical_accuracy: 0.7462 - val_loss: 2.0251 - val_categorical_accuracy: 0.5270\n",
      "Epoch 76/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4972 - categorical_accuracy: 0.7492 - val_loss: 2.0353 - val_categorical_accuracy: 0.5150\n",
      "Epoch 77/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4813 - categorical_accuracy: 0.7563 - val_loss: 2.0237 - val_categorical_accuracy: 0.5060\n",
      "Epoch 78/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4813 - categorical_accuracy: 0.7525 - val_loss: 2.0396 - val_categorical_accuracy: 0.5170\n",
      "Epoch 79/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4728 - categorical_accuracy: 0.7663 - val_loss: 2.0432 - val_categorical_accuracy: 0.5050\n",
      "Epoch 80/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4786 - categorical_accuracy: 0.7538 - val_loss: 2.0381 - val_categorical_accuracy: 0.5230\n",
      "Epoch 81/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4803 - categorical_accuracy: 0.7550 - val_loss: 2.0968 - val_categorical_accuracy: 0.4780\n",
      "Epoch 82/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.4695 - categorical_accuracy: 0.7628 - val_loss: 2.0496 - val_categorical_accuracy: 0.5120\n",
      "Epoch 83/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.4716 - categorical_accuracy: 0.7605 - val_loss: 2.0445 - val_categorical_accuracy: 0.5030\n",
      "Epoch 84/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.4636 - categorical_accuracy: 0.7715 - val_loss: 2.0265 - val_categorical_accuracy: 0.5110\n",
      "Epoch 85/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.4565 - categorical_accuracy: 0.7753 - val_loss: 2.0637 - val_categorical_accuracy: 0.4970\n",
      "Epoch 86/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4488 - categorical_accuracy: 0.7735 - val_loss: 2.0673 - val_categorical_accuracy: 0.4830\n",
      "Epoch 87/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.4483 - categorical_accuracy: 0.7800 - val_loss: 2.0251 - val_categorical_accuracy: 0.5170\n",
      "Epoch 88/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4519 - categorical_accuracy: 0.7768 - val_loss: 2.0442 - val_categorical_accuracy: 0.5050\n",
      "Epoch 89/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4399 - categorical_accuracy: 0.7853 - val_loss: 2.0370 - val_categorical_accuracy: 0.5040\n",
      "Epoch 90/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4452 - categorical_accuracy: 0.7722 - val_loss: 2.0413 - val_categorical_accuracy: 0.5110\n",
      "Epoch 91/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4362 - categorical_accuracy: 0.7970 - val_loss: 2.0812 - val_categorical_accuracy: 0.4840\n",
      "Epoch 92/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4416 - categorical_accuracy: 0.7812 - val_loss: 2.0321 - val_categorical_accuracy: 0.5110\n",
      "Epoch 93/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.4245 - categorical_accuracy: 0.7908 - val_loss: 2.0195 - val_categorical_accuracy: 0.4970\n",
      "Epoch 94/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.4293 - categorical_accuracy: 0.7872 - val_loss: 2.0322 - val_categorical_accuracy: 0.5190\n",
      "Epoch 95/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4207 - categorical_accuracy: 0.7935 - val_loss: 2.0347 - val_categorical_accuracy: 0.5030\n",
      "Epoch 96/300\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 1.4154 - categorical_accuracy: 0.8075 - val_loss: 2.0480 - val_categorical_accuracy: 0.5210\n",
      "Epoch 97/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4132 - categorical_accuracy: 0.7987 - val_loss: 2.0238 - val_categorical_accuracy: 0.5160\n",
      "Epoch 98/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4125 - categorical_accuracy: 0.8018 - val_loss: 2.0364 - val_categorical_accuracy: 0.5230\n",
      "Epoch 99/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.4110 - categorical_accuracy: 0.8037 - val_loss: 2.0409 - val_categorical_accuracy: 0.5070\n",
      "Epoch 100/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4062 - categorical_accuracy: 0.8055 - val_loss: 2.0438 - val_categorical_accuracy: 0.5190\n",
      "Epoch 101/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4077 - categorical_accuracy: 0.8027 - val_loss: 2.0179 - val_categorical_accuracy: 0.5190\n",
      "Epoch 102/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3983 - categorical_accuracy: 0.8158 - val_loss: 2.0457 - val_categorical_accuracy: 0.5120\n",
      "Epoch 103/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3922 - categorical_accuracy: 0.8180 - val_loss: 2.0285 - val_categorical_accuracy: 0.5160\n",
      "Epoch 104/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3986 - categorical_accuracy: 0.8105 - val_loss: 2.0248 - val_categorical_accuracy: 0.5340\n",
      "Epoch 105/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3850 - categorical_accuracy: 0.8213 - val_loss: 2.0350 - val_categorical_accuracy: 0.5280\n",
      "Epoch 106/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3816 - categorical_accuracy: 0.8192 - val_loss: 2.0313 - val_categorical_accuracy: 0.5350\n",
      "Epoch 107/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3793 - categorical_accuracy: 0.8245 - val_loss: 2.0491 - val_categorical_accuracy: 0.5260\n",
      "Epoch 108/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.3673 - categorical_accuracy: 0.8235 - val_loss: 2.0581 - val_categorical_accuracy: 0.5310\n",
      "Epoch 109/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3680 - categorical_accuracy: 0.8273 - val_loss: 2.0367 - val_categorical_accuracy: 0.5270\n",
      "Epoch 110/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3742 - categorical_accuracy: 0.8192 - val_loss: 2.0492 - val_categorical_accuracy: 0.5330\n",
      "Epoch 111/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3693 - categorical_accuracy: 0.8357 - val_loss: 2.0237 - val_categorical_accuracy: 0.5300\n",
      "Epoch 112/300\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.3648 - categorical_accuracy: 0.8324INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 17ms/step - loss: 1.3646 - categorical_accuracy: 0.8328 - val_loss: 2.0260 - val_categorical_accuracy: 0.5370\n",
      "Epoch 113/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3629 - categorical_accuracy: 0.8300 - val_loss: 2.0548 - val_categorical_accuracy: 0.5190\n",
      "Epoch 114/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.3543 - categorical_accuracy: 0.8350 - val_loss: 2.0647 - val_categorical_accuracy: 0.5270\n",
      "Epoch 115/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3620 - categorical_accuracy: 0.8295 - val_loss: 2.0435 - val_categorical_accuracy: 0.5260\n",
      "Epoch 116/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3517 - categorical_accuracy: 0.8335 - val_loss: 2.0584 - val_categorical_accuracy: 0.5240\n",
      "Epoch 117/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3420 - categorical_accuracy: 0.8485 - val_loss: 2.0789 - val_categorical_accuracy: 0.5190\n",
      "Epoch 118/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3422 - categorical_accuracy: 0.8465 - val_loss: 2.0691 - val_categorical_accuracy: 0.5230\n",
      "Epoch 119/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.3411 - categorical_accuracy: 0.8510 - val_loss: 2.0784 - val_categorical_accuracy: 0.5170\n",
      "Epoch 120/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3416 - categorical_accuracy: 0.8455 - val_loss: 2.0774 - val_categorical_accuracy: 0.5310\n",
      "Epoch 121/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3413 - categorical_accuracy: 0.8487 - val_loss: 2.0753 - val_categorical_accuracy: 0.5210\n",
      "Epoch 122/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.3287 - categorical_accuracy: 0.8572 - val_loss: 2.0902 - val_categorical_accuracy: 0.5170\n",
      "Epoch 123/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3337 - categorical_accuracy: 0.8475 - val_loss: 2.0764 - val_categorical_accuracy: 0.5310\n",
      "Epoch 124/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.3247 - categorical_accuracy: 0.8537 - val_loss: 2.0840 - val_categorical_accuracy: 0.5280\n",
      "Epoch 125/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.3293 - categorical_accuracy: 0.8485 - val_loss: 2.0794 - val_categorical_accuracy: 0.5260\n",
      "Epoch 126/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3220 - categorical_accuracy: 0.8570 - val_loss: 2.0788 - val_categorical_accuracy: 0.5190\n",
      "Epoch 127/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3259 - categorical_accuracy: 0.8543 - val_loss: 2.0680 - val_categorical_accuracy: 0.5230\n",
      "Epoch 128/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3211 - categorical_accuracy: 0.8505 - val_loss: 2.0618 - val_categorical_accuracy: 0.5270\n",
      "Epoch 129/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3201 - categorical_accuracy: 0.8547 - val_loss: 2.0716 - val_categorical_accuracy: 0.5310\n",
      "Epoch 130/300\n",
      "499/500 [============================>.] - ETA: 0s - loss: 1.3095 - categorical_accuracy: 0.8652INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 10s 20ms/step - loss: 1.3090 - categorical_accuracy: 0.8655 - val_loss: 2.0945 - val_categorical_accuracy: 0.5450\n",
      "Epoch 131/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3175 - categorical_accuracy: 0.8655 - val_loss: 2.0812 - val_categorical_accuracy: 0.5320\n",
      "Epoch 132/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3119 - categorical_accuracy: 0.8665 - val_loss: 2.0780 - val_categorical_accuracy: 0.5320\n",
      "Epoch 133/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3048 - categorical_accuracy: 0.8675 - val_loss: 2.0773 - val_categorical_accuracy: 0.5280\n",
      "Epoch 134/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3042 - categorical_accuracy: 0.8670 - val_loss: 2.0836 - val_categorical_accuracy: 0.5330\n",
      "Epoch 135/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3009 - categorical_accuracy: 0.8695 - val_loss: 2.0890 - val_categorical_accuracy: 0.5260\n",
      "Epoch 136/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2991 - categorical_accuracy: 0.8710 - val_loss: 2.0877 - val_categorical_accuracy: 0.5240\n",
      "Epoch 137/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.3013 - categorical_accuracy: 0.8673 - val_loss: 2.0822 - val_categorical_accuracy: 0.5270\n",
      "Epoch 138/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2999 - categorical_accuracy: 0.8745 - val_loss: 2.0868 - val_categorical_accuracy: 0.5180\n",
      "Epoch 139/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3009 - categorical_accuracy: 0.8677 - val_loss: 2.0850 - val_categorical_accuracy: 0.5210\n",
      "Epoch 140/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2960 - categorical_accuracy: 0.8765 - val_loss: 2.0911 - val_categorical_accuracy: 0.5350\n",
      "Epoch 141/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2945 - categorical_accuracy: 0.8677 - val_loss: 2.0955 - val_categorical_accuracy: 0.5240\n",
      "Epoch 142/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2956 - categorical_accuracy: 0.8767 - val_loss: 2.0953 - val_categorical_accuracy: 0.5330\n",
      "Epoch 143/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2902 - categorical_accuracy: 0.8760 - val_loss: 2.0912 - val_categorical_accuracy: 0.5350\n",
      "Epoch 144/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2807 - categorical_accuracy: 0.8830 - val_loss: 2.0931 - val_categorical_accuracy: 0.5280\n",
      "Epoch 145/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2814 - categorical_accuracy: 0.8775 - val_loss: 2.0912 - val_categorical_accuracy: 0.5340\n",
      "Epoch 146/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2948 - categorical_accuracy: 0.8710 - val_loss: 2.0850 - val_categorical_accuracy: 0.5350\n",
      "Epoch 147/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.2872 - categorical_accuracy: 0.8790 - val_loss: 2.0982 - val_categorical_accuracy: 0.5340\n",
      "Epoch 148/300\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 1.2832 - categorical_accuracy: 0.8823 - val_loss: 2.0948 - val_categorical_accuracy: 0.5220\n",
      "Epoch 149/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2836 - categorical_accuracy: 0.8808 - val_loss: 2.0984 - val_categorical_accuracy: 0.5390\n",
      "Epoch 150/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2827 - categorical_accuracy: 0.8838 - val_loss: 2.0904 - val_categorical_accuracy: 0.5330\n",
      "Epoch 151/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4547 - categorical_accuracy: 0.7755 - val_loss: 2.1176 - val_categorical_accuracy: 0.5200\n",
      "Epoch 152/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4519 - categorical_accuracy: 0.7753 - val_loss: 2.1721 - val_categorical_accuracy: 0.5050\n",
      "Epoch 153/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.4543 - categorical_accuracy: 0.7703 - val_loss: 2.0552 - val_categorical_accuracy: 0.5370\n",
      "Epoch 154/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4462 - categorical_accuracy: 0.7753 - val_loss: 2.0677 - val_categorical_accuracy: 0.5350\n",
      "Epoch 155/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4294 - categorical_accuracy: 0.7860 - val_loss: 2.0604 - val_categorical_accuracy: 0.5260\n",
      "Epoch 156/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4417 - categorical_accuracy: 0.7797 - val_loss: 2.0641 - val_categorical_accuracy: 0.5310\n",
      "Epoch 157/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4323 - categorical_accuracy: 0.7845 - val_loss: 2.0730 - val_categorical_accuracy: 0.5210\n",
      "Epoch 158/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4130 - categorical_accuracy: 0.7970 - val_loss: 2.1505 - val_categorical_accuracy: 0.5210\n",
      "Epoch 159/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4168 - categorical_accuracy: 0.7897 - val_loss: 2.1062 - val_categorical_accuracy: 0.4970\n",
      "Epoch 160/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4141 - categorical_accuracy: 0.8005 - val_loss: 2.0638 - val_categorical_accuracy: 0.5260\n",
      "Epoch 161/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4107 - categorical_accuracy: 0.8027 - val_loss: 2.0931 - val_categorical_accuracy: 0.5250\n",
      "Epoch 162/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4047 - categorical_accuracy: 0.8000 - val_loss: 2.1204 - val_categorical_accuracy: 0.5070\n",
      "Epoch 163/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.4073 - categorical_accuracy: 0.8060 - val_loss: 2.0882 - val_categorical_accuracy: 0.5290\n",
      "Epoch 164/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3771 - categorical_accuracy: 0.8230 - val_loss: 2.0902 - val_categorical_accuracy: 0.5240\n",
      "Epoch 165/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3945 - categorical_accuracy: 0.8133 - val_loss: 2.1130 - val_categorical_accuracy: 0.5100\n",
      "Epoch 166/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3952 - categorical_accuracy: 0.8060 - val_loss: 2.0820 - val_categorical_accuracy: 0.5350\n",
      "Epoch 167/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.3828 - categorical_accuracy: 0.8190 - val_loss: 2.0762 - val_categorical_accuracy: 0.5230\n",
      "Epoch 168/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3773 - categorical_accuracy: 0.8220 - val_loss: 2.1104 - val_categorical_accuracy: 0.5230\n",
      "Epoch 169/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3724 - categorical_accuracy: 0.8220 - val_loss: 2.1583 - val_categorical_accuracy: 0.5100\n",
      "Epoch 170/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3596 - categorical_accuracy: 0.8310 - val_loss: 2.1548 - val_categorical_accuracy: 0.4990\n",
      "Epoch 171/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.3610 - categorical_accuracy: 0.8292 - val_loss: 2.0776 - val_categorical_accuracy: 0.5250\n",
      "Epoch 172/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3611 - categorical_accuracy: 0.8310 - val_loss: 2.1007 - val_categorical_accuracy: 0.5170\n",
      "Epoch 173/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3448 - categorical_accuracy: 0.8428 - val_loss: 2.1123 - val_categorical_accuracy: 0.5310\n",
      "Epoch 174/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3495 - categorical_accuracy: 0.8390 - val_loss: 2.0708 - val_categorical_accuracy: 0.5140\n",
      "Epoch 175/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3493 - categorical_accuracy: 0.8328 - val_loss: 2.1464 - val_categorical_accuracy: 0.5000\n",
      "Epoch 176/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3374 - categorical_accuracy: 0.8503 - val_loss: 2.1232 - val_categorical_accuracy: 0.5220\n",
      "Epoch 177/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.3383 - categorical_accuracy: 0.8425 - val_loss: 2.1308 - val_categorical_accuracy: 0.5160\n",
      "Epoch 178/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3369 - categorical_accuracy: 0.8435 - val_loss: 2.1403 - val_categorical_accuracy: 0.5210\n",
      "Epoch 179/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3373 - categorical_accuracy: 0.8480 - val_loss: 2.1277 - val_categorical_accuracy: 0.5170\n",
      "Epoch 180/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.3327 - categorical_accuracy: 0.8505 - val_loss: 2.1229 - val_categorical_accuracy: 0.5260\n",
      "Epoch 181/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3308 - categorical_accuracy: 0.8462 - val_loss: 2.1105 - val_categorical_accuracy: 0.5130\n",
      "Epoch 182/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3132 - categorical_accuracy: 0.8602 - val_loss: 2.1068 - val_categorical_accuracy: 0.5140\n",
      "Epoch 183/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3267 - categorical_accuracy: 0.8562 - val_loss: 2.0764 - val_categorical_accuracy: 0.5440\n",
      "Epoch 184/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3090 - categorical_accuracy: 0.8605 - val_loss: 2.1299 - val_categorical_accuracy: 0.5230\n",
      "Epoch 185/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3157 - categorical_accuracy: 0.8690 - val_loss: 2.1201 - val_categorical_accuracy: 0.5190\n",
      "Epoch 186/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.3069 - categorical_accuracy: 0.8710 - val_loss: 2.1339 - val_categorical_accuracy: 0.5300\n",
      "Epoch 187/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.3041 - categorical_accuracy: 0.8622 - val_loss: 2.1029 - val_categorical_accuracy: 0.5230\n",
      "Epoch 188/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2899 - categorical_accuracy: 0.8752 - val_loss: 2.1816 - val_categorical_accuracy: 0.4940\n",
      "Epoch 189/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2974 - categorical_accuracy: 0.8717 - val_loss: 2.1286 - val_categorical_accuracy: 0.5200\n",
      "Epoch 190/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2879 - categorical_accuracy: 0.8770 - val_loss: 2.1506 - val_categorical_accuracy: 0.5360\n",
      "Epoch 191/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2797 - categorical_accuracy: 0.8888 - val_loss: 2.1599 - val_categorical_accuracy: 0.5150\n",
      "Epoch 192/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2857 - categorical_accuracy: 0.8800 - val_loss: 2.1549 - val_categorical_accuracy: 0.4980\n",
      "Epoch 193/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2824 - categorical_accuracy: 0.8788 - val_loss: 2.1226 - val_categorical_accuracy: 0.5280\n",
      "Epoch 194/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2736 - categorical_accuracy: 0.8898 - val_loss: 2.1761 - val_categorical_accuracy: 0.5240\n",
      "Epoch 195/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2761 - categorical_accuracy: 0.8840 - val_loss: 2.1407 - val_categorical_accuracy: 0.5260\n",
      "Epoch 196/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2768 - categorical_accuracy: 0.8863 - val_loss: 2.1105 - val_categorical_accuracy: 0.5310\n",
      "Epoch 197/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2700 - categorical_accuracy: 0.8873 - val_loss: 2.1174 - val_categorical_accuracy: 0.5280\n",
      "Epoch 198/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2593 - categorical_accuracy: 0.8955 - val_loss: 2.1869 - val_categorical_accuracy: 0.5250\n",
      "Epoch 199/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2655 - categorical_accuracy: 0.8928 - val_loss: 2.1294 - val_categorical_accuracy: 0.5320\n",
      "Epoch 200/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2623 - categorical_accuracy: 0.8988 - val_loss: 2.1038 - val_categorical_accuracy: 0.5210\n",
      "Epoch 201/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2599 - categorical_accuracy: 0.8955 - val_loss: 2.1175 - val_categorical_accuracy: 0.5220\n",
      "Epoch 202/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2604 - categorical_accuracy: 0.8947 - val_loss: 2.1561 - val_categorical_accuracy: 0.5150\n",
      "Epoch 203/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2567 - categorical_accuracy: 0.8935 - val_loss: 2.1415 - val_categorical_accuracy: 0.5170\n",
      "Epoch 204/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2424 - categorical_accuracy: 0.9095 - val_loss: 2.1638 - val_categorical_accuracy: 0.5310\n",
      "Epoch 205/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.2507 - categorical_accuracy: 0.9053 - val_loss: 2.1228 - val_categorical_accuracy: 0.5260\n",
      "Epoch 206/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2474 - categorical_accuracy: 0.9035 - val_loss: 2.1226 - val_categorical_accuracy: 0.5330\n",
      "Epoch 207/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2360 - categorical_accuracy: 0.9075 - val_loss: 2.1770 - val_categorical_accuracy: 0.5180\n",
      "Epoch 208/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2347 - categorical_accuracy: 0.9135 - val_loss: 2.1625 - val_categorical_accuracy: 0.5250\n",
      "Epoch 209/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2353 - categorical_accuracy: 0.9080 - val_loss: 2.1529 - val_categorical_accuracy: 0.5210\n",
      "Epoch 210/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2263 - categorical_accuracy: 0.9178 - val_loss: 2.1870 - val_categorical_accuracy: 0.5240\n",
      "Epoch 211/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2336 - categorical_accuracy: 0.9125 - val_loss: 2.1586 - val_categorical_accuracy: 0.5150\n",
      "Epoch 212/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2179 - categorical_accuracy: 0.9208 - val_loss: 2.1529 - val_categorical_accuracy: 0.5220\n",
      "Epoch 213/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2265 - categorical_accuracy: 0.9165 - val_loss: 2.1467 - val_categorical_accuracy: 0.5210\n",
      "Epoch 214/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.2142 - categorical_accuracy: 0.9255 - val_loss: 2.1791 - val_categorical_accuracy: 0.5290\n",
      "Epoch 215/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2140 - categorical_accuracy: 0.9245 - val_loss: 2.1673 - val_categorical_accuracy: 0.5180\n",
      "Epoch 216/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2174 - categorical_accuracy: 0.9235 - val_loss: 2.1487 - val_categorical_accuracy: 0.5290\n",
      "Epoch 217/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2115 - categorical_accuracy: 0.9275 - val_loss: 2.1220 - val_categorical_accuracy: 0.5210\n",
      "Epoch 218/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2151 - categorical_accuracy: 0.9222 - val_loss: 2.1216 - val_categorical_accuracy: 0.5290\n",
      "Epoch 219/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2078 - categorical_accuracy: 0.9260 - val_loss: 2.1381 - val_categorical_accuracy: 0.5340\n",
      "Epoch 220/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.2040 - categorical_accuracy: 0.9333 - val_loss: 2.1552 - val_categorical_accuracy: 0.5180\n",
      "Epoch 221/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1987 - categorical_accuracy: 0.9348 - val_loss: 2.1723 - val_categorical_accuracy: 0.5100\n",
      "Epoch 222/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1933 - categorical_accuracy: 0.9337 - val_loss: 2.1272 - val_categorical_accuracy: 0.5190\n",
      "Epoch 223/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1970 - categorical_accuracy: 0.9330 - val_loss: 2.1189 - val_categorical_accuracy: 0.5190\n",
      "Epoch 224/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.2010 - categorical_accuracy: 0.9305 - val_loss: 2.1190 - val_categorical_accuracy: 0.5310\n",
      "Epoch 225/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1911 - categorical_accuracy: 0.9358 - val_loss: 2.1559 - val_categorical_accuracy: 0.5150\n",
      "Epoch 226/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.1842 - categorical_accuracy: 0.9375 - val_loss: 2.1303 - val_categorical_accuracy: 0.5180\n",
      "Epoch 227/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1937 - categorical_accuracy: 0.9360 - val_loss: 2.1514 - val_categorical_accuracy: 0.5260\n",
      "Epoch 228/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1836 - categorical_accuracy: 0.9430 - val_loss: 2.1696 - val_categorical_accuracy: 0.5260\n",
      "Epoch 229/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1871 - categorical_accuracy: 0.9400 - val_loss: 2.1843 - val_categorical_accuracy: 0.5110\n",
      "Epoch 230/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1822 - categorical_accuracy: 0.9433 - val_loss: 2.1735 - val_categorical_accuracy: 0.5180\n",
      "Epoch 231/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1804 - categorical_accuracy: 0.9433 - val_loss: 2.1477 - val_categorical_accuracy: 0.5220\n",
      "Epoch 232/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1851 - categorical_accuracy: 0.9380 - val_loss: 2.1317 - val_categorical_accuracy: 0.5160\n",
      "Epoch 233/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1723 - categorical_accuracy: 0.9442 - val_loss: 2.1635 - val_categorical_accuracy: 0.5250\n",
      "Epoch 234/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1694 - categorical_accuracy: 0.9505 - val_loss: 2.1687 - val_categorical_accuracy: 0.5150\n",
      "Epoch 235/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1735 - categorical_accuracy: 0.9503 - val_loss: 2.1384 - val_categorical_accuracy: 0.5270\n",
      "Epoch 236/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1736 - categorical_accuracy: 0.9465 - val_loss: 2.1615 - val_categorical_accuracy: 0.5150\n",
      "Epoch 237/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1723 - categorical_accuracy: 0.9445 - val_loss: 2.1483 - val_categorical_accuracy: 0.5200\n",
      "Epoch 238/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1613 - categorical_accuracy: 0.9572 - val_loss: 2.1336 - val_categorical_accuracy: 0.5190\n",
      "Epoch 239/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1632 - categorical_accuracy: 0.9480 - val_loss: 2.1039 - val_categorical_accuracy: 0.5300\n",
      "Epoch 240/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1636 - categorical_accuracy: 0.9503 - val_loss: 2.1261 - val_categorical_accuracy: 0.5350\n",
      "Epoch 241/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1631 - categorical_accuracy: 0.9513 - val_loss: 2.1811 - val_categorical_accuracy: 0.5270\n",
      "Epoch 242/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1570 - categorical_accuracy: 0.9572 - val_loss: 2.1470 - val_categorical_accuracy: 0.5270\n",
      "Epoch 243/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1567 - categorical_accuracy: 0.9560 - val_loss: 2.1555 - val_categorical_accuracy: 0.5140\n",
      "Epoch 244/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1590 - categorical_accuracy: 0.9548 - val_loss: 2.1503 - val_categorical_accuracy: 0.5320\n",
      "Epoch 245/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1559 - categorical_accuracy: 0.9538 - val_loss: 2.1838 - val_categorical_accuracy: 0.5110\n",
      "Epoch 246/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1552 - categorical_accuracy: 0.9570 - val_loss: 2.1210 - val_categorical_accuracy: 0.5230\n",
      "Epoch 247/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.1566 - categorical_accuracy: 0.9532 - val_loss: 2.1555 - val_categorical_accuracy: 0.5170\n",
      "Epoch 248/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1522 - categorical_accuracy: 0.9550 - val_loss: 2.1302 - val_categorical_accuracy: 0.5180\n",
      "Epoch 249/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1476 - categorical_accuracy: 0.9632 - val_loss: 2.1277 - val_categorical_accuracy: 0.5280\n",
      "Epoch 250/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1442 - categorical_accuracy: 0.9655 - val_loss: 2.1612 - val_categorical_accuracy: 0.5210\n",
      "Epoch 251/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1474 - categorical_accuracy: 0.9582 - val_loss: 2.1253 - val_categorical_accuracy: 0.5250\n",
      "Epoch 252/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1447 - categorical_accuracy: 0.9640 - val_loss: 2.1562 - val_categorical_accuracy: 0.5220\n",
      "Epoch 253/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1431 - categorical_accuracy: 0.9635 - val_loss: 2.1667 - val_categorical_accuracy: 0.5240\n",
      "Epoch 254/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1425 - categorical_accuracy: 0.9613 - val_loss: 2.1403 - val_categorical_accuracy: 0.5330\n",
      "Epoch 255/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1416 - categorical_accuracy: 0.9640 - val_loss: 2.1295 - val_categorical_accuracy: 0.5250\n",
      "Epoch 256/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1403 - categorical_accuracy: 0.9628 - val_loss: 2.1186 - val_categorical_accuracy: 0.5320\n",
      "Epoch 257/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1363 - categorical_accuracy: 0.9655 - val_loss: 2.1466 - val_categorical_accuracy: 0.5270\n",
      "Epoch 258/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1363 - categorical_accuracy: 0.9655 - val_loss: 2.1165 - val_categorical_accuracy: 0.5230\n",
      "Epoch 259/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1352 - categorical_accuracy: 0.9653 - val_loss: 2.0942 - val_categorical_accuracy: 0.5310\n",
      "Epoch 260/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1432 - categorical_accuracy: 0.9650 - val_loss: 2.1423 - val_categorical_accuracy: 0.5300\n",
      "Epoch 261/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1358 - categorical_accuracy: 0.9693 - val_loss: 2.1230 - val_categorical_accuracy: 0.5220\n",
      "Epoch 262/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1287 - categorical_accuracy: 0.9703 - val_loss: 2.0972 - val_categorical_accuracy: 0.5290\n",
      "Epoch 263/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1316 - categorical_accuracy: 0.9688 - val_loss: 2.1176 - val_categorical_accuracy: 0.5360\n",
      "Epoch 264/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1299 - categorical_accuracy: 0.9680 - val_loss: 2.1132 - val_categorical_accuracy: 0.5400\n",
      "Epoch 265/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1302 - categorical_accuracy: 0.9678 - val_loss: 2.1455 - val_categorical_accuracy: 0.5290\n",
      "Epoch 266/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1264 - categorical_accuracy: 0.9710 - val_loss: 2.0933 - val_categorical_accuracy: 0.5380\n",
      "Epoch 267/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1310 - categorical_accuracy: 0.9707 - val_loss: 2.1198 - val_categorical_accuracy: 0.5260\n",
      "Epoch 268/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1303 - categorical_accuracy: 0.9682 - val_loss: 2.1205 - val_categorical_accuracy: 0.5300\n",
      "Epoch 269/300\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.1267 - categorical_accuracy: 0.9705INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 1.1266 - categorical_accuracy: 0.9703 - val_loss: 2.0949 - val_categorical_accuracy: 0.5460\n",
      "Epoch 270/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1212 - categorical_accuracy: 0.9740 - val_loss: 2.0985 - val_categorical_accuracy: 0.5460\n",
      "Epoch 271/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1196 - categorical_accuracy: 0.9715 - val_loss: 2.1031 - val_categorical_accuracy: 0.5400\n",
      "Epoch 272/300\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.1324 - categorical_accuracy: 0.9705INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 21ms/step - loss: 1.1326 - categorical_accuracy: 0.9700 - val_loss: 2.0920 - val_categorical_accuracy: 0.5470\n",
      "Epoch 273/300\n",
      "496/500 [============================>.] - ETA: 0s - loss: 1.1207 - categorical_accuracy: 0.9733INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/Transformer-embed=64-heads=2-ff=128-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 18ms/step - loss: 1.1205 - categorical_accuracy: 0.9732 - val_loss: 2.0848 - val_categorical_accuracy: 0.5580\n",
      "Epoch 274/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1217 - categorical_accuracy: 0.9718 - val_loss: 2.1025 - val_categorical_accuracy: 0.5470\n",
      "Epoch 275/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1203 - categorical_accuracy: 0.9740 - val_loss: 2.1123 - val_categorical_accuracy: 0.5350\n",
      "Epoch 276/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1215 - categorical_accuracy: 0.9707 - val_loss: 2.0885 - val_categorical_accuracy: 0.5430\n",
      "Epoch 277/300\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 1.1188 - categorical_accuracy: 0.9753 - val_loss: 2.1138 - val_categorical_accuracy: 0.5370\n",
      "Epoch 278/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1203 - categorical_accuracy: 0.9707 - val_loss: 2.1219 - val_categorical_accuracy: 0.5440\n",
      "Epoch 279/300\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 1.1223 - categorical_accuracy: 0.9722 - val_loss: 2.1040 - val_categorical_accuracy: 0.5400\n",
      "Epoch 280/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1119 - categorical_accuracy: 0.9728 - val_loss: 2.1092 - val_categorical_accuracy: 0.5360\n",
      "Epoch 281/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1172 - categorical_accuracy: 0.9762 - val_loss: 2.1117 - val_categorical_accuracy: 0.5500\n",
      "Epoch 282/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1176 - categorical_accuracy: 0.9718 - val_loss: 2.1202 - val_categorical_accuracy: 0.5460\n",
      "Epoch 283/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1196 - categorical_accuracy: 0.9735 - val_loss: 2.1118 - val_categorical_accuracy: 0.5370\n",
      "Epoch 284/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1119 - categorical_accuracy: 0.9758 - val_loss: 2.0920 - val_categorical_accuracy: 0.5490\n",
      "Epoch 285/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1095 - categorical_accuracy: 0.9790 - val_loss: 2.0827 - val_categorical_accuracy: 0.5440\n",
      "Epoch 286/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1167 - categorical_accuracy: 0.9732 - val_loss: 2.1018 - val_categorical_accuracy: 0.5400\n",
      "Epoch 287/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1080 - categorical_accuracy: 0.9812 - val_loss: 2.1054 - val_categorical_accuracy: 0.5470\n",
      "Epoch 288/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1097 - categorical_accuracy: 0.9765 - val_loss: 2.0804 - val_categorical_accuracy: 0.5450\n",
      "Epoch 289/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1111 - categorical_accuracy: 0.9772 - val_loss: 2.0867 - val_categorical_accuracy: 0.5440\n",
      "Epoch 290/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1114 - categorical_accuracy: 0.9758 - val_loss: 2.0951 - val_categorical_accuracy: 0.5510\n",
      "Epoch 291/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1082 - categorical_accuracy: 0.9780 - val_loss: 2.1041 - val_categorical_accuracy: 0.5420\n",
      "Epoch 292/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1115 - categorical_accuracy: 0.9770 - val_loss: 2.1099 - val_categorical_accuracy: 0.5490\n",
      "Epoch 293/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1078 - categorical_accuracy: 0.9793 - val_loss: 2.1042 - val_categorical_accuracy: 0.5440\n",
      "Epoch 294/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1136 - categorical_accuracy: 0.9740 - val_loss: 2.0764 - val_categorical_accuracy: 0.5450\n",
      "Epoch 295/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1077 - categorical_accuracy: 0.9780 - val_loss: 2.0918 - val_categorical_accuracy: 0.5500\n",
      "Epoch 296/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1004 - categorical_accuracy: 0.9803 - val_loss: 2.0800 - val_categorical_accuracy: 0.5460\n",
      "Epoch 297/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1067 - categorical_accuracy: 0.9790 - val_loss: 2.0751 - val_categorical_accuracy: 0.5530\n",
      "Epoch 298/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1052 - categorical_accuracy: 0.9810 - val_loss: 2.0948 - val_categorical_accuracy: 0.5560\n",
      "Epoch 299/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1058 - categorical_accuracy: 0.9760 - val_loss: 2.0948 - val_categorical_accuracy: 0.5490\n",
      "Epoch 300/300\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 1.1058 - categorical_accuracy: 0.9758 - val_loss: 2.0758 - val_categorical_accuracy: 0.5530\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>▁▂▃▃▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▆▆▇▇▇▇▇▇████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>████▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁████▇▇▆▆▅▅▄▄▃▃▂▂▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▇▆▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>▁▂▅▅▇▇▇▇▇▇▇▇▇▇█▇▇██▇▇▇▇▇▇▇▇▇██▇▇█▇██████</td></tr><tr><td>epoch/val_loss</td><td>█▆▃▂▁▁▁▂▂▂▃▂▃▃▃▃▃▃▄▄▅▃▄▄▄▅▄▅▅▄▅▅▅▅▄▄▄▄▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/categorical_accuracy</td><td>0.97575</td></tr><tr><td>epoch/epoch</td><td>299</td></tr><tr><td>epoch/learning_rate</td><td>5e-05</td></tr><tr><td>epoch/loss</td><td>1.10578</td></tr><tr><td>epoch/val_categorical_accuracy</td><td>0.553</td></tr><tr><td>epoch/val_loss</td><td>2.07578</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Transformer-embed=128-heads=2-ff=128-D=0.2-Dense=256-LipsEyesHandsPose_5k</strong> at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/u544yx92' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset/runs/u544yx92</a><br/> View project at: <a href='https://wandb.ai/mlewand7/mediapipe-asl-dataset' target=\"_blank\">https://wandb.ai/mlewand7/mediapipe-asl-dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240416_154118-u544yx92/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f94783909d0>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dim = 64  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 128  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = layers.Input(shape=X_shape)\n",
    "embedding_layer = TokenAndPositionEmbedding(X_shape[0], X_shape[1])\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(X_shape[1], num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(256, activation=\"gelu\")(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "outputs = Dense(len(train_dataset_parquet.unique_labels), activation='softmax')(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "keras_train(model, filepath=os.path.join(\"models\", f\"Transformer-embed={embed_dim}-heads={num_heads}-ff={ff_dim}-D=0.1-Dense=128-Dense=256-LipsEyesHandsPose_5k.tf\"),\n",
    "            run_name=\"Transformer-embed=128-heads=2-ff=128-D=0.2-Dense=256-LipsEyesHandsPose_5k\",\n",
    "            mediapipe_features=\"reduced-LipsEyesHandsPose\",\n",
    "            T_max=151, epochs=300, \n",
    "            max_lr = 3e-4, min_lr = 5e-5,\n",
    "            USE_WANDB=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 5087314,
     "sourceId": 46105,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
